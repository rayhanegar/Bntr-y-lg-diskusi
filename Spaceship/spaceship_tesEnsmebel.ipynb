{"cells":[{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"c854b4b384a54e55b2c757b96dd44964","deepnote_cell_type":"text-cell-h1"},"source":"# Spaceship Lightgbm Submission","block_group":"082f42d739ae4c33b25722d4d688de17"},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1715204859952,"execution_millis":2529,"deepnote_to_be_reexecuted":false,"cell_id":"92865d000c424961a42d4614d167541c","deepnote_cell_type":"code"},"source":"!pip install lightgbm==4.3.0","block_group":"ed00792b3cca4a8e878c8e325bb73c4b","execution_count":null,"outputs":[{"name":"stdout","text":"Collecting lightgbm==4.3.0\n  Downloading lightgbm-4.3.0-py3-none-manylinux_2_28_x86_64.whl (3.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m103.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy in /shared-libs/python3.9/py/lib/python3.9/site-packages (from lightgbm==4.3.0) (1.23.4)\nRequirement already satisfied: scipy in /shared-libs/python3.9/py/lib/python3.9/site-packages (from lightgbm==4.3.0) (1.9.3)\nInstalling collected packages: lightgbm\nSuccessfully installed lightgbm-4.3.0\n\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/53812f76-791e-435e-83ac-a813f48efe18","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1715204862492,"execution_millis":5427,"deepnote_to_be_reexecuted":false,"cell_id":"814cd973c5f04bd7bfc68bebbe1c1129","deepnote_cell_type":"code"},"source":"!pip install catboost==1.2.5","block_group":"0c66df5c00b240f8b468248f4db9dc4c","execution_count":null,"outputs":[{"name":"stdout","text":"Collecting catboost==1.2.5\n  Downloading catboost-1.2.5-cp39-cp39-manylinux2014_x86_64.whl (98.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.16.0 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from catboost==1.2.5) (1.23.4)\nRequirement already satisfied: pandas>=0.24 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from catboost==1.2.5) (2.1.4)\nRequirement already satisfied: scipy in /shared-libs/python3.9/py/lib/python3.9/site-packages (from catboost==1.2.5) (1.9.3)\nRequirement already satisfied: plotly in /shared-libs/python3.9/py/lib/python3.9/site-packages (from catboost==1.2.5) (5.10.0)\nCollecting graphviz\n  Downloading graphviz-0.20.3-py3-none-any.whl (47 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.1/47.1 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: six in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from catboost==1.2.5) (1.16.0)\nRequirement already satisfied: matplotlib in /shared-libs/python3.9/py/lib/python3.9/site-packages (from catboost==1.2.5) (3.6.0)\nRequirement already satisfied: tzdata>=2022.1 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from pandas>=0.24->catboost==1.2.5) (2022.5)\nRequirement already satisfied: python-dateutil>=2.8.2 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from pandas>=0.24->catboost==1.2.5) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from pandas>=0.24->catboost==1.2.5) (2022.5)\nRequirement already satisfied: kiwisolver>=1.0.1 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from matplotlib->catboost==1.2.5) (1.4.4)\nRequirement already satisfied: packaging>=20.0 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from matplotlib->catboost==1.2.5) (21.3)\nRequirement already satisfied: contourpy>=1.0.1 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from matplotlib->catboost==1.2.5) (1.0.5)\nRequirement already satisfied: cycler>=0.10 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from matplotlib->catboost==1.2.5) (0.11.0)\nRequirement already satisfied: pillow>=6.2.0 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from matplotlib->catboost==1.2.5) (9.2.0)\nRequirement already satisfied: fonttools>=4.22.0 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from matplotlib->catboost==1.2.5) (4.37.4)\nRequirement already satisfied: pyparsing>=2.2.1 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from matplotlib->catboost==1.2.5) (3.0.9)\nRequirement already satisfied: tenacity>=6.2.0 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from plotly->catboost==1.2.5) (8.1.0)\nInstalling collected packages: graphviz, catboost\nSuccessfully installed catboost-1.2.5 graphviz-0.20.3\n\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/ba456adc-bb1c-459b-95e0-4f7566e0455c","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1715204867925,"execution_millis":8659,"deepnote_to_be_reexecuted":false,"cell_id":"956b874714b743c4818176454c1103ff","deepnote_cell_type":"code"},"source":"!pip install xgboost==2.0.3","block_group":"87b0286d38d5476f93f4520fa914b8ff","execution_count":null,"outputs":[{"name":"stdout","text":"Collecting xgboost==2.0.3\n  Downloading xgboost-2.0.3-py3-none-manylinux2014_x86_64.whl (297.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.1/297.1 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy in /shared-libs/python3.9/py/lib/python3.9/site-packages (from xgboost==2.0.3) (1.23.4)\nRequirement already satisfied: scipy in /shared-libs/python3.9/py/lib/python3.9/site-packages (from xgboost==2.0.3) (1.9.3)\nInstalling collected packages: xgboost\nSuccessfully installed xgboost-2.0.3\n\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/7f8fb26f-3ceb-4b11-8700-3f688f8907c3","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1715204876594,"execution_millis":751,"deepnote_to_be_reexecuted":false,"cell_id":"63a45bbd14d04d59acabf3d336b8f13e","deepnote_cell_type":"code"},"source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport lightgbm as lgb\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nimport numpy as np\nfrom sklearn.model_selection import GridSearchCV, train_test_split, StratifiedKFold\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score\n","block_group":"6d67ae98cbb84c27b03fe6f9075b0099","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"f4da06631846417cb8e2818869343d5e","deepnote_cell_type":"text-cell-h1"},"source":"# Bring the Data In","block_group":"0c79d79c638f4ac7a0e36c426c763934"},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1715204877350,"execution_millis":484,"deepnote_to_be_reexecuted":false,"cell_id":"b17e09e9ccdc414c8ce24cb89d9a888b","deepnote_cell_type":"code"},"source":"X_df = pd.read_csv('spaceship_train_X_v2.csv')\ny_df = pd.read_csv('spaceship_train_y.csv')\nX_submission = pd.read_csv('spaceship_test_X_v2.csv')\n\ny_df.drop('Unnamed: 0', axis=1, inplace=True)\nX_df.head()","block_group":"9acd7481bdec43f3ba43972876e9aaa0","execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"application/vnd.deepnote.dataframe.v3+json":{"column_count":39,"row_count":5,"columns":[{"name":"PassengerId","dtype":"object","stats":{"unique_count":5,"nan_count":0,"categories":[{"name":"0001_01","count":1},{"name":"0002_01","count":1},{"name":"3 others","count":3}]}},{"name":"CryoSleep","dtype":"int64","stats":{"unique_count":1,"nan_count":0,"min":"0","max":"0","histogram":[{"bin_start":-0.5,"bin_end":-0.4,"count":0},{"bin_start":-0.4,"bin_end":-0.3,"count":0},{"bin_start":-0.3,"bin_end":-0.19999999999999996,"count":0},{"bin_start":-0.19999999999999996,"bin_end":-0.09999999999999998,"count":0},{"bin_start":-0.09999999999999998,"bin_end":0,"count":0},{"bin_start":0,"bin_end":0.10000000000000009,"count":5},{"bin_start":0.10000000000000009,"bin_end":0.20000000000000007,"count":0},{"bin_start":0.20000000000000007,"bin_end":0.30000000000000004,"count":0},{"bin_start":0.30000000000000004,"bin_end":0.4,"count":0},{"bin_start":0.4,"bin_end":0.5,"count":0}]}},{"name":"VIP","dtype":"int64","stats":{"unique_count":2,"nan_count":0,"min":"0","max":"1","histogram":[{"bin_start":0,"bin_end":0.1,"count":4},{"bin_start":0.1,"bin_end":0.2,"count":0},{"bin_start":0.2,"bin_end":0.30000000000000004,"count":0},{"bin_start":0.30000000000000004,"bin_end":0.4,"count":0},{"bin_start":0.4,"bin_end":0.5,"count":0},{"bin_start":0.5,"bin_end":0.6000000000000001,"count":0},{"bin_start":0.6000000000000001,"bin_end":0.7000000000000001,"count":0},{"bin_start":0.7000000000000001,"bin_end":0.8,"count":0},{"bin_start":0.8,"bin_end":0.9,"count":0},{"bin_start":0.9,"bin_end":1,"count":1}]}},{"name":"RoomService","dtype":"float64","stats":{"unique_count":4,"nan_count":0,"min":"-0.3375298928969419","max":"0.1213487208940084","histogram":[{"bin_start":-0.3375298928969419,"bin_end":-0.2916420315178469,"count":2},{"bin_start":-0.2916420315178469,"bin_end":-0.24575417013875184,"count":1},{"bin_start":-0.24575417013875184,"bin_end":-0.19986630875965683,"count":0},{"bin_start":-0.19986630875965683,"bin_end":-0.1539784473805618,"count":1},{"bin_start":-0.1539784473805618,"bin_end":-0.10809058600146676,"count":0},{"bin_start":-0.10809058600146676,"bin_end":-0.06220272462237175,"count":0},{"bin_start":-0.06220272462237175,"bin_end":-0.01631486324327669,"count":0},{"bin_start":-0.01631486324327669,"bin_end":0.029572998135818318,"count":0},{"bin_start":0.029572998135818318,"bin_end":0.07546085951491333,"count":0},{"bin_start":0.07546085951491333,"bin_end":0.1213487208940084,"count":1}]}},{"name":"FoodCourt","dtype":"float64","stats":{"unique_count":5,"nan_count":0,"min":"-0.2838651282377854","max":"1.9566433495195388","histogram":[{"bin_start":-0.2838651282377854,"bin_end":-0.059814280462053004,"count":3},{"bin_start":-0.059814280462053004,"bin_end":0.16423656731367942,"count":0},{"bin_start":0.16423656731367942,"bin_end":0.3882874150894119,"count":0},{"bin_start":0.3882874150894119,"bin_end":0.6123382628651443,"count":1},{"bin_start":0.6123382628651443,"bin_end":0.8363891106408766,"count":0},{"bin_start":0.8363891106408766,"bin_end":1.0604399584166093,"count":0},{"bin_start":1.0604399584166093,"bin_end":1.2844908061923417,"count":0},{"bin_start":1.2844908061923417,"bin_end":1.508541653968074,"count":0},{"bin_start":1.508541653968074,"bin_end":1.7325925017438064,"count":0},{"bin_start":1.7325925017438064,"bin_end":1.9566433495195388,"count":1}]}},{"name":"ShoppingMall","dtype":"float64","stats":{"unique_count":4,"nan_count":0,"min":"-0.2873829429551493","max":"0.3332397845143109","histogram":[{"bin_start":-0.2873829429551493,"bin_end":-0.2253206702082033,"count":3},{"bin_start":-0.2253206702082033,"bin_end":-0.16325839746125728,"count":0},{"bin_start":-0.16325839746125728,"bin_end":-0.10119612471431128,"count":0},{"bin_start":-0.10119612471431128,"bin_end":-0.03913385196736524,"count":0},{"bin_start":-0.03913385196736524,"bin_end":0.022928420779580794,"count":1},{"bin_start":0.022928420779580794,"bin_end":0.08499069352652677,"count":0},{"bin_start":0.08499069352652677,"bin_end":0.1470529662734728,"count":0},{"bin_start":0.1470529662734728,"bin_end":0.20911523902041884,"count":0},{"bin_start":0.20911523902041884,"bin_end":0.2711775117673648,"count":0},{"bin_start":0.2711775117673648,"bin_end":0.3332397845143109,"count":1}]}},{"name":"Spa","dtype":"float64","stats":{"unique_count":5,"nan_count":0,"min":"-0.2738262777680785","max":"5.692511954386446","histogram":[{"bin_start":-0.2738262777680785,"bin_end":0.32280754544737394,"count":3},{"bin_start":0.32280754544737394,"bin_end":0.9194413686628264,"count":0},{"bin_start":0.9194413686628264,"bin_end":1.5160751918782789,"count":0},{"bin_start":1.5160751918782789,"bin_end":2.1127090150937313,"count":0},{"bin_start":2.1127090150937313,"bin_end":2.7093428383091838,"count":1},{"bin_start":2.7093428383091838,"bin_end":3.305976661524636,"count":0},{"bin_start":3.305976661524636,"bin_end":3.902610484740089,"count":0},{"bin_start":3.902610484740089,"bin_end":4.499244307955541,"count":0},{"bin_start":4.499244307955541,"bin_end":5.095878131170993,"count":0},{"bin_start":5.095878131170993,"bin_end":5.692511954386446,"count":1}]}},{"name":"VRDeck","dtype":"float64","stats":{"unique_count":5,"nan_count":0,"min":"-0.2658309074152016","max":"-0.0956509707084443","histogram":[{"bin_start":-0.2658309074152016,"bin_end":-0.24881291374452588,"count":2},{"bin_start":-0.24881291374452588,"bin_end":-0.23179492007385014,"count":0},{"bin_start":-0.23179492007385014,"bin_end":-0.21477692640317442,"count":2},{"bin_start":-0.21477692640317442,"bin_end":-0.19775893273249867,"count":0},{"bin_start":-0.19775893273249867,"bin_end":-0.18074093906182295,"count":0},{"bin_start":-0.18074093906182295,"bin_end":-0.16372294539114723,"count":0},{"bin_start":-0.16372294539114723,"bin_end":-0.1467049517204715,"count":0},{"bin_start":-0.1467049517204715,"bin_end":-0.12968695804979577,"count":0},{"bin_start":-0.12968695804979577,"bin_end":-0.11266896437912005,"count":0},{"bin_start":-0.11266896437912005,"bin_end":-0.0956509707084443,"count":1}]}},{"name":"Expenditure","dtype":"float64","stats":{"unique_count":5,"nan_count":0,"min":"-0.5183569229651437","max":"3.1745957912176115","histogram":[{"bin_start":-0.5183569229651437,"bin_end":-0.14906165154686818,"count":2},{"bin_start":-0.14906165154686818,"bin_end":0.22023361987140733,"count":1},{"bin_start":0.22023361987140733,"bin_end":0.5895288912896828,"count":0},{"bin_start":0.5895288912896828,"bin_end":0.9588241627079583,"count":0},{"bin_start":0.9588241627079583,"bin_end":1.328119434126234,"count":1},{"bin_start":1.328119434126234,"bin_end":1.6974147055445092,"count":0},{"bin_start":1.6974147055445092,"bin_end":2.066709976962785,"count":0},{"bin_start":2.066709976962785,"bin_end":2.4360052483810604,"count":0},{"bin_start":2.4360052483810604,"bin_end":2.805300519799336,"count":0},{"bin_start":2.805300519799336,"bin_end":3.1745957912176115,"count":1}]}},{"name":"NoSpending","dtype":"int64","stats":{"unique_count":2,"nan_count":0,"min":"0","max":"1","histogram":[{"bin_start":0,"bin_end":0.1,"count":4},{"bin_start":0.1,"bin_end":0.2,"count":0},{"bin_start":0.2,"bin_end":0.30000000000000004,"count":0},{"bin_start":0.30000000000000004,"bin_end":0.4,"count":0},{"bin_start":0.4,"bin_end":0.5,"count":0},{"bin_start":0.5,"bin_end":0.6000000000000001,"count":0},{"bin_start":0.6000000000000001,"bin_end":0.7000000000000001,"count":0},{"bin_start":0.7000000000000001,"bin_end":0.8,"count":0},{"bin_start":0.8,"bin_end":0.9,"count":0},{"bin_start":0.9,"bin_end":1,"count":1}]}},{"name":"Group","dtype":"int64","stats":{"unique_count":4,"nan_count":0,"min":"1","max":"4","histogram":[{"bin_start":1,"bin_end":1.3,"count":1},{"bin_start":1.3,"bin_end":1.6,"count":0},{"bin_start":1.6,"bin_end":1.9,"count":0},{"bin_start":1.9,"bin_end":2.2,"count":1},{"bin_start":2.2,"bin_end":2.5,"count":0},{"bin_start":2.5,"bin_end":2.8,"count":0},{"bin_start":2.8,"bin_end":3.1,"count":2},{"bin_start":3.1,"bin_end":3.4,"count":0},{"bin_start":3.4,"bin_end":3.6999999999999997,"count":0},{"bin_start":3.6999999999999997,"bin_end":4,"count":1}]}},{"name":"GroupSize","dtype":"int64","stats":{"unique_count":2,"nan_count":0,"min":"1","max":"2","histogram":[{"bin_start":1,"bin_end":1.1,"count":3},{"bin_start":1.1,"bin_end":1.2,"count":0},{"bin_start":1.2,"bin_end":1.3,"count":0},{"bin_start":1.3,"bin_end":1.4,"count":0},{"bin_start":1.4,"bin_end":1.5,"count":0},{"bin_start":1.5,"bin_end":1.6,"count":0},{"bin_start":1.6,"bin_end":1.7000000000000002,"count":0},{"bin_start":1.7000000000000002,"bin_end":1.8,"count":0},{"bin_start":1.8,"bin_end":1.9,"count":0},{"bin_start":1.9,"bin_end":2,"count":2}]}},{"name":"Solo","dtype":"int64","stats":{"unique_count":2,"nan_count":0,"min":"0","max":"1","histogram":[{"bin_start":0,"bin_end":0.1,"count":2},{"bin_start":0.1,"bin_end":0.2,"count":0},{"bin_start":0.2,"bin_end":0.30000000000000004,"count":0},{"bin_start":0.30000000000000004,"bin_end":0.4,"count":0},{"bin_start":0.4,"bin_end":0.5,"count":0},{"bin_start":0.5,"bin_end":0.6000000000000001,"count":0},{"bin_start":0.6000000000000001,"bin_end":0.7000000000000001,"count":0},{"bin_start":0.7000000000000001,"bin_end":0.8,"count":0},{"bin_start":0.8,"bin_end":0.9,"count":0},{"bin_start":0.9,"bin_end":1,"count":3}]}},{"name":"CabinRegion_1","dtype":"int64","stats":{"unique_count":1,"nan_count":0,"min":"1","max":"1","histogram":[{"bin_start":0.5,"bin_end":0.6,"count":0},{"bin_start":0.6,"bin_end":0.7,"count":0},{"bin_start":0.7,"bin_end":0.8,"count":0},{"bin_start":0.8,"bin_end":0.9,"count":0},{"bin_start":0.9,"bin_end":1,"count":0},{"bin_start":1,"bin_end":1.1,"count":5},{"bin_start":1.1,"bin_end":1.2000000000000002,"count":0},{"bin_start":1.2000000000000002,"bin_end":1.3,"count":0},{"bin_start":1.3,"bin_end":1.4,"count":0},{"bin_start":1.4,"bin_end":1.5,"count":0}]}},{"name":"CabinRegion_2","dtype":"int64","stats":{"unique_count":1,"nan_count":0,"min":"0","max":"0","histogram":[{"bin_start":-0.5,"bin_end":-0.4,"count":0},{"bin_start":-0.4,"bin_end":-0.3,"count":0},{"bin_start":-0.3,"bin_end":-0.19999999999999996,"count":0},{"bin_start":-0.19999999999999996,"bin_end":-0.09999999999999998,"count":0},{"bin_start":-0.09999999999999998,"bin_end":0,"count":0},{"bin_start":0,"bin_end":0.10000000000000009,"count":5},{"bin_start":0.10000000000000009,"bin_end":0.20000000000000007,"count":0},{"bin_start":0.20000000000000007,"bin_end":0.30000000000000004,"count":0},{"bin_start":0.30000000000000004,"bin_end":0.4,"count":0},{"bin_start":0.4,"bin_end":0.5,"count":0}]}},{"name":"CabinRegion_3","dtype":"int64","stats":{"unique_count":1,"nan_count":0,"min":"0","max":"0","histogram":[{"bin_start":-0.5,"bin_end":-0.4,"count":0},{"bin_start":-0.4,"bin_end":-0.3,"count":0},{"bin_start":-0.3,"bin_end":-0.19999999999999996,"count":0},{"bin_start":-0.19999999999999996,"bin_end":-0.09999999999999998,"count":0},{"bin_start":-0.09999999999999998,"bin_end":0,"count":0},{"bin_start":0,"bin_end":0.10000000000000009,"count":5},{"bin_start":0.10000000000000009,"bin_end":0.20000000000000007,"count":0},{"bin_start":0.20000000000000007,"bin_end":0.30000000000000004,"count":0},{"bin_start":0.30000000000000004,"bin_end":0.4,"count":0},{"bin_start":0.4,"bin_end":0.5,"count":0}]}},{"name":"CabinRegion_4","dtype":"int64","stats":{"unique_count":1,"nan_count":0,"min":"0","max":"0","histogram":[{"bin_start":-0.5,"bin_end":-0.4,"count":0},{"bin_start":-0.4,"bin_end":-0.3,"count":0},{"bin_start":-0.3,"bin_end":-0.19999999999999996,"count":0},{"bin_start":-0.19999999999999996,"bin_end":-0.09999999999999998,"count":0},{"bin_start":-0.09999999999999998,"bin_end":0,"count":0},{"bin_start":0,"bin_end":0.10000000000000009,"count":5},{"bin_start":0.10000000000000009,"bin_end":0.20000000000000007,"count":0},{"bin_start":0.20000000000000007,"bin_end":0.30000000000000004,"count":0},{"bin_start":0.30000000000000004,"bin_end":0.4,"count":0},{"bin_start":0.4,"bin_end":0.5,"count":0}]}},{"name":"CabinRegion_5","dtype":"int64","stats":{"unique_count":1,"nan_count":0,"min":"0","max":"0","histogram":[{"bin_start":-0.5,"bin_end":-0.4,"count":0},{"bin_start":-0.4,"bin_end":-0.3,"count":0},{"bin_start":-0.3,"bin_end":-0.19999999999999996,"count":0},{"bin_start":-0.19999999999999996,"bin_end":-0.09999999999999998,"count":0},{"bin_start":-0.09999999999999998,"bin_end":0,"count":0},{"bin_start":0,"bin_end":0.10000000000000009,"count":5},{"bin_start":0.10000000000000009,"bin_end":0.20000000000000007,"count":0},{"bin_start":0.20000000000000007,"bin_end":0.30000000000000004,"count":0},{"bin_start":0.30000000000000004,"bin_end":0.4,"count":0},{"bin_start":0.4,"bin_end":0.5,"count":0}]}},{"name":"CabinRegion_6","dtype":"int64","stats":{"unique_count":1,"nan_count":0,"min":"0","max":"0","histogram":[{"bin_start":-0.5,"bin_end":-0.4,"count":0},{"bin_start":-0.4,"bin_end":-0.3,"count":0},{"bin_start":-0.3,"bin_end":-0.19999999999999996,"count":0},{"bin_start":-0.19999999999999996,"bin_end":-0.09999999999999998,"count":0},{"bin_start":-0.09999999999999998,"bin_end":0,"count":0},{"bin_start":0,"bin_end":0.10000000000000009,"count":5},{"bin_start":0.10000000000000009,"bin_end":0.20000000000000007,"count":0},{"bin_start":0.20000000000000007,"bin_end":0.30000000000000004,"count":0},{"bin_start":0.30000000000000004,"bin_end":0.4,"count":0},{"bin_start":0.4,"bin_end":0.5,"count":0}]}},{"name":"CabinRegion_7","dtype":"int64","stats":{"unique_count":1,"nan_count":0,"min":"0","max":"0","histogram":[{"bin_start":-0.5,"bin_end":-0.4,"count":0},{"bin_start":-0.4,"bin_end":-0.3,"count":0},{"bin_start":-0.3,"bin_end":-0.19999999999999996,"count":0},{"bin_start":-0.19999999999999996,"bin_end":-0.09999999999999998,"count":0},{"bin_start":-0.09999999999999998,"bin_end":0,"count":0},{"bin_start":0,"bin_end":0.10000000000000009,"count":5},{"bin_start":0.10000000000000009,"bin_end":0.20000000000000007,"count":0},{"bin_start":0.20000000000000007,"bin_end":0.30000000000000004,"count":0},{"bin_start":0.30000000000000004,"bin_end":0.4,"count":0},{"bin_start":0.4,"bin_end":0.5,"count":0}]}},{"name":"FamilySize","dtype":"int64","stats":{"unique_count":3,"nan_count":0,"min":"4","max":"9","histogram":[{"bin_start":4,"bin_end":4.5,"count":2},{"bin_start":4.5,"bin_end":5,"count":0},{"bin_start":5,"bin_end":5.5,"count":0},{"bin_start":5.5,"bin_end":6,"count":0},{"bin_start":6,"bin_end":6.5,"count":0},{"bin_start":6.5,"bin_end":7,"count":0},{"bin_start":7,"bin_end":7.5,"count":2},{"bin_start":7.5,"bin_end":8,"count":0},{"bin_start":8,"bin_end":8.5,"count":0},{"bin_start":8.5,"bin_end":9,"count":1}]}},{"name":"HomePlanet_Earth","dtype":"int64","stats":{"unique_count":2,"nan_count":0,"min":"0","max":"1","histogram":[{"bin_start":0,"bin_end":0.1,"count":3},{"bin_start":0.1,"bin_end":0.2,"count":0},{"bin_start":0.2,"bin_end":0.30000000000000004,"count":0},{"bin_start":0.30000000000000004,"bin_end":0.4,"count":0},{"bin_start":0.4,"bin_end":0.5,"count":0},{"bin_start":0.5,"bin_end":0.6000000000000001,"count":0},{"bin_start":0.6000000000000001,"bin_end":0.7000000000000001,"count":0},{"bin_start":0.7000000000000001,"bin_end":0.8,"count":0},{"bin_start":0.8,"bin_end":0.9,"count":0},{"bin_start":0.9,"bin_end":1,"count":2}]}},{"name":"HomePlanet_Europa","dtype":"int64","stats":{"unique_count":2,"nan_count":0,"min":"0","max":"1","histogram":[{"bin_start":0,"bin_end":0.1,"count":2},{"bin_start":0.1,"bin_end":0.2,"count":0},{"bin_start":0.2,"bin_end":0.30000000000000004,"count":0},{"bin_start":0.30000000000000004,"bin_end":0.4,"count":0},{"bin_start":0.4,"bin_end":0.5,"count":0},{"bin_start":0.5,"bin_end":0.6000000000000001,"count":0},{"bin_start":0.6000000000000001,"bin_end":0.7000000000000001,"count":0},{"bin_start":0.7000000000000001,"bin_end":0.8,"count":0},{"bin_start":0.8,"bin_end":0.9,"count":0},{"bin_start":0.9,"bin_end":1,"count":3}]}},{"name":"HomePlanet_Mars","dtype":"int64","stats":{"unique_count":1,"nan_count":0,"min":"0","max":"0","histogram":[{"bin_start":-0.5,"bin_end":-0.4,"count":0},{"bin_start":-0.4,"bin_end":-0.3,"count":0},{"bin_start":-0.3,"bin_end":-0.19999999999999996,"count":0},{"bin_start":-0.19999999999999996,"bin_end":-0.09999999999999998,"count":0},{"bin_start":-0.09999999999999998,"bin_end":0,"count":0},{"bin_start":0,"bin_end":0.10000000000000009,"count":5},{"bin_start":0.10000000000000009,"bin_end":0.20000000000000007,"count":0},{"bin_start":0.20000000000000007,"bin_end":0.30000000000000004,"count":0},{"bin_start":0.30000000000000004,"bin_end":0.4,"count":0},{"bin_start":0.4,"bin_end":0.5,"count":0}]}},{"name":"Destination_55 Cancri e","dtype":"int64","stats":{"unique_count":1,"nan_count":0,"min":"0","max":"0","histogram":[{"bin_start":-0.5,"bin_end":-0.4,"count":0},{"bin_start":-0.4,"bin_end":-0.3,"count":0},{"bin_start":-0.3,"bin_end":-0.19999999999999996,"count":0},{"bin_start":-0.19999999999999996,"bin_end":-0.09999999999999998,"count":0},{"bin_start":-0.09999999999999998,"bin_end":0,"count":0},{"bin_start":0,"bin_end":0.10000000000000009,"count":5},{"bin_start":0.10000000000000009,"bin_end":0.20000000000000007,"count":0},{"bin_start":0.20000000000000007,"bin_end":0.30000000000000004,"count":0},{"bin_start":0.30000000000000004,"bin_end":0.4,"count":0},{"bin_start":0.4,"bin_end":0.5,"count":0}]}},{"name":"Destination_PSO J318.5-22","dtype":"int64","stats":{"unique_count":1,"nan_count":0,"min":"0","max":"0","histogram":[{"bin_start":-0.5,"bin_end":-0.4,"count":0},{"bin_start":-0.4,"bin_end":-0.3,"count":0},{"bin_start":-0.3,"bin_end":-0.19999999999999996,"count":0},{"bin_start":-0.19999999999999996,"bin_end":-0.09999999999999998,"count":0},{"bin_start":-0.09999999999999998,"bin_end":0,"count":0},{"bin_start":0,"bin_end":0.10000000000000009,"count":5},{"bin_start":0.10000000000000009,"bin_end":0.20000000000000007,"count":0},{"bin_start":0.20000000000000007,"bin_end":0.30000000000000004,"count":0},{"bin_start":0.30000000000000004,"bin_end":0.4,"count":0},{"bin_start":0.4,"bin_end":0.5,"count":0}]}},{"name":"Destination_TRAPPIST-1e","dtype":"int64","stats":{"unique_count":1,"nan_count":0,"min":"1","max":"1","histogram":[{"bin_start":0.5,"bin_end":0.6,"count":0},{"bin_start":0.6,"bin_end":0.7,"count":0},{"bin_start":0.7,"bin_end":0.8,"count":0},{"bin_start":0.8,"bin_end":0.9,"count":0},{"bin_start":0.9,"bin_end":1,"count":0},{"bin_start":1,"bin_end":1.1,"count":5},{"bin_start":1.1,"bin_end":1.2000000000000002,"count":0},{"bin_start":1.2000000000000002,"bin_end":1.3,"count":0},{"bin_start":1.3,"bin_end":1.4,"count":0},{"bin_start":1.4,"bin_end":1.5,"count":0}]}},{"name":"CabinSide_P","dtype":"int64","stats":{"unique_count":2,"nan_count":0,"min":"0","max":"1","histogram":[{"bin_start":0,"bin_end":0.1,"count":4},{"bin_start":0.1,"bin_end":0.2,"count":0},{"bin_start":0.2,"bin_end":0.30000000000000004,"count":0},{"bin_start":0.30000000000000004,"bin_end":0.4,"count":0},{"bin_start":0.4,"bin_end":0.5,"count":0},{"bin_start":0.5,"bin_end":0.6000000000000001,"count":0},{"bin_start":0.6000000000000001,"bin_end":0.7000000000000001,"count":0},{"bin_start":0.7000000000000001,"bin_end":0.8,"count":0},{"bin_start":0.8,"bin_end":0.9,"count":0},{"bin_start":0.9,"bin_end":1,"count":1}]}},{"name":"CabinSide_S","dtype":"int64","stats":{"unique_count":2,"nan_count":0,"min":"0","max":"1","histogram":[{"bin_start":0,"bin_end":0.1,"count":1},{"bin_start":0.1,"bin_end":0.2,"count":0},{"bin_start":0.2,"bin_end":0.30000000000000004,"count":0},{"bin_start":0.30000000000000004,"bin_end":0.4,"count":0},{"bin_start":0.4,"bin_end":0.5,"count":0},{"bin_start":0.5,"bin_end":0.6000000000000001,"count":0},{"bin_start":0.6000000000000001,"bin_end":0.7000000000000001,"count":0},{"bin_start":0.7000000000000001,"bin_end":0.8,"count":0},{"bin_start":0.8,"bin_end":0.9,"count":0},{"bin_start":0.9,"bin_end":1,"count":4}]}},{"name":"CabinSide_Z","dtype":"int64","stats":{"unique_count":1,"nan_count":0,"min":"0","max":"0","histogram":[{"bin_start":-0.5,"bin_end":-0.4,"count":0},{"bin_start":-0.4,"bin_end":-0.3,"count":0},{"bin_start":-0.3,"bin_end":-0.19999999999999996,"count":0},{"bin_start":-0.19999999999999996,"bin_end":-0.09999999999999998,"count":0},{"bin_start":-0.09999999999999998,"bin_end":0,"count":0},{"bin_start":0,"bin_end":0.10000000000000009,"count":5},{"bin_start":0.10000000000000009,"bin_end":0.20000000000000007,"count":0},{"bin_start":0.20000000000000007,"bin_end":0.30000000000000004,"count":0},{"bin_start":0.30000000000000004,"bin_end":0.4,"count":0},{"bin_start":0.4,"bin_end":0.5,"count":0}]}},{"name":"CabinDeck_A","dtype":"int64","stats":{"unique_count":2,"nan_count":0,"min":"0","max":"1","histogram":[{"bin_start":0,"bin_end":0.1,"count":3},{"bin_start":0.1,"bin_end":0.2,"count":0},{"bin_start":0.2,"bin_end":0.30000000000000004,"count":0},{"bin_start":0.30000000000000004,"bin_end":0.4,"count":0},{"bin_start":0.4,"bin_end":0.5,"count":0},{"bin_start":0.5,"bin_end":0.6000000000000001,"count":0},{"bin_start":0.6000000000000001,"bin_end":0.7000000000000001,"count":0},{"bin_start":0.7000000000000001,"bin_end":0.8,"count":0},{"bin_start":0.8,"bin_end":0.9,"count":0},{"bin_start":0.9,"bin_end":1,"count":2}]}},{"name":"CabinDeck_B","dtype":"int64","stats":{"unique_count":2,"nan_count":0,"min":"0","max":"1","histogram":[{"bin_start":0,"bin_end":0.1,"count":4},{"bin_start":0.1,"bin_end":0.2,"count":0},{"bin_start":0.2,"bin_end":0.30000000000000004,"count":0},{"bin_start":0.30000000000000004,"bin_end":0.4,"count":0},{"bin_start":0.4,"bin_end":0.5,"count":0},{"bin_start":0.5,"bin_end":0.6000000000000001,"count":0},{"bin_start":0.6000000000000001,"bin_end":0.7000000000000001,"count":0},{"bin_start":0.7000000000000001,"bin_end":0.8,"count":0},{"bin_start":0.8,"bin_end":0.9,"count":0},{"bin_start":0.9,"bin_end":1,"count":1}]}},{"name":"CabinDeck_C","dtype":"int64","stats":{"unique_count":1,"nan_count":0,"min":"0","max":"0","histogram":[{"bin_start":-0.5,"bin_end":-0.4,"count":0},{"bin_start":-0.4,"bin_end":-0.3,"count":0},{"bin_start":-0.3,"bin_end":-0.19999999999999996,"count":0},{"bin_start":-0.19999999999999996,"bin_end":-0.09999999999999998,"count":0},{"bin_start":-0.09999999999999998,"bin_end":0,"count":0},{"bin_start":0,"bin_end":0.10000000000000009,"count":5},{"bin_start":0.10000000000000009,"bin_end":0.20000000000000007,"count":0},{"bin_start":0.20000000000000007,"bin_end":0.30000000000000004,"count":0},{"bin_start":0.30000000000000004,"bin_end":0.4,"count":0},{"bin_start":0.4,"bin_end":0.5,"count":0}]}},{"name":"CabinDeck_D","dtype":"int64","stats":{"unique_count":1,"nan_count":0,"min":"0","max":"0","histogram":[{"bin_start":-0.5,"bin_end":-0.4,"count":0},{"bin_start":-0.4,"bin_end":-0.3,"count":0},{"bin_start":-0.3,"bin_end":-0.19999999999999996,"count":0},{"bin_start":-0.19999999999999996,"bin_end":-0.09999999999999998,"count":0},{"bin_start":-0.09999999999999998,"bin_end":0,"count":0},{"bin_start":0,"bin_end":0.10000000000000009,"count":5},{"bin_start":0.10000000000000009,"bin_end":0.20000000000000007,"count":0},{"bin_start":0.20000000000000007,"bin_end":0.30000000000000004,"count":0},{"bin_start":0.30000000000000004,"bin_end":0.4,"count":0},{"bin_start":0.4,"bin_end":0.5,"count":0}]}},{"name":"CabinDeck_E","dtype":"int64","stats":{"unique_count":1,"nan_count":0,"min":"0","max":"0","histogram":[{"bin_start":-0.5,"bin_end":-0.4,"count":0},{"bin_start":-0.4,"bin_end":-0.3,"count":0},{"bin_start":-0.3,"bin_end":-0.19999999999999996,"count":0},{"bin_start":-0.19999999999999996,"bin_end":-0.09999999999999998,"count":0},{"bin_start":-0.09999999999999998,"bin_end":0,"count":0},{"bin_start":0,"bin_end":0.10000000000000009,"count":5},{"bin_start":0.10000000000000009,"bin_end":0.20000000000000007,"count":0},{"bin_start":0.20000000000000007,"bin_end":0.30000000000000004,"count":0},{"bin_start":0.30000000000000004,"bin_end":0.4,"count":0},{"bin_start":0.4,"bin_end":0.5,"count":0}]}},{"name":"CabinDeck_F","dtype":"int64","stats":{"unique_count":2,"nan_count":0,"min":"0","max":"1","histogram":[{"bin_start":0,"bin_end":0.1,"count":3},{"bin_start":0.1,"bin_end":0.2,"count":0},{"bin_start":0.2,"bin_end":0.30000000000000004,"count":0},{"bin_start":0.30000000000000004,"bin_end":0.4,"count":0},{"bin_start":0.4,"bin_end":0.5,"count":0},{"bin_start":0.5,"bin_end":0.6000000000000001,"count":0},{"bin_start":0.6000000000000001,"bin_end":0.7000000000000001,"count":0},{"bin_start":0.7000000000000001,"bin_end":0.8,"count":0},{"bin_start":0.8,"bin_end":0.9,"count":0},{"bin_start":0.9,"bin_end":1,"count":2}]}},{"name":"CabinDeck_G","dtype":"int64","stats":{"unique_count":1,"nan_count":0,"min":"0","max":"0","histogram":[{"bin_start":-0.5,"bin_end":-0.4,"count":0},{"bin_start":-0.4,"bin_end":-0.3,"count":0},{"bin_start":-0.3,"bin_end":-0.19999999999999996,"count":0},{"bin_start":-0.19999999999999996,"bin_end":-0.09999999999999998,"count":0},{"bin_start":-0.09999999999999998,"bin_end":0,"count":0},{"bin_start":0,"bin_end":0.10000000000000009,"count":5},{"bin_start":0.10000000000000009,"bin_end":0.20000000000000007,"count":0},{"bin_start":0.20000000000000007,"bin_end":0.30000000000000004,"count":0},{"bin_start":0.30000000000000004,"bin_end":0.4,"count":0},{"bin_start":0.4,"bin_end":0.5,"count":0}]}},{"name":"CabinDeck_T","dtype":"int64","stats":{"unique_count":1,"nan_count":0,"min":"0","max":"0","histogram":[{"bin_start":-0.5,"bin_end":-0.4,"count":0},{"bin_start":-0.4,"bin_end":-0.3,"count":0},{"bin_start":-0.3,"bin_end":-0.19999999999999996,"count":0},{"bin_start":-0.19999999999999996,"bin_end":-0.09999999999999998,"count":0},{"bin_start":-0.09999999999999998,"bin_end":0,"count":0},{"bin_start":0,"bin_end":0.10000000000000009,"count":5},{"bin_start":0.10000000000000009,"bin_end":0.20000000000000007,"count":0},{"bin_start":0.20000000000000007,"bin_end":0.30000000000000004,"count":0},{"bin_start":0.30000000000000004,"bin_end":0.4,"count":0},{"bin_start":0.4,"bin_end":0.5,"count":0}]}},{"name":"AgeEncoded","dtype":"float64","stats":{"unique_count":4,"nan_count":0,"min":"1.0","max":"4.0","histogram":[{"bin_start":1,"bin_end":1.3,"count":1},{"bin_start":1.3,"bin_end":1.6,"count":0},{"bin_start":1.6,"bin_end":1.9,"count":0},{"bin_start":1.9,"bin_end":2.2,"count":2},{"bin_start":2.2,"bin_end":2.5,"count":0},{"bin_start":2.5,"bin_end":2.8,"count":0},{"bin_start":2.8,"bin_end":3.1,"count":1},{"bin_start":3.1,"bin_end":3.4,"count":0},{"bin_start":3.4,"bin_end":3.6999999999999997,"count":0},{"bin_start":3.6999999999999997,"bin_end":4,"count":1}]}},{"name":"_deepnote_index_column","dtype":"int64"}],"rows":[{"PassengerId":"0001_01","CryoSleep":0,"VIP":0,"RoomService":-0.3375298928969419,"FoodCourt":-0.2838651282377854,"ShoppingMall":-0.2873829429551493,"Spa":-0.2738262777680785,"VRDeck":-0.2658309074152016,"Expenditure":-0.5183569229651437,"NoSpending":1,"Group":1,"GroupSize":1,"Solo":1,"CabinRegion_1":1,"CabinRegion_2":0,"CabinRegion_3":0,"CabinRegion_4":0,"CabinRegion_5":0,"CabinRegion_6":0,"CabinRegion_7":0,"FamilySize":4,"HomePlanet_Earth":0,"HomePlanet_Europa":1,"HomePlanet_Mars":0,"Destination_55 Cancri e":0,"Destination_PSO J318.5-22":0,"Destination_TRAPPIST-1e":1,"CabinSide_P":1,"CabinSide_S":0,"CabinSide_Z":0,"CabinDeck_A":0,"CabinDeck_B":1,"CabinDeck_C":0,"CabinDeck_D":0,"CabinDeck_E":0,"CabinDeck_F":0,"CabinDeck_G":0,"CabinDeck_T":0,"AgeEncoded":3,"_deepnote_index_column":0},{"PassengerId":"0002_01","CryoSleep":0,"VIP":0,"RoomService":-0.1724547480018475,"FoodCourt":-0.2782262646192686,"ShoppingMall":-0.2455620044464256,"Spa":0.2139651875264611,"VRDeck":-0.2270334088913813,"Expenditure":-0.2565815981420186,"NoSpending":0,"Group":2,"GroupSize":1,"Solo":1,"CabinRegion_1":1,"CabinRegion_2":0,"CabinRegion_3":0,"CabinRegion_4":0,"CabinRegion_5":0,"CabinRegion_6":0,"CabinRegion_7":0,"FamilySize":4,"HomePlanet_Earth":1,"HomePlanet_Europa":0,"HomePlanet_Mars":0,"Destination_55 Cancri e":0,"Destination_PSO J318.5-22":0,"Destination_TRAPPIST-1e":1,"CabinSide_P":0,"CabinSide_S":1,"CabinSide_Z":0,"CabinDeck_A":0,"CabinDeck_B":0,"CabinDeck_C":0,"CabinDeck_D":0,"CabinDeck_E":0,"CabinDeck_F":1,"CabinDeck_G":0,"CabinDeck_T":0,"AgeEncoded":2,"_deepnote_index_column":1},{"PassengerId":"0003_01","CryoSleep":0,"VIP":1,"RoomService":-0.2724085054612624,"FoodCourt":1.9566433495195388,"ShoppingMall":-0.2873829429551493,"Spa":5.692511954386446,"VRDeck":-0.2226246022409472,"Expenditure":3.1745957912176115,"NoSpending":0,"Group":3,"GroupSize":2,"Solo":0,"CabinRegion_1":1,"CabinRegion_2":0,"CabinRegion_3":0,"CabinRegion_4":0,"CabinRegion_5":0,"CabinRegion_6":0,"CabinRegion_7":0,"FamilySize":7,"HomePlanet_Earth":0,"HomePlanet_Europa":1,"HomePlanet_Mars":0,"Destination_55 Cancri e":0,"Destination_PSO J318.5-22":0,"Destination_TRAPPIST-1e":1,"CabinSide_P":0,"CabinSide_S":1,"CabinSide_Z":0,"CabinDeck_A":1,"CabinDeck_B":0,"CabinDeck_C":0,"CabinDeck_D":0,"CabinDeck_E":0,"CabinDeck_F":0,"CabinDeck_G":0,"CabinDeck_T":0,"AgeEncoded":4,"_deepnote_index_column":2},{"PassengerId":"0003_02","CryoSleep":0,"VIP":0,"RoomService":-0.3375298928969419,"FoodCourt":0.5199862076018809,"ShoppingMall":0.3332397845143109,"Spa":2.6840203305479915,"VRDeck":-0.0956509707084443,"Expenditure":1.3226065026931382,"NoSpending":0,"Group":3,"GroupSize":2,"Solo":0,"CabinRegion_1":1,"CabinRegion_2":0,"CabinRegion_3":0,"CabinRegion_4":0,"CabinRegion_5":0,"CabinRegion_6":0,"CabinRegion_7":0,"FamilySize":7,"HomePlanet_Earth":0,"HomePlanet_Europa":1,"HomePlanet_Mars":0,"Destination_55 Cancri e":0,"Destination_PSO J318.5-22":0,"Destination_TRAPPIST-1e":1,"CabinSide_P":0,"CabinSide_S":1,"CabinSide_Z":0,"CabinDeck_A":1,"CabinDeck_B":0,"CabinDeck_C":0,"CabinDeck_D":0,"CabinDeck_E":0,"CabinDeck_F":0,"CabinDeck_G":0,"CabinDeck_T":0,"AgeEncoded":2,"_deepnote_index_column":3},{"PassengerId":"0004_01","CryoSleep":0,"VIP":0,"RoomService":0.1213487208940084,"FoodCourt":-0.2400073000937662,"ShoppingMall":-0.0347844743624579,"Spa":0.2281813322344987,"VRDeck":-0.264067384755028,"Expenditure":-0.1303176846743428,"NoSpending":0,"Group":4,"GroupSize":1,"Solo":1,"CabinRegion_1":1,"CabinRegion_2":0,"CabinRegion_3":0,"CabinRegion_4":0,"CabinRegion_5":0,"CabinRegion_6":0,"CabinRegion_7":0,"FamilySize":9,"HomePlanet_Earth":1,"HomePlanet_Europa":0,"HomePlanet_Mars":0,"Destination_55 Cancri e":0,"Destination_PSO J318.5-22":0,"Destination_TRAPPIST-1e":1,"CabinSide_P":0,"CabinSide_S":1,"CabinSide_Z":0,"CabinDeck_A":0,"CabinDeck_B":0,"CabinDeck_C":0,"CabinDeck_D":0,"CabinDeck_E":0,"CabinDeck_F":1,"CabinDeck_G":0,"CabinDeck_T":0,"AgeEncoded":1,"_deepnote_index_column":4}]},"text/plain":"  PassengerId  CryoSleep  VIP  RoomService  FoodCourt  ShoppingMall       Spa  \\\n0     0001_01          0    0    -0.337530  -0.283865     -0.287383 -0.273826   \n1     0002_01          0    0    -0.172455  -0.278226     -0.245562  0.213965   \n2     0003_01          0    1    -0.272409   1.956643     -0.287383  5.692512   \n3     0003_02          0    0    -0.337530   0.519986      0.333240  2.684020   \n4     0004_01          0    0     0.121349  -0.240007     -0.034784  0.228181   \n\n     VRDeck  Expenditure  NoSpending  ...  CabinSide_Z  CabinDeck_A  \\\n0 -0.265831    -0.518357           1  ...            0            0   \n1 -0.227033    -0.256582           0  ...            0            0   \n2 -0.222625     3.174596           0  ...            0            1   \n3 -0.095651     1.322607           0  ...            0            1   \n4 -0.264067    -0.130318           0  ...            0            0   \n\n   CabinDeck_B  CabinDeck_C  CabinDeck_D  CabinDeck_E  CabinDeck_F  \\\n0            1            0            0            0            0   \n1            0            0            0            0            1   \n2            0            0            0            0            0   \n3            0            0            0            0            0   \n4            0            0            0            0            1   \n\n   CabinDeck_G  CabinDeck_T  AgeEncoded  \n0            0            0         3.0  \n1            0            0         2.0  \n2            0            0         4.0  \n3            0            0         2.0  \n4            0            0         1.0  \n\n[5 rows x 39 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>CryoSleep</th>\n      <th>VIP</th>\n      <th>RoomService</th>\n      <th>FoodCourt</th>\n      <th>ShoppingMall</th>\n      <th>Spa</th>\n      <th>VRDeck</th>\n      <th>Expenditure</th>\n      <th>NoSpending</th>\n      <th>...</th>\n      <th>CabinSide_Z</th>\n      <th>CabinDeck_A</th>\n      <th>CabinDeck_B</th>\n      <th>CabinDeck_C</th>\n      <th>CabinDeck_D</th>\n      <th>CabinDeck_E</th>\n      <th>CabinDeck_F</th>\n      <th>CabinDeck_G</th>\n      <th>CabinDeck_T</th>\n      <th>AgeEncoded</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0001_01</td>\n      <td>0</td>\n      <td>0</td>\n      <td>-0.337530</td>\n      <td>-0.283865</td>\n      <td>-0.287383</td>\n      <td>-0.273826</td>\n      <td>-0.265831</td>\n      <td>-0.518357</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0002_01</td>\n      <td>0</td>\n      <td>0</td>\n      <td>-0.172455</td>\n      <td>-0.278226</td>\n      <td>-0.245562</td>\n      <td>0.213965</td>\n      <td>-0.227033</td>\n      <td>-0.256582</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0003_01</td>\n      <td>0</td>\n      <td>1</td>\n      <td>-0.272409</td>\n      <td>1.956643</td>\n      <td>-0.287383</td>\n      <td>5.692512</td>\n      <td>-0.222625</td>\n      <td>3.174596</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0003_02</td>\n      <td>0</td>\n      <td>0</td>\n      <td>-0.337530</td>\n      <td>0.519986</td>\n      <td>0.333240</td>\n      <td>2.684020</td>\n      <td>-0.095651</td>\n      <td>1.322607</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0004_01</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.121349</td>\n      <td>-0.240007</td>\n      <td>-0.034784</td>\n      <td>0.228181</td>\n      <td>-0.264067</td>\n      <td>-0.130318</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 39 columns</p>\n</div>"},"metadata":{}}],"outputs_reference":"s3:deepnote-cell-outputs-production/c3c9442a-aace-4b0a-9b31-715f1856c116","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1715204877839,"execution_millis":484,"deepnote_to_be_reexecuted":false,"cell_id":"326e66ab810649b58c3b22a7f271174d","deepnote_cell_type":"code"},"source":"from sklearn.model_selection import train_test_split\n\n# Memisahkan data menjadi train dan validation set\nX_train, X_val, y_train, y_val = train_test_split(X_df, y_df, test_size=0.2, random_state=42)\n# Mengubah target menjadi 1D array\ny_train = y_train['Transported'].values\ny_test = y_val['Transported'].values","block_group":"f3e98e79ef544fb98587f50a6297debd","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"fff84610e9e1492e836d0b2cdb78ebab","deepnote_cell_type":"text-cell-h1"},"source":"# Model Training","block_group":"e01f12f006324577a8685ffcb2e53c45"},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1715204877856,"execution_millis":467,"deepnote_to_be_reexecuted":false,"cell_id":"1224dbf4d21a469b975aa241ea38893f","deepnote_cell_type":"code"},"source":"base_estimators = [\n    ('xgb', XGBClassifier(use_label_encoder=False, eval_metric='logloss')),\n    ('lgbm', LGBMClassifier()),\n    ('catboost', CatBoostClassifier(verbose=0))\n]\n\n\nstacking_pipeline = Pipeline([\n    ('scaler', StandardScaler()),  # Opsional, tergantung pada kebutuhan preprocessing data Anda\n    ('stacking', StackingClassifier(\n        estimators=base_estimators, \n        final_estimator=LogisticRegression(),\n        passthrough=True\n    ))\n])\n\nparam_grid = {\n    'stacking__lgbm__n_estimators': [500],\n    'stacking__lgbm__max_depth': [10],\n    'stacking__lgbm__learning_rate': [0.01],\n    'stacking__lgbm__subsample': [0.8],\n    'stacking__lgbm__colsample_bytree': [1],\n    'stacking__xgb__n_estimators': [200],\n    'stacking__xgb__learning_rate': [0.1],\n    'stacking__xgb__max_depth': [5],\n    'stacking__xgb__colsample_bytree': [0.7],\n    'stacking__xgb__subsample': [0.8],\n    'stacking__catboost__iterations': [500],\n    'stacking__catboost__learning_rate': [0.01],\n    'stacking__catboost__depth': [10],\n    'stacking__catboost__l2_leaf_reg': [5],\n    'stacking__catboost__border_count': [64],\n    'stacking__catboost__loss_function': ['CrossEntropy']\n}\n\n# Menggunakan StratifiedKFold untuk handling imbalanced data\ncv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n","block_group":"ffa1a5a6dc0742f6a823ee9eafca1c28","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1715204877914,"execution_millis":587796,"deepnote_to_be_reexecuted":false,"cell_id":"9966cef9144d428e97b0f6adbfd3c4fe","deepnote_cell_type":"code"},"source":"# Membuat GridSearchCV\ngrid_search = GridSearchCV(stacking_pipeline, param_grid, cv=cv, scoring='accuracy', verbose=1)\ngrid_search.fit(X_train, y_train)\n\n# Menampilkan parameter terbaik dan skor\nprint(\"Best parameters:\", grid_search.best_params_)\nprint(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))","block_group":"f25cc19a7dc748ebb5ccf17f344e75e3","execution_count":null,"outputs":[{"name":"stdout","text":"Fitting 10 folds for each of 1 candidates, totalling 10 fits\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 3150, number of negative: 3108\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001367 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 6258, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503356 -> initscore=0.013423\n[LightGBM] [Info] Start training from score 0.013423\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 2520, number of negative: 2486\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000951 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 5006, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503396 -> initscore=0.013584\n[LightGBM] [Info] Start training from score 0.013584\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 2520, number of negative: 2486\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001060 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 5006, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503396 -> initscore=0.013584\n[LightGBM] [Info] Start training from score 0.013584\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 2520, number of negative: 2486\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000943 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 5006, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503396 -> initscore=0.013584\n[LightGBM] [Info] Start training from score 0.013584\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 2520, number of negative: 2487\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000971 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 5007, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503295 -> initscore=0.013182\n[LightGBM] [Info] Start training from score 0.013182\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 2520, number of negative: 2487\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001063 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 5007, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503295 -> initscore=0.013182\n[LightGBM] [Info] Start training from score 0.013182\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 3150, number of negative: 3108\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001352 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 6258, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503356 -> initscore=0.013423\n[LightGBM] [Info] Start training from score 0.013423\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 2520, number of negative: 2486\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001008 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 5006, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503396 -> initscore=0.013584\n[LightGBM] [Info] Start training from score 0.013584\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 2520, number of negative: 2486\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000917 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 5006, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503396 -> initscore=0.013584\n[LightGBM] [Info] Start training from score 0.013584\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 2520, number of negative: 2486\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000961 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 5006, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503396 -> initscore=0.013584\n[LightGBM] [Info] Start training from score 0.013584\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 2520, number of negative: 2487\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001069 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 5007, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503295 -> initscore=0.013182\n[LightGBM] [Info] Start training from score 0.013182\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 2520, number of negative: 2487\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001310 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2153\n[LightGBM] [Info] Number of data points in the train set: 5007, number of used features: 37\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503295 -> initscore=0.013182\n[LightGBM] [Info] Start training from score 0.013182\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 3150, number of negative: 3108\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001142 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 6258, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503356 -> initscore=0.013423\n[LightGBM] [Info] Start training from score 0.013423\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 2520, number of negative: 2486\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000987 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 5006, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503396 -> initscore=0.013584\n[LightGBM] [Info] Start training from score 0.013584\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 2520, number of negative: 2486\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000934 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 5006, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503396 -> initscore=0.013584\n[LightGBM] [Info] Start training from score 0.013584\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 2520, number of negative: 2486\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000944 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 5006, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503396 -> initscore=0.013584\n[LightGBM] [Info] Start training from score 0.013584\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 2520, number of negative: 2487\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000991 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 5007, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503295 -> initscore=0.013182\n[LightGBM] [Info] Start training from score 0.013182\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 2520, number of negative: 2487\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000951 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 5007, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503295 -> initscore=0.013182\n[LightGBM] [Info] Start training from score 0.013182\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 3150, number of negative: 3108\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001172 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 6258, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503356 -> initscore=0.013423\n[LightGBM] [Info] Start training from score 0.013423\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 2520, number of negative: 2486\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001119 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 5006, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503396 -> initscore=0.013584\n[LightGBM] [Info] Start training from score 0.013584\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 2520, number of negative: 2486\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000916 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 5006, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503396 -> initscore=0.013584\n[LightGBM] [Info] Start training from score 0.013584\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 2520, number of negative: 2486\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000990 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 5006, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503396 -> initscore=0.013584\n[LightGBM] [Info] Start training from score 0.013584\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 2520, number of negative: 2487\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000942 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 5007, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503295 -> initscore=0.013182\n[LightGBM] [Info] Start training from score 0.013182\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 2520, number of negative: 2487\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000943 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 5007, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503295 -> initscore=0.013182\n[LightGBM] [Info] Start training from score 0.013182\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 3150, number of negative: 3109\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001185 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 6259, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503275 -> initscore=0.013101\n[LightGBM] [Info] Start training from score 0.013101\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 2520, number of negative: 2487\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000997 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 5007, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503295 -> initscore=0.013182\n[LightGBM] [Info] Start training from score 0.013182\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 2520, number of negative: 2487\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000916 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 5007, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503295 -> initscore=0.013182\n[LightGBM] [Info] Start training from score 0.013182\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 2520, number of negative: 2487\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001171 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 5007, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503295 -> initscore=0.013182\n[LightGBM] [Info] Start training from score 0.013182\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 2520, number of negative: 2487\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000949 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 5007, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503295 -> initscore=0.013182\n[LightGBM] [Info] Start training from score 0.013182\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 2520, number of negative: 2488\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000946 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 5008, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503195 -> initscore=0.012780\n[LightGBM] [Info] Start training from score 0.012780\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 3150, number of negative: 3109\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001472 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 6259, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503275 -> initscore=0.013101\n[LightGBM] [Info] Start training from score 0.013101\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 2520, number of negative: 2487\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000982 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 5007, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503295 -> initscore=0.013182\n[LightGBM] [Info] Start training from score 0.013182\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 2520, number of negative: 2487\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000938 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 5007, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503295 -> initscore=0.013182\n[LightGBM] [Info] Start training from score 0.013182\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 2520, number of negative: 2487\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000998 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 5007, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503295 -> initscore=0.013182\n[LightGBM] [Info] Start training from score 0.013182\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 2520, number of negative: 2487\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000969 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 5007, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503295 -> initscore=0.013182\n[LightGBM] [Info] Start training from score 0.013182\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 2520, number of negative: 2488\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001331 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 5008, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503195 -> initscore=0.012780\n[LightGBM] [Info] Start training from score 0.012780\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 3150, number of negative: 3109\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001232 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 6259, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503275 -> initscore=0.013101\n[LightGBM] [Info] Start training from score 0.013101\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 2520, number of negative: 2487\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001023 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 5007, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503295 -> initscore=0.013182\n[LightGBM] [Info] Start training from score 0.013182\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 2520, number of negative: 2487\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000936 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 5007, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503295 -> initscore=0.013182\n[LightGBM] [Info] Start training from score 0.013182\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 2520, number of negative: 2487\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000974 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 5007, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503295 -> initscore=0.013182\n[LightGBM] [Info] Start training from score 0.013182\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 2520, number of negative: 2487\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000946 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 5007, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503295 -> initscore=0.013182\n[LightGBM] [Info] Start training from score 0.013182\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 2520, number of negative: 2488\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000916 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 5008, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503195 -> initscore=0.012780\n[LightGBM] [Info] Start training from score 0.012780\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 3150, number of negative: 3109\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001167 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 6259, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503275 -> initscore=0.013101\n[LightGBM] [Info] Start training from score 0.013101\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 2520, number of negative: 2487\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000983 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 5007, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503295 -> initscore=0.013182\n[LightGBM] [Info] Start training from score 0.013182\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 2520, number of negative: 2487\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000956 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 5007, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503295 -> initscore=0.013182\n[LightGBM] [Info] Start training from score 0.013182\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 2520, number of negative: 2487\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001120 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 5007, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503295 -> initscore=0.013182\n[LightGBM] [Info] Start training from score 0.013182\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 2520, number of negative: 2487\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001100 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 5007, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503295 -> initscore=0.013182\n[LightGBM] [Info] Start training from score 0.013182\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 2520, number of negative: 2488\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000918 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 5008, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503195 -> initscore=0.012780\n[LightGBM] [Info] Start training from score 0.012780\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 3150, number of negative: 3109\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001361 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 6259, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503275 -> initscore=0.013101\n[LightGBM] [Info] Start training from score 0.013101\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 2520, number of negative: 2487\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000998 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 5007, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503295 -> initscore=0.013182\n[LightGBM] [Info] Start training from score 0.013182\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 2520, number of negative: 2487\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001015 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 5007, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503295 -> initscore=0.013182\n[LightGBM] [Info] Start training from score 0.013182\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 2520, number of negative: 2487\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000945 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 5007, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503295 -> initscore=0.013182\n[LightGBM] [Info] Start training from score 0.013182\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 2520, number of negative: 2487\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000943 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 5007, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503295 -> initscore=0.013182\n[LightGBM] [Info] Start training from score 0.013182\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 2520, number of negative: 2488\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000995 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 5008, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503195 -> initscore=0.012780\n[LightGBM] [Info] Start training from score 0.012780\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 3150, number of negative: 3109\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001166 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 6259, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503275 -> initscore=0.013101\n[LightGBM] [Info] Start training from score 0.013101\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 2520, number of negative: 2487\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000959 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 5007, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503295 -> initscore=0.013182\n[LightGBM] [Info] Start training from score 0.013182\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 2520, number of negative: 2487\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000923 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 5007, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503295 -> initscore=0.013182\n[LightGBM] [Info] Start training from score 0.013182\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 2520, number of negative: 2487\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000947 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 5007, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503295 -> initscore=0.013182\n[LightGBM] [Info] Start training from score 0.013182\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 2520, number of negative: 2487\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000950 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 5007, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503295 -> initscore=0.013182\n[LightGBM] [Info] Start training from score 0.013182\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 2520, number of negative: 2488\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000966 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 5008, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503195 -> initscore=0.012780\n[LightGBM] [Info] Start training from score 0.012780\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 3500, number of negative: 3454\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001392 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 6954, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503307 -> initscore=0.013230\n[LightGBM] [Info] Start training from score 0.013230\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 2800, number of negative: 2763\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001095 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 5563, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503326 -> initscore=0.013302\n[LightGBM] [Info] Start training from score 0.013302\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 2800, number of negative: 2763\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001015 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 5563, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503326 -> initscore=0.013302\n[LightGBM] [Info] Start training from score 0.013302\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 2800, number of negative: 2763\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001067 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 5563, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503326 -> initscore=0.013302\n[LightGBM] [Info] Start training from score 0.013302\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 2800, number of negative: 2763\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001073 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 5563, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503326 -> initscore=0.013302\n[LightGBM] [Info] Start training from score 0.013302\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 2800, number of negative: 2764\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001015 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 5564, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503235 -> initscore=0.012941\n[LightGBM] [Info] Start training from score 0.012941\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\nBest parameters: {'stacking__catboost__border_count': 64, 'stacking__catboost__depth': 10, 'stacking__catboost__iterations': 500, 'stacking__catboost__l2_leaf_reg': 5, 'stacking__catboost__learning_rate': 0.01, 'stacking__catboost__loss_function': 'CrossEntropy', 'stacking__lgbm__colsample_bytree': 1, 'stacking__lgbm__learning_rate': 0.01, 'stacking__lgbm__max_depth': 10, 'stacking__lgbm__n_estimators': 500, 'stacking__lgbm__subsample': 0.8, 'stacking__xgb__colsample_bytree': 0.7, 'stacking__xgb__learning_rate': 0.1, 'stacking__xgb__max_depth': 5, 'stacking__xgb__n_estimators': 200, 'stacking__xgb__subsample': 0.8}\nBest cross-validation score: 0.81\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/2fd59d29-160e-48cd-97cc-f578d2fb9f5f","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1715205580406,"execution_millis":246,"deepnote_to_be_reexecuted":false,"cell_id":"e3107f6a15864c5a80ca3b30ddcff4b9","deepnote_cell_type":"code"},"source":"# Evaluasi pada validation set\ny_pred = grid_search.predict(X_val)\naccuracy = accuracy_score(y_val, y_pred)\nprint(classification_report(y_val, y_pred))\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_val, y_pred))","block_group":"1f877861536747488f387d65084fb3b1","execution_count":null,"outputs":[{"name":"stdout","text":"[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n              precision    recall  f1-score   support\n\n           0       0.81      0.77      0.79       861\n           1       0.78      0.82      0.80       878\n\n    accuracy                           0.79      1739\n   macro avg       0.79      0.79      0.79      1739\nweighted avg       0.79      0.79      0.79      1739\n\nConfusion Matrix:\n[[659 202]\n [156 722]]\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/4f04fdba-9e6d-44a7-985a-118881116074","content_dependencies":null},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"61a94ca029e440a0b64f8917e523d733","deepnote_cell_type":"text-cell-h1"},"source":"# Submission Prediction","block_group":"4c856cb090fc4ca8847589c39997e4a2"},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1715205602798,"execution_millis":632046,"deepnote_to_be_reexecuted":false,"cell_id":"ea1646d84ff14dc2a0345b6322477108","deepnote_cell_type":"code"},"source":"# For training, we use ALL data from spaceship_train_X_v2.csv and spaceship_train_y.csv\ngrid_search.fit(X_df, y_df['Transported'].values)\n# Prediksi data submission\ny_prediction = grid_search.predict(X_submission)\nprint(y_prediction)\n","block_group":"51e9e9b1a74b47e3beec7c41ad94151c","execution_count":null,"outputs":[{"name":"stdout","text":"Fitting 10 folds for each of 1 candidates, totalling 10 fits\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 3940, number of negative: 3883\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001385 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 7823, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503643 -> initscore=0.014573\n[LightGBM] [Info] Start training from score 0.014573\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 3152, number of negative: 3106\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001445 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 6258, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503675 -> initscore=0.014701\n[LightGBM] [Info] Start training from score 0.014701\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 3152, number of negative: 3106\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001151 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 6258, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503675 -> initscore=0.014701\n[LightGBM] [Info] Start training from score 0.014701\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 3152, number of negative: 3106\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001098 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 6258, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503675 -> initscore=0.014701\n[LightGBM] [Info] Start training from score 0.014701\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 3152, number of negative: 3107\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001121 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 6259, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503595 -> initscore=0.014380\n[LightGBM] [Info] Start training from score 0.014380\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 3152, number of negative: 3107\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001538 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2150\n[LightGBM] [Info] Number of data points in the train set: 6259, number of used features: 36\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503595 -> initscore=0.014380\n[LightGBM] [Info] Start training from score 0.014380\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 3940, number of negative: 3883\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001384 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 7823, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503643 -> initscore=0.014573\n[LightGBM] [Info] Start training from score 0.014573\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 3152, number of negative: 3106\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001163 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 6258, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503675 -> initscore=0.014701\n[LightGBM] [Info] Start training from score 0.014701\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 3152, number of negative: 3106\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001113 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 6258, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503675 -> initscore=0.014701\n[LightGBM] [Info] Start training from score 0.014701\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 3152, number of negative: 3106\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001493 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 6258, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503675 -> initscore=0.014701\n[LightGBM] [Info] Start training from score 0.014701\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 3152, number of negative: 3107\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001121 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 6259, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503595 -> initscore=0.014380\n[LightGBM] [Info] Start training from score 0.014380\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 3152, number of negative: 3107\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001095 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2150\n[LightGBM] [Info] Number of data points in the train set: 6259, number of used features: 36\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503595 -> initscore=0.014380\n[LightGBM] [Info] Start training from score 0.014380\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 3940, number of negative: 3883\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001869 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 7823, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503643 -> initscore=0.014573\n[LightGBM] [Info] Start training from score 0.014573\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 3152, number of negative: 3106\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001119 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 6258, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503675 -> initscore=0.014701\n[LightGBM] [Info] Start training from score 0.014701\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 3152, number of negative: 3106\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001240 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 6258, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503675 -> initscore=0.014701\n[LightGBM] [Info] Start training from score 0.014701\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 3152, number of negative: 3106\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001090 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 6258, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503675 -> initscore=0.014701\n[LightGBM] [Info] Start training from score 0.014701\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 3152, number of negative: 3107\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001082 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 6259, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503595 -> initscore=0.014380\n[LightGBM] [Info] Start training from score 0.014380\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 3152, number of negative: 3107\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001083 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2150\n[LightGBM] [Info] Number of data points in the train set: 6259, number of used features: 36\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503595 -> initscore=0.014380\n[LightGBM] [Info] Start training from score 0.014380\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 3941, number of negative: 3883\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001401 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 7824, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503707 -> initscore=0.014826\n[LightGBM] [Info] Start training from score 0.014826\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 3153, number of negative: 3106\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001122 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 6259, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503755 -> initscore=0.015019\n[LightGBM] [Info] Start training from score 0.015019\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 3153, number of negative: 3106\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001165 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 6259, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503755 -> initscore=0.015019\n[LightGBM] [Info] Start training from score 0.015019\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 3153, number of negative: 3106\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001591 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 6259, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503755 -> initscore=0.015019\n[LightGBM] [Info] Start training from score 0.015019\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 3152, number of negative: 3107\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001293 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 6259, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503595 -> initscore=0.014380\n[LightGBM] [Info] Start training from score 0.014380\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 3153, number of negative: 3107\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001440 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2153\n[LightGBM] [Info] Number of data points in the train set: 6260, number of used features: 37\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503674 -> initscore=0.014697\n[LightGBM] [Info] Start training from score 0.014697\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 3941, number of negative: 3883\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001444 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 7824, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503707 -> initscore=0.014826\n[LightGBM] [Info] Start training from score 0.014826\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 3153, number of negative: 3106\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001177 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 6259, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503755 -> initscore=0.015019\n[LightGBM] [Info] Start training from score 0.015019\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 3153, number of negative: 3106\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001120 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 6259, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503755 -> initscore=0.015019\n[LightGBM] [Info] Start training from score 0.015019\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 3153, number of negative: 3106\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001090 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 6259, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503755 -> initscore=0.015019\n[LightGBM] [Info] Start training from score 0.015019\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 3152, number of negative: 3107\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001118 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 6259, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503595 -> initscore=0.014380\n[LightGBM] [Info] Start training from score 0.014380\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 3153, number of negative: 3107\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001256 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2150\n[LightGBM] [Info] Number of data points in the train set: 6260, number of used features: 36\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503674 -> initscore=0.014697\n[LightGBM] [Info] Start training from score 0.014697\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 3940, number of negative: 3884\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001371 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 7824, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503579 -> initscore=0.014315\n[LightGBM] [Info] Start training from score 0.014315\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 3152, number of negative: 3107\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001165 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 6259, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503595 -> initscore=0.014380\n[LightGBM] [Info] Start training from score 0.014380\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 3152, number of negative: 3107\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001124 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 6259, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503595 -> initscore=0.014380\n[LightGBM] [Info] Start training from score 0.014380\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 3152, number of negative: 3107\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001145 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 6259, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503595 -> initscore=0.014380\n[LightGBM] [Info] Start training from score 0.014380\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 3152, number of negative: 3107\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001494 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 6259, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503595 -> initscore=0.014380\n[LightGBM] [Info] Start training from score 0.014380\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 3152, number of negative: 3108\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001152 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2153\n[LightGBM] [Info] Number of data points in the train set: 6260, number of used features: 37\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503514 -> initscore=0.014058\n[LightGBM] [Info] Start training from score 0.014058\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 3940, number of negative: 3884\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001382 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 7824, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503579 -> initscore=0.014315\n[LightGBM] [Info] Start training from score 0.014315\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 3152, number of negative: 3107\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001210 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 6259, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503595 -> initscore=0.014380\n[LightGBM] [Info] Start training from score 0.014380\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 3152, number of negative: 3107\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001120 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 6259, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503595 -> initscore=0.014380\n[LightGBM] [Info] Start training from score 0.014380\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 3152, number of negative: 3107\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001117 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 6259, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503595 -> initscore=0.014380\n[LightGBM] [Info] Start training from score 0.014380\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 3152, number of negative: 3107\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001109 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 6259, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503595 -> initscore=0.014380\n[LightGBM] [Info] Start training from score 0.014380\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 3152, number of negative: 3108\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001052 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2150\n[LightGBM] [Info] Number of data points in the train set: 6260, number of used features: 36\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503514 -> initscore=0.014058\n[LightGBM] [Info] Start training from score 0.014058\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 3940, number of negative: 3884\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001352 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 7824, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503579 -> initscore=0.014315\n[LightGBM] [Info] Start training from score 0.014315\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 3152, number of negative: 3107\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001137 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 6259, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503595 -> initscore=0.014380\n[LightGBM] [Info] Start training from score 0.014380\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 3152, number of negative: 3107\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001115 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 6259, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503595 -> initscore=0.014380\n[LightGBM] [Info] Start training from score 0.014380\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 3152, number of negative: 3107\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001074 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 6259, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503595 -> initscore=0.014380\n[LightGBM] [Info] Start training from score 0.014380\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 3152, number of negative: 3107\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001085 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 6259, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503595 -> initscore=0.014380\n[LightGBM] [Info] Start training from score 0.014380\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 3152, number of negative: 3108\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001071 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2150\n[LightGBM] [Info] Number of data points in the train set: 6260, number of used features: 36\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503514 -> initscore=0.014058\n[LightGBM] [Info] Start training from score 0.014058\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 3940, number of negative: 3884\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001418 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 7824, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503579 -> initscore=0.014315\n[LightGBM] [Info] Start training from score 0.014315\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 3152, number of negative: 3107\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001121 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 6259, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503595 -> initscore=0.014380\n[LightGBM] [Info] Start training from score 0.014380\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 3152, number of negative: 3107\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001157 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 6259, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503595 -> initscore=0.014380\n[LightGBM] [Info] Start training from score 0.014380\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 3152, number of negative: 3107\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001056 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 6259, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503595 -> initscore=0.014380\n[LightGBM] [Info] Start training from score 0.014380\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 3152, number of negative: 3107\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001098 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 6259, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503595 -> initscore=0.014380\n[LightGBM] [Info] Start training from score 0.014380\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 3152, number of negative: 3108\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001083 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2153\n[LightGBM] [Info] Number of data points in the train set: 6260, number of used features: 37\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503514 -> initscore=0.014058\n[LightGBM] [Info] Start training from score 0.014058\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 3940, number of negative: 3884\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001385 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 7824, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503579 -> initscore=0.014315\n[LightGBM] [Info] Start training from score 0.014315\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 3152, number of negative: 3107\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001179 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 6259, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503595 -> initscore=0.014380\n[LightGBM] [Info] Start training from score 0.014380\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 3152, number of negative: 3107\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001230 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 6259, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503595 -> initscore=0.014380\n[LightGBM] [Info] Start training from score 0.014380\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 3152, number of negative: 3107\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001323 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 6259, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503595 -> initscore=0.014380\n[LightGBM] [Info] Start training from score 0.014380\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 3152, number of negative: 3107\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001579 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 6259, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503595 -> initscore=0.014380\n[LightGBM] [Info] Start training from score 0.014380\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 3152, number of negative: 3108\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001078 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2150\n[LightGBM] [Info] Number of data points in the train set: 6260, number of used features: 36\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503514 -> initscore=0.014058\n[LightGBM] [Info] Start training from score 0.014058\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4378, number of negative: 4315\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001577 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 8693, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503624 -> initscore=0.014495\n[LightGBM] [Info] Start training from score 0.014495\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 3502, number of negative: 3452\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001252 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 6954, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503595 -> initscore=0.014380\n[LightGBM] [Info] Start training from score 0.014380\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 3502, number of negative: 3452\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001283 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 6954, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503595 -> initscore=0.014380\n[LightGBM] [Info] Start training from score 0.014380\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 3502, number of negative: 3452\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001229 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 6954, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503595 -> initscore=0.014380\n[LightGBM] [Info] Start training from score 0.014380\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 3503, number of negative: 3452\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001172 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2156\n[LightGBM] [Info] Number of data points in the train set: 6955, number of used features: 38\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503666 -> initscore=0.014666\n[LightGBM] [Info] Start training from score 0.014666\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 3503, number of negative: 3452\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001380 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2153\n[LightGBM] [Info] Number of data points in the train set: 6955, number of used features: 37\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503666 -> initscore=0.014666\n[LightGBM] [Info] Start training from score 0.014666\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[1 0 1 ... 1 1 1]\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/7e5e127b-1121-4126-b36e-c2d31e061b9f","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1715206234866,"execution_millis":477,"deepnote_table_state":{"sortBy":[],"filters":[],"pageSize":100,"pageIndex":0},"deepnote_table_loading":false,"deepnote_to_be_reexecuted":false,"cell_id":"b8f15353c7984fbea9079be065ce6961","deepnote_cell_type":"code"},"source":"submission_dict = {'PassengerId':X_submission['PassengerId'], 'Transported':y_prediction.astype('bool')}\nsubmission_dict = pd.DataFrame(submission_dict)\nsubmission_dict","block_group":"dd269131f3a14a0d86bd4f70d52f78ba","execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":12,"data":{"application/vnd.deepnote.dataframe.v3+json":{"column_count":2,"row_count":4277,"columns":[{"name":"PassengerId","dtype":"object","stats":{"unique_count":4277,"nan_count":0,"categories":[{"name":"0013_01","count":1},{"name":"0018_01","count":1},{"name":"4275 others","count":4275}]}},{"name":"Transported","dtype":"bool","stats":{"unique_count":2,"nan_count":0,"categories":[{"name":"True","count":2305},{"name":"False","count":1972}]}},{"name":"_deepnote_index_column","dtype":"int64"}],"rows":[{"PassengerId":"0013_01","Transported":"True","_deepnote_index_column":0},{"PassengerId":"0018_01","Transported":"False","_deepnote_index_column":1},{"PassengerId":"0019_01","Transported":"True","_deepnote_index_column":2},{"PassengerId":"0021_01","Transported":"True","_deepnote_index_column":3},{"PassengerId":"0023_01","Transported":"True","_deepnote_index_column":4},{"PassengerId":"0027_01","Transported":"True","_deepnote_index_column":5},{"PassengerId":"0029_01","Transported":"True","_deepnote_index_column":6},{"PassengerId":"0032_01","Transported":"True","_deepnote_index_column":7},{"PassengerId":"0032_02","Transported":"True","_deepnote_index_column":8},{"PassengerId":"0033_01","Transported":"True","_deepnote_index_column":9},{"PassengerId":"0037_01","Transported":"False","_deepnote_index_column":10},{"PassengerId":"0040_01","Transported":"False","_deepnote_index_column":11},{"PassengerId":"0040_02","Transported":"True","_deepnote_index_column":12},{"PassengerId":"0042_01","Transported":"True","_deepnote_index_column":13},{"PassengerId":"0046_01","Transported":"False","_deepnote_index_column":14},{"PassengerId":"0046_02","Transported":"False","_deepnote_index_column":15},{"PassengerId":"0046_03","Transported":"False","_deepnote_index_column":16},{"PassengerId":"0047_01","Transported":"True","_deepnote_index_column":17},{"PassengerId":"0047_02","Transported":"True","_deepnote_index_column":18},{"PassengerId":"0047_03","Transported":"False","_deepnote_index_column":19},{"PassengerId":"0048_01","Transported":"True","_deepnote_index_column":20},{"PassengerId":"0049_01","Transported":"False","_deepnote_index_column":21},{"PassengerId":"0054_01","Transported":"True","_deepnote_index_column":22},{"PassengerId":"0054_02","Transported":"True","_deepnote_index_column":23},{"PassengerId":"0054_03","Transported":"True","_deepnote_index_column":24},{"PassengerId":"0055_01","Transported":"False","_deepnote_index_column":25},{"PassengerId":"0057_01","Transported":"True","_deepnote_index_column":26},{"PassengerId":"0059_01","Transported":"True","_deepnote_index_column":27},{"PassengerId":"0060_01","Transported":"True","_deepnote_index_column":28},{"PassengerId":"0063_01","Transported":"True","_deepnote_index_column":29},{"PassengerId":"0065_01","Transported":"True","_deepnote_index_column":30},{"PassengerId":"0075_01","Transported":"False","_deepnote_index_column":31},{"PassengerId":"0079_01","Transported":"True","_deepnote_index_column":32},{"PassengerId":"0080_01","Transported":"False","_deepnote_index_column":33},{"PassengerId":"0083_01","Transported":"False","_deepnote_index_column":34},{"PassengerId":"0087_01","Transported":"False","_deepnote_index_column":35},{"PassengerId":"0089_01","Transported":"True","_deepnote_index_column":36},{"PassengerId":"0093_01","Transported":"True","_deepnote_index_column":37},{"PassengerId":"0094_01","Transported":"True","_deepnote_index_column":38},{"PassengerId":"0094_02","Transported":"False","_deepnote_index_column":39},{"PassengerId":"0095_01","Transported":"True","_deepnote_index_column":40},{"PassengerId":"0096_01","Transported":"True","_deepnote_index_column":41},{"PassengerId":"0100_01","Transported":"True","_deepnote_index_column":42},{"PassengerId":"0100_02","Transported":"False","_deepnote_index_column":43},{"PassengerId":"0104_01","Transported":"False","_deepnote_index_column":44},{"PassengerId":"0106_01","Transported":"True","_deepnote_index_column":45},{"PassengerId":"0109_01","Transported":"False","_deepnote_index_column":46},{"PassengerId":"0117_01","Transported":"False","_deepnote_index_column":47},{"PassengerId":"0118_01","Transported":"True","_deepnote_index_column":48},{"PassengerId":"0121_01","Transported":"False","_deepnote_index_column":49},{"PassengerId":"0124_01","Transported":"True","_deepnote_index_column":50},{"PassengerId":"0125_01","Transported":"True","_deepnote_index_column":51},{"PassengerId":"0125_02","Transported":"False","_deepnote_index_column":52},{"PassengerId":"0130_01","Transported":"True","_deepnote_index_column":53},{"PassengerId":"0131_01","Transported":"True","_deepnote_index_column":54},{"PassengerId":"0132_01","Transported":"True","_deepnote_index_column":55},{"PassengerId":"0135_01","Transported":"False","_deepnote_index_column":56},{"PassengerId":"0137_01","Transported":"True","_deepnote_index_column":57},{"PassengerId":"0142_01","Transported":"True","_deepnote_index_column":58},{"PassengerId":"0142_02","Transported":"False","_deepnote_index_column":59},{"PassengerId":"0142_03","Transported":"True","_deepnote_index_column":60},{"PassengerId":"0143_01","Transported":"True","_deepnote_index_column":61},{"PassengerId":"0145_01","Transported":"False","_deepnote_index_column":62},{"PassengerId":"0150_01","Transported":"True","_deepnote_index_column":63},{"PassengerId":"0150_02","Transported":"True","_deepnote_index_column":64},{"PassengerId":"0153_01","Transported":"True","_deepnote_index_column":65},{"PassengerId":"0154_01","Transported":"True","_deepnote_index_column":66},{"PassengerId":"0155_01","Transported":"False","_deepnote_index_column":67},{"PassengerId":"0156_01","Transported":"True","_deepnote_index_column":68},{"PassengerId":"0157_01","Transported":"False","_deepnote_index_column":69},{"PassengerId":"0158_01","Transported":"False","_deepnote_index_column":70},{"PassengerId":"0158_02","Transported":"True","_deepnote_index_column":71},{"PassengerId":"0159_01","Transported":"False","_deepnote_index_column":72},{"PassengerId":"0161_01","Transported":"False","_deepnote_index_column":73},{"PassengerId":"0162_01","Transported":"True","_deepnote_index_column":74},{"PassengerId":"0166_01","Transported":"True","_deepnote_index_column":75},{"PassengerId":"0168_01","Transported":"True","_deepnote_index_column":76},{"PassengerId":"0175_01","Transported":"True","_deepnote_index_column":77},{"PassengerId":"0175_02","Transported":"True","_deepnote_index_column":78},{"PassengerId":"0175_03","Transported":"True","_deepnote_index_column":79},{"PassengerId":"0175_04","Transported":"True","_deepnote_index_column":80},{"PassengerId":"0175_05","Transported":"False","_deepnote_index_column":81},{"PassengerId":"0176_01","Transported":"False","_deepnote_index_column":82},{"PassengerId":"0180_01","Transported":"False","_deepnote_index_column":83},{"PassengerId":"0184_01","Transported":"False","_deepnote_index_column":84},{"PassengerId":"0185_01","Transported":"True","_deepnote_index_column":85},{"PassengerId":"0187_01","Transported":"True","_deepnote_index_column":86},{"PassengerId":"0191_01","Transported":"False","_deepnote_index_column":87},{"PassengerId":"0194_01","Transported":"True","_deepnote_index_column":88},{"PassengerId":"0194_02","Transported":"True","_deepnote_index_column":89},{"PassengerId":"0194_03","Transported":"False","_deepnote_index_column":90},{"PassengerId":"0204_01","Transported":"False","_deepnote_index_column":91},{"PassengerId":"0208_01","Transported":"False","_deepnote_index_column":92},{"PassengerId":"0209_01","Transported":"False","_deepnote_index_column":93},{"PassengerId":"0214_01","Transported":"False","_deepnote_index_column":94},{"PassengerId":"0214_02","Transported":"False","_deepnote_index_column":95},{"PassengerId":"0215_01","Transported":"True","_deepnote_index_column":96},{"PassengerId":"0218_01","Transported":"False","_deepnote_index_column":97},{"PassengerId":"0226_01","Transported":"True","_deepnote_index_column":98},{"PassengerId":"0227_01","Transported":"True","_deepnote_index_column":99}]},"text/plain":"     PassengerId  Transported\n0        0013_01         True\n1        0018_01        False\n2        0019_01         True\n3        0021_01         True\n4        0023_01         True\n...          ...          ...\n4272     9266_02         True\n4273     9269_01        False\n4274     9271_01         True\n4275     9273_01         True\n4276     9277_01         True\n\n[4277 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>Transported</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0013_01</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0018_01</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0019_01</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0021_01</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0023_01</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4272</th>\n      <td>9266_02</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>4273</th>\n      <td>9269_01</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>4274</th>\n      <td>9271_01</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>4275</th>\n      <td>9273_01</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>4276</th>\n      <td>9277_01</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n<p>4277 rows × 2 columns</p>\n</div>"},"metadata":{}}],"outputs_reference":"s3:deepnote-cell-outputs-production/3dcd2864-6fa9-4a31-a737-66d2779dd37c","content_dependencies":null},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"49d14ceb01ac45f7a3a8403e82644220","deepnote_cell_type":"text-cell-h2"},"source":"## Export CSV","block_group":"11179e1880944debabb23367b2d4f7f0"},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1715206234969,"execution_millis":374,"deepnote_to_be_reexecuted":false,"cell_id":"e3a8fa4be9df48e0b178915e66238b8f","deepnote_cell_type":"code"},"source":"submission_dict.to_csv('spaceship_ensembel2.csv', index=False)","block_group":"af3684a41e194ff4ba4e0fbc07d01dd3","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=2a79941c-6614-47fe-9427-0e9f23998893' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote_persisted_session":{"createdAt":"2024-05-08T22:30:32.500Z"},"deepnote_notebook_id":"ad47d539b3554952908185779760c415","deepnote_execution_queue":[]}}
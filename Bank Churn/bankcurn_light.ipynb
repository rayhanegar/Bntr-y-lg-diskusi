{"cells":[{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"2b597a04c72b401bbeb4e7e7a0a519a5","deepnote_cell_type":"text-cell-h1"},"source":"# Spaceship KNN Submission","block_group":"082f42d739ae4c33b25722d4d688de17"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"2bff1da269e84f71a71ddb34cba7b421","deepnote_cell_type":"text-cell-p"},"source":"Hello there whoever handles this aku lupa, ini cuma coba-coba aja si pake KNeighborsClassifier buat test submission. Feel free to modify ya, lov u makasi semangat. -Egar","block_group":"7354f831abba43d4bdedda716dd7347c"},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1715169450776,"execution_millis":2753,"deepnote_to_be_reexecuted":false,"cell_id":"a3167010a462474bb8c66f7a0405423d","deepnote_cell_type":"code"},"source":"!pip install lightgbm==4.3.0","block_group":"32808e7135214b61b0c590e77d06923c","execution_count":null,"outputs":[{"name":"stdout","text":"Requirement already satisfied: lightgbm==4.3.0 in /root/venv/lib/python3.9/site-packages (4.3.0)\nRequirement already satisfied: scipy in /shared-libs/python3.9/py/lib/python3.9/site-packages (from lightgbm==4.3.0) (1.9.3)\nRequirement already satisfied: numpy in /shared-libs/python3.9/py/lib/python3.9/site-packages (from lightgbm==4.3.0) (1.23.4)\n\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/45577966-db87-4459-bafe-36f0c9d234fb","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1715169457792,"execution_millis":73,"deepnote_to_be_reexecuted":false,"cell_id":"18e2e30981ad48aabd4462e5f27137f0","deepnote_cell_type":"code"},"source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.decomposition import PCA\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport lightgbm as lgb\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\n","block_group":"6d67ae98cbb84c27b03fe6f9075b0099","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"b94534c051e749e58171fa0bcdeee442","deepnote_cell_type":"text-cell-h1"},"source":"# Bring the Data In","block_group":"0c79d79c638f4ac7a0e36c426c763934"},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1715169461056,"execution_millis":893,"deepnote_to_be_reexecuted":false,"cell_id":"3828bc89a22a46979c070cabab0ffb77","deepnote_cell_type":"code"},"source":"# Load data\nX_df = pd.read_csv('preprocessedbankchurn_train.csv')\nX_df.drop(\"Exited\", axis=1, inplace=True)\ny_df = pd.read_csv('preprocessedbankchurn_train.csv')['Exited']\nX_submission = pd.read_csv('preprocessedbankchurn_test.csv')\n\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(X_df, y_df, test_size=0.2, random_state=42)","block_group":"9acd7481bdec43f3ba43972876e9aaa0","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"af2560e0a0a4454383611d8133a55cbf","deepnote_cell_type":"text-cell-h1"},"source":"# Model Training","block_group":"e01f12f006324577a8685ffcb2e53c45"},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1715169463355,"execution_millis":41,"deepnote_to_be_reexecuted":false,"cell_id":"3477862b25864dabbf3a5d587d672c93","deepnote_cell_type":"code"},"source":"\n# Membuat pipeline\n\npipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('classifier', LGBMClassifier(random_state=42))\n])\n\n\n\nparam_grid = {\n    'classifier__n_estimators': [500],\n    'classifier__max_depth': [10],\n    'classifier__learning_rate': [0.01],\n    'classifier__subsample': [1],\n    'classifier__colsample_bytree': [0.8]\n}\n\n# Menggunakan StratifiedKFold untuk handling imbalanced data\ncv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n","block_group":"67f9157ecbae4561854a5e040d06f879","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1715169467149,"execution_millis":82913,"deepnote_to_be_reexecuted":false,"cell_id":"5ba197b22e1f4cc38ec14a3e300ee83e","deepnote_cell_type":"code"},"source":"# GridSearchCV setup\ngrid_search = GridSearchCV(pipeline, param_grid, cv=cv, scoring='roc_auc', verbose=1)\ngrid_search.fit(X_train, y_train)\n\n# Evaluate the model\nbest_model = grid_search.best_estimator_\ny_pred_proba = best_model.predict_proba(X_test)[:, 1]  # Get probabilities for the positive class\nprint(\"Best parameters:\", grid_search.best_params_)\nprint(\"ROC-AUC on test set:\", roc_auc_score(y_test, y_pred_proba))","block_group":"9c8bbf083d734e45b75bd18cb20f4730","execution_count":null,"outputs":[{"name":"stdout","text":"Fitting 10 folds for each of 1 candidates, totalling 10 fits\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 25170, number of negative: 93654\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004067 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 118824, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211826 -> initscore=-1.313954\n[LightGBM] [Info] Start training from score -1.313954\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 25169, number of negative: 93655\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004173 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 118824, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211817 -> initscore=-1.314005\n[LightGBM] [Info] Start training from score -1.314005\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 25169, number of negative: 93655\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004378 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1890\n[LightGBM] [Info] Number of data points in the train set: 118824, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211817 -> initscore=-1.314005\n[LightGBM] [Info] Start training from score -1.314005\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 25169, number of negative: 93655\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004249 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 118824, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211817 -> initscore=-1.314005\n[LightGBM] [Info] Start training from score -1.314005\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 25169, number of negative: 93655\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004067 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 118824, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211817 -> initscore=-1.314005\n[LightGBM] [Info] Start training from score -1.314005\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 25169, number of negative: 93655\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005269 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1890\n[LightGBM] [Info] Number of data points in the train set: 118824, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211817 -> initscore=-1.314005\n[LightGBM] [Info] Start training from score -1.314005\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 25169, number of negative: 93655\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005982 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 118824, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211817 -> initscore=-1.314005\n[LightGBM] [Info] Start training from score -1.314005\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 25170, number of negative: 93655\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004055 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1890\n[LightGBM] [Info] Number of data points in the train set: 118825, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211824 -> initscore=-1.313965\n[LightGBM] [Info] Start training from score -1.313965\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 25170, number of negative: 93655\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004127 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1890\n[LightGBM] [Info] Number of data points in the train set: 118825, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211824 -> initscore=-1.313965\n[LightGBM] [Info] Start training from score -1.313965\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 25170, number of negative: 93655\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004122 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 118825, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211824 -> initscore=-1.313965\n[LightGBM] [Info] Start training from score -1.313965\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 27966, number of negative: 104061\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021790 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1890\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211820 -> initscore=-1.313988\n[LightGBM] [Info] Start training from score -1.313988\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\nBest parameters: {'classifier__colsample_bytree': 0.8, 'classifier__learning_rate': 0.01, 'classifier__max_depth': 10, 'classifier__n_estimators': 500, 'classifier__subsample': 1}\nROC-AUC on test set: 0.9007634208991739\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/ef697e4a-5369-426c-a621-8d7e24219700","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1715169550072,"execution_millis":83524,"deepnote_to_be_reexecuted":false,"cell_id":"596e15638dd944ca873e9d5e505c36a8","deepnote_cell_type":"code"},"source":"from sklearn.model_selection import RandomizedSearchCV\n# Inisialisasi RandomizedSearchCV\nrandom_search = RandomizedSearchCV(\n    estimator=pipeline, \n    param_distributions=param_grid, \n    n_iter=50,  # Jumlah iterasi pencarian acak\n    cv=cv, \n    scoring='roc_auc', \n    random_state=42,\n    verbose=1,\n    n_jobs=-1\n)\n\n# Melatih model\nrandom_search.fit(X_train, y_train)\n\n# Menampilkan hasil terbaik\nprint(\"Best parameters:\", random_search.best_params_)\nprint(\"Best score:\", random_search.best_score_)\nprint(\"ROC-AUC on test set:\", roc_auc_score(y_test, y_pred_proba))","block_group":"0052d734544d4497ab37008ab0b532d5","execution_count":null,"outputs":[{"name":"stdout","text":"Fitting 10 folds for each of 1 candidates, totalling 10 fits\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/model_selection/_search.py:306: UserWarning: The total space of parameters 1 is smaller than n_iter=50. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n  warnings.warn(\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 25170, number of negative: 93654\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005140 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 118824, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211826 -> initscore=-1.313954\n[LightGBM] [Info] Start training from score -1.313954\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 25169, number of negative: 93655\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004205 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 118824, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211817 -> initscore=-1.314005\n[LightGBM] [Info] Start training from score -1.314005\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 25169, number of negative: 93655\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004100 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1890\n[LightGBM] [Info] Number of data points in the train set: 118824, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211817 -> initscore=-1.314005\n[LightGBM] [Info] Start training from score -1.314005\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 25169, number of negative: 93655\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014063 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 118824, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211817 -> initscore=-1.314005\n[LightGBM] [Info] Start training from score -1.314005\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 25169, number of negative: 93655\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004068 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 118824, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211817 -> initscore=-1.314005\n[LightGBM] [Info] Start training from score -1.314005\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 25169, number of negative: 93655\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004157 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1890\n[LightGBM] [Info] Number of data points in the train set: 118824, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211817 -> initscore=-1.314005\n[LightGBM] [Info] Start training from score -1.314005\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 25169, number of negative: 93655\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018128 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 118824, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211817 -> initscore=-1.314005\n[LightGBM] [Info] Start training from score -1.314005\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 25170, number of negative: 93655\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004209 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1890\n[LightGBM] [Info] Number of data points in the train set: 118825, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211824 -> initscore=-1.313965\n[LightGBM] [Info] Start training from score -1.313965\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 25170, number of negative: 93655\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005829 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1890\n[LightGBM] [Info] Number of data points in the train set: 118825, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211824 -> initscore=-1.313965\n[LightGBM] [Info] Start training from score -1.313965\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 25170, number of negative: 93655\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004019 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 118825, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211824 -> initscore=-1.313965\n[LightGBM] [Info] Start training from score -1.313965\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 27966, number of negative: 104061\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006102 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1890\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211820 -> initscore=-1.313988\n[LightGBM] [Info] Start training from score -1.313988\nBest parameters: {'classifier__subsample': 1, 'classifier__n_estimators': 500, 'classifier__max_depth': 10, 'classifier__learning_rate': 0.01, 'classifier__colsample_bytree': 0.8}\nBest score: 0.8986303267960715\nROC-AUC on test set: 0.9007634208991739\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/3a3ac35a-b096-4cb1-96f6-8663a0fc8574","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1715169838259,"execution_millis":99140,"deepnote_to_be_reexecuted":false,"cell_id":"c0191a35fca7462bb41ab5d00fcfff4a","deepnote_cell_type":"code"},"source":"# Evaluation on the test set\ny_pred = best_model.predict(X_test)\nprint(classification_report(y_test, y_pred))\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test, y_pred))\n\n# Fit the model on all training data\ngrid_search.fit(X_df, y_df)","block_group":"da8f0bbe4e924e31ba932bc2b30b8c4f","execution_count":null,"outputs":[{"name":"stdout","text":"[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n              precision    recall  f1-score   support\n\n           0       0.89      0.95      0.92     26052\n           1       0.75      0.58      0.66      6955\n\n    accuracy                           0.87     33007\n   macro avg       0.82      0.76      0.79     33007\nweighted avg       0.86      0.87      0.87     33007\n\nConfusion Matrix:\n[[24737  1315]\n [ 2923  4032]]\nFitting 10 folds for each of 1 candidates, totalling 10 fits\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005966 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n[LightGBM] [Info] Start training from score -1.315306\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005086 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n[LightGBM] [Info] Start training from score -1.315306\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007098 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1890\n[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n[LightGBM] [Info] Start training from score -1.315306\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005221 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1890\n[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n[LightGBM] [Info] Start training from score -1.315346\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007032 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1890\n[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n[LightGBM] [Info] Start training from score -1.315314\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005136 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n[LightGBM] [Info] Start training from score -1.315314\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005056 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n[LightGBM] [Info] Start training from score -1.315314\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005685 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n[LightGBM] [Info] Start training from score -1.315314\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005294 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n[LightGBM] [Info] Start training from score -1.315314\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008196 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n[LightGBM] [Info] Start training from score -1.315314\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 34921, number of negative: 130113\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005599 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1890\n[LightGBM] [Info] Number of data points in the train set: 165034, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315315\n[LightGBM] [Info] Start training from score -1.315315\n","output_type":"stream"},{"output_type":"execute_result","execution_count":28,"data":{"text/plain":"GridSearchCV(cv=StratifiedKFold(n_splits=10, random_state=42, shuffle=True),\n             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n                                       ('classifier',\n                                        LGBMClassifier(random_state=42))]),\n             param_grid={'classifier__colsample_bytree': [0.8],\n                         'classifier__learning_rate': [0.01],\n                         'classifier__max_depth': [10],\n                         'classifier__n_estimators': [500],\n                         'classifier__subsample': [1]},\n             scoring='roc_auc', verbose=1)","text/html":"<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=StratifiedKFold(n_splits=10, random_state=42, shuffle=True),\n             estimator=Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n                                       (&#x27;classifier&#x27;,\n                                        LGBMClassifier(random_state=42))]),\n             param_grid={&#x27;classifier__colsample_bytree&#x27;: [0.8],\n                         &#x27;classifier__learning_rate&#x27;: [0.01],\n                         &#x27;classifier__max_depth&#x27;: [10],\n                         &#x27;classifier__n_estimators&#x27;: [500],\n                         &#x27;classifier__subsample&#x27;: [1]},\n             scoring=&#x27;roc_auc&#x27;, verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=StratifiedKFold(n_splits=10, random_state=42, shuffle=True),\n             estimator=Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n                                       (&#x27;classifier&#x27;,\n                                        LGBMClassifier(random_state=42))]),\n             param_grid={&#x27;classifier__colsample_bytree&#x27;: [0.8],\n                         &#x27;classifier__learning_rate&#x27;: [0.01],\n                         &#x27;classifier__max_depth&#x27;: [10],\n                         &#x27;classifier__n_estimators&#x27;: [500],\n                         &#x27;classifier__subsample&#x27;: [1]},\n             scoring=&#x27;roc_auc&#x27;, verbose=1)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n                (&#x27;classifier&#x27;, LGBMClassifier(random_state=42))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier(random_state=42)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"},"metadata":{}}],"outputs_reference":"s3:deepnote-cell-outputs-production/4b7e11ff-defe-40a1-9f42-616872a851a8","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1715169937445,"execution_millis":414773,"deepnote_to_be_reexecuted":false,"cell_id":"53cb8630ac534abbb22abc4c4b472604","deepnote_cell_type":"code"},"source":"# Melakukan cross-validation dengan GridSearchCV\nscores = cross_val_score(grid_search, X_df, y_df, cv=5)\n\n# Menampilkan hasil cross-validation\nprint(\"Cross-validation scores:\", scores)\nprint(\"Mean cross-validation score:\", scores.mean())","block_group":"eb852dfb891b49b9a1b48a4681907ae2","execution_count":null,"outputs":[{"name":"stdout","text":"Fitting 10 folds for each of 1 candidates, totalling 10 fits\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 25143, number of negative: 93681\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017343 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 118824, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315316\n[LightGBM] [Info] Start training from score -1.315316\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 25143, number of negative: 93681\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004128 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1890\n[LightGBM] [Info] Number of data points in the train set: 118824, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315316\n[LightGBM] [Info] Start training from score -1.315316\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 25143, number of negative: 93681\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013997 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 118824, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315316\n[LightGBM] [Info] Start training from score -1.315316\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 25143, number of negative: 93681\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004165 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 118824, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315316\n[LightGBM] [Info] Start training from score -1.315316\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 25143, number of negative: 93681\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004514 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 118824, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315316\n[LightGBM] [Info] Start training from score -1.315316\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 25143, number of negative: 93681\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006191 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1890\n[LightGBM] [Info] Number of data points in the train set: 118824, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315316\n[LightGBM] [Info] Start training from score -1.315316\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 25143, number of negative: 93681\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008165 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1890\n[LightGBM] [Info] Number of data points in the train set: 118824, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315316\n[LightGBM] [Info] Start training from score -1.315316\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 25144, number of negative: 93681\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004091 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 118825, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211605 -> initscore=-1.315276\n[LightGBM] [Info] Start training from score -1.315276\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 25144, number of negative: 93681\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004147 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1890\n[LightGBM] [Info] Number of data points in the train set: 118825, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211605 -> initscore=-1.315276\n[LightGBM] [Info] Start training from score -1.315276\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 25144, number of negative: 93681\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004067 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1890\n[LightGBM] [Info] Number of data points in the train set: 118825, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211605 -> initscore=-1.315276\n[LightGBM] [Info] Start training from score -1.315276\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004602 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1890\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\nFitting 10 folds for each of 1 candidates, totalling 10 fits\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 25143, number of negative: 93681\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004037 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1888\n[LightGBM] [Info] Number of data points in the train set: 118824, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315316\n[LightGBM] [Info] Start training from score -1.315316\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 25143, number of negative: 93681\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004283 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 118824, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315316\n[LightGBM] [Info] Start training from score -1.315316\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 25143, number of negative: 93681\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019313 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 118824, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315316\n[LightGBM] [Info] Start training from score -1.315316\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 25143, number of negative: 93681\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004111 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1888\n[LightGBM] [Info] Number of data points in the train set: 118824, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315316\n[LightGBM] [Info] Start training from score -1.315316\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 25143, number of negative: 93681\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005874 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 118824, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315316\n[LightGBM] [Info] Start training from score -1.315316\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 25143, number of negative: 93681\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004134 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 118824, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315316\n[LightGBM] [Info] Start training from score -1.315316\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 25143, number of negative: 93681\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006430 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 118824, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315316\n[LightGBM] [Info] Start training from score -1.315316\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 25144, number of negative: 93681\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006359 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1888\n[LightGBM] [Info] Number of data points in the train set: 118825, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211605 -> initscore=-1.315276\n[LightGBM] [Info] Start training from score -1.315276\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 25144, number of negative: 93681\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005521 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 118825, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211605 -> initscore=-1.315276\n[LightGBM] [Info] Start training from score -1.315276\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 25144, number of negative: 93681\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004012 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 118825, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211605 -> initscore=-1.315276\n[LightGBM] [Info] Start training from score -1.315276\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004665 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\nFitting 10 folds for each of 1 candidates, totalling 10 fits\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 25143, number of negative: 93681\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004076 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1888\n[LightGBM] [Info] Number of data points in the train set: 118824, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315316\n[LightGBM] [Info] Start training from score -1.315316\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 25143, number of negative: 93681\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005955 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 118824, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315316\n[LightGBM] [Info] Start training from score -1.315316\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 25143, number of negative: 93681\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005782 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 118824, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315316\n[LightGBM] [Info] Start training from score -1.315316\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 25143, number of negative: 93681\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004058 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1888\n[LightGBM] [Info] Number of data points in the train set: 118824, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315316\n[LightGBM] [Info] Start training from score -1.315316\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 25143, number of negative: 93681\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004077 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 118824, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315316\n[LightGBM] [Info] Start training from score -1.315316\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 25143, number of negative: 93681\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004111 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 118824, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315316\n[LightGBM] [Info] Start training from score -1.315316\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 25143, number of negative: 93681\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004523 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 118824, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315316\n[LightGBM] [Info] Start training from score -1.315316\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 25144, number of negative: 93681\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004676 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 118825, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211605 -> initscore=-1.315276\n[LightGBM] [Info] Start training from score -1.315276\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 25144, number of negative: 93681\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005992 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 118825, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211605 -> initscore=-1.315276\n[LightGBM] [Info] Start training from score -1.315276\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 25144, number of negative: 93681\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009440 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 118825, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211605 -> initscore=-1.315276\n[LightGBM] [Info] Start training from score -1.315276\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004602 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\nFitting 10 folds for each of 1 candidates, totalling 10 fits\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 25143, number of negative: 93681\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004184 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1888\n[LightGBM] [Info] Number of data points in the train set: 118824, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315316\n[LightGBM] [Info] Start training from score -1.315316\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 25142, number of negative: 93682\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004142 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 118824, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211590 -> initscore=-1.315366\n[LightGBM] [Info] Start training from score -1.315366\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 25142, number of negative: 93682\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004090 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1888\n[LightGBM] [Info] Number of data points in the train set: 118824, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211590 -> initscore=-1.315366\n[LightGBM] [Info] Start training from score -1.315366\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 25142, number of negative: 93682\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006448 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 118824, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211590 -> initscore=-1.315366\n[LightGBM] [Info] Start training from score -1.315366\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 25142, number of negative: 93682\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004144 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1888\n[LightGBM] [Info] Number of data points in the train set: 118824, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211590 -> initscore=-1.315366\n[LightGBM] [Info] Start training from score -1.315366\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 25142, number of negative: 93682\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008042 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1888\n[LightGBM] [Info] Number of data points in the train set: 118824, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211590 -> initscore=-1.315366\n[LightGBM] [Info] Start training from score -1.315366\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 25142, number of negative: 93682\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004176 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 118824, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211590 -> initscore=-1.315366\n[LightGBM] [Info] Start training from score -1.315366\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 25143, number of negative: 93682\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004829 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1888\n[LightGBM] [Info] Number of data points in the train set: 118825, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211597 -> initscore=-1.315327\n[LightGBM] [Info] Start training from score -1.315327\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 25143, number of negative: 93682\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004059 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 118825, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211597 -> initscore=-1.315327\n[LightGBM] [Info] Start training from score -1.315327\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 25143, number of negative: 93682\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004162 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 118825, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211597 -> initscore=-1.315327\n[LightGBM] [Info] Start training from score -1.315327\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 27936, number of negative: 104091\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004585 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211593 -> initscore=-1.315349\n[LightGBM] [Info] Start training from score -1.315349\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\nFitting 10 folds for each of 1 candidates, totalling 10 fits\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 25144, number of negative: 93681\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004020 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 118825, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211605 -> initscore=-1.315276\n[LightGBM] [Info] Start training from score -1.315276\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 25143, number of negative: 93682\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005107 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1888\n[LightGBM] [Info] Number of data points in the train set: 118825, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211597 -> initscore=-1.315327\n[LightGBM] [Info] Start training from score -1.315327\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 25143, number of negative: 93682\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006522 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1888\n[LightGBM] [Info] Number of data points in the train set: 118825, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211597 -> initscore=-1.315327\n[LightGBM] [Info] Start training from score -1.315327\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 25143, number of negative: 93682\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004035 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 118825, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211597 -> initscore=-1.315327\n[LightGBM] [Info] Start training from score -1.315327\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 25143, number of negative: 93682\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004053 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 118825, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211597 -> initscore=-1.315327\n[LightGBM] [Info] Start training from score -1.315327\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 25143, number of negative: 93682\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004126 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1888\n[LightGBM] [Info] Number of data points in the train set: 118825, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211597 -> initscore=-1.315327\n[LightGBM] [Info] Start training from score -1.315327\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 25143, number of negative: 93682\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006248 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 118825, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211597 -> initscore=-1.315327\n[LightGBM] [Info] Start training from score -1.315327\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 25143, number of negative: 93682\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004044 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1888\n[LightGBM] [Info] Number of data points in the train set: 118825, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211597 -> initscore=-1.315327\n[LightGBM] [Info] Start training from score -1.315327\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 25144, number of negative: 93682\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004147 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1888\n[LightGBM] [Info] Number of data points in the train set: 118826, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211604 -> initscore=-1.315287\n[LightGBM] [Info] Start training from score -1.315287\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 25144, number of negative: 93682\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004979 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 118826, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211604 -> initscore=-1.315287\n[LightGBM] [Info] Start training from score -1.315287\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104091\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007082 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 132028, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n[LightGBM] [Info] Start training from score -1.315314\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\nCross-validation scores: [0.90226569 0.89703024 0.89930708 0.89917339 0.8974412 ]\nMean cross-validation score: 0.8990435192235312\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/61aa0e87-02f7-44fb-8749-c511b9358861","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1715170352220,"execution_millis":103754,"deepnote_to_be_reexecuted":false,"cell_id":"ab5fde0fb97544b6979d83dc89f90c59","deepnote_cell_type":"code"},"source":"# Evaluation on the test set\ny_pred = grid_search.predict(X_test)\nprint(classification_report(y_test, y_pred))\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test, y_pred))\n\n# Fit the model on all training data\ngrid_search.fit(X_df, y_df)","block_group":"fdcab2c1d81640bdac23f739f948b48b","execution_count":null,"outputs":[{"name":"stdout","text":"[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n              precision    recall  f1-score   support\n\n           0       0.90      0.95      0.92     26052\n           1       0.76      0.59      0.66      6955\n\n    accuracy                           0.87     33007\n   macro avg       0.83      0.77      0.79     33007\nweighted avg       0.87      0.87      0.87     33007\n\nConfusion Matrix:\n[[24765  1287]\n [ 2877  4078]]\nFitting 10 folds for each of 1 candidates, totalling 10 fits\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005561 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n[LightGBM] [Info] Start training from score -1.315306\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005070 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n[LightGBM] [Info] Start training from score -1.315306\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007436 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1890\n[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n[LightGBM] [Info] Start training from score -1.315306\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005071 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1890\n[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n[LightGBM] [Info] Start training from score -1.315346\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005118 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1890\n[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n[LightGBM] [Info] Start training from score -1.315314\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005018 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n[LightGBM] [Info] Start training from score -1.315314\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005072 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n[LightGBM] [Info] Start training from score -1.315314\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005128 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n[LightGBM] [Info] Start training from score -1.315314\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005051 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n[LightGBM] [Info] Start training from score -1.315314\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007412 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n[LightGBM] [Info] Start training from score -1.315314\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 34921, number of negative: 130113\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008372 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1890\n[LightGBM] [Info] Number of data points in the train set: 165034, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315315\n[LightGBM] [Info] Start training from score -1.315315\n","output_type":"stream"},{"output_type":"execute_result","execution_count":30,"data":{"text/plain":"GridSearchCV(cv=StratifiedKFold(n_splits=10, random_state=42, shuffle=True),\n             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n                                       ('classifier',\n                                        LGBMClassifier(random_state=42))]),\n             param_grid={'classifier__colsample_bytree': [0.8],\n                         'classifier__learning_rate': [0.01],\n                         'classifier__max_depth': [10],\n                         'classifier__n_estimators': [500],\n                         'classifier__subsample': [1]},\n             scoring='roc_auc', verbose=1)","text/html":"<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=StratifiedKFold(n_splits=10, random_state=42, shuffle=True),\n             estimator=Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n                                       (&#x27;classifier&#x27;,\n                                        LGBMClassifier(random_state=42))]),\n             param_grid={&#x27;classifier__colsample_bytree&#x27;: [0.8],\n                         &#x27;classifier__learning_rate&#x27;: [0.01],\n                         &#x27;classifier__max_depth&#x27;: [10],\n                         &#x27;classifier__n_estimators&#x27;: [500],\n                         &#x27;classifier__subsample&#x27;: [1]},\n             scoring=&#x27;roc_auc&#x27;, verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=StratifiedKFold(n_splits=10, random_state=42, shuffle=True),\n             estimator=Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n                                       (&#x27;classifier&#x27;,\n                                        LGBMClassifier(random_state=42))]),\n             param_grid={&#x27;classifier__colsample_bytree&#x27;: [0.8],\n                         &#x27;classifier__learning_rate&#x27;: [0.01],\n                         &#x27;classifier__max_depth&#x27;: [10],\n                         &#x27;classifier__n_estimators&#x27;: [500],\n                         &#x27;classifier__subsample&#x27;: [1]},\n             scoring=&#x27;roc_auc&#x27;, verbose=1)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n                (&#x27;classifier&#x27;, LGBMClassifier(random_state=42))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier(random_state=42)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"},"metadata":{}}],"outputs_reference":"s3:deepnote-cell-outputs-production/24ecf191-e262-474e-9d94-46b50989c133","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1715170455977,"execution_millis":4244,"deepnote_to_be_reexecuted":false,"cell_id":"b448100ee9b54fe7b22464c77daf5b9e","deepnote_cell_type":"code"},"source":"# Predict probabilities for submission data\ny_submission_proba = grid_search.predict_proba(X_submission)[:, 1]  # Probabilities of Exited\nprint(y_submission_proba)\n\n# Prepare submission dictionary\nsubmission_dict = {'id': X_submission['id'], 'Exited': y_submission_proba}\nsubmission_dict = pd.DataFrame(submission_dict)\nsubmission_dict","block_group":"ffa1a5a6dc0742f6a823ee9eafca1c28","execution_count":null,"outputs":[{"name":"stdout","text":"[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[0.01788578 0.84044794 0.02779601 ... 0.028606   0.183122   0.16679128]\n","output_type":"stream"},{"output_type":"execute_result","execution_count":31,"data":{"application/vnd.deepnote.dataframe.v3+json":{"column_count":2,"row_count":110023,"columns":[{"name":"id","dtype":"int64"},{"name":"Exited","dtype":"float64"},{"name":"_deepnote_index_column","dtype":"int64"}],"rows":[{"id":165034,"Exited":0.01788577959198581,"_deepnote_index_column":0},{"id":165035,"Exited":0.8404479364796582,"_deepnote_index_column":1},{"id":165036,"Exited":0.027796009225333658,"_deepnote_index_column":2},{"id":165037,"Exited":0.23202946984247375,"_deepnote_index_column":3},{"id":165038,"Exited":0.3833444803476499,"_deepnote_index_column":4},{"id":165039,"Exited":0.028028842307969145,"_deepnote_index_column":5},{"id":165040,"Exited":0.03712306340423593,"_deepnote_index_column":6},{"id":165041,"Exited":0.09255136480504343,"_deepnote_index_column":7},{"id":165042,"Exited":0.6066983307145828,"_deepnote_index_column":8},{"id":165043,"Exited":0.010462306994832858,"_deepnote_index_column":9}]},"text/plain":"            id    Exited\n0       165034  0.017886\n1       165035  0.840448\n2       165036  0.027796\n3       165037  0.232029\n4       165038  0.383344\n...        ...       ...\n110018  275052  0.043406\n110019  275053  0.101982\n110020  275054  0.028606\n110021  275055  0.183122\n110022  275056  0.166791\n\n[110023 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>Exited</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>165034</td>\n      <td>0.017886</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>165035</td>\n      <td>0.840448</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>165036</td>\n      <td>0.027796</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>165037</td>\n      <td>0.232029</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>165038</td>\n      <td>0.383344</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>110018</th>\n      <td>275052</td>\n      <td>0.043406</td>\n    </tr>\n    <tr>\n      <th>110019</th>\n      <td>275053</td>\n      <td>0.101982</td>\n    </tr>\n    <tr>\n      <th>110020</th>\n      <td>275054</td>\n      <td>0.028606</td>\n    </tr>\n    <tr>\n      <th>110021</th>\n      <td>275055</td>\n      <td>0.183122</td>\n    </tr>\n    <tr>\n      <th>110022</th>\n      <td>275056</td>\n      <td>0.166791</td>\n    </tr>\n  </tbody>\n</table>\n<p>110023 rows × 2 columns</p>\n</div>"},"metadata":{}}],"outputs_reference":"s3:deepnote-cell-outputs-production/16269c5e-6e0f-4da7-8b1c-97660ea62bea","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1715170460230,"execution_millis":724,"deepnote_to_be_reexecuted":false,"cell_id":"4e63edc5c0d847aa8457be2ee24169c0","deepnote_cell_type":"code"},"source":"submission_dict.to_csv('bankChurn_light3.csv', index=False)","block_group":"271dcf9f53f8461ca073abccc0e239ab","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=2a79941c-6614-47fe-9427-0e9f23998893' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote_persisted_session":{"createdAt":"2024-05-08T13:00:09.339Z"},"deepnote_notebook_id":"956109dfb4904c3d91998748df849922","deepnote_execution_queue":[]}}
{"cells":[{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"67568f044dba42e28fd4484ee2cdb0b9","deepnote_cell_type":"text-cell-h1"},"source":"# Spaceship ensembel Submission","block_group":"082f42d739ae4c33b25722d4d688de17"},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1715228247544,"execution_millis":771,"deepnote_to_be_reexecuted":false,"cell_id":"2ca96ef8f71048a383240ae64f88f83b","deepnote_cell_type":"code"},"source":"import pandas as pd\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score","block_group":"6d67ae98cbb84c27b03fe6f9075b0099","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"0ec3eaeb76744a11bde7d5f022f5e18b","deepnote_cell_type":"text-cell-h1"},"source":"# Bring the Data In","block_group":"0c79d79c638f4ac7a0e36c426c763934"},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1715228248325,"execution_millis":1271,"deepnote_to_be_reexecuted":false,"cell_id":"dcd6d07e6986413d931691245085e693","deepnote_cell_type":"code"},"source":"# Load data\nX_df = pd.read_csv('preprocessedbankchurn_train.csv')\nX_df.drop(\"Exited\", axis=1, inplace=True)\ny_df = pd.read_csv('preprocessedbankchurn_train.csv')['Exited']\nX_submission = pd.read_csv('preprocessedbankchurn_test.csv')\n\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(X_df, y_df, test_size=0.2, random_state=42)","block_group":"9acd7481bdec43f3ba43972876e9aaa0","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"a7b83575a44d49e09cb61340cb6fed12","deepnote_cell_type":"text-cell-h1"},"source":"# Model Training","block_group":"e01f12f006324577a8685ffcb2e53c45"},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1715228249605,"execution_millis":219,"deepnote_to_be_reexecuted":false,"cell_id":"d31d5a14b26448e5ac8c2fdcd16b3a6f","deepnote_cell_type":"code"},"source":"# Base estimator\nbase_estimators = [\n    ('svc', SVC(probability=True)),\n    ('dt', DecisionTreeClassifier()),\n    ('knn', KNeighborsClassifier())\n]\n\n# Membuat pipeline\nstacking_pipeline = Pipeline([\n    ('scaler', StandardScaler()),  # Preprocessing: Normalisasi data\n    ('stacking', StackingClassifier(estimators=base_estimators, final_estimator=LogisticRegression()))\n])\n\nparam_grid = {\n    'stacking__svc__C': [10],\n    'stacking__dt__max_depth': [20],\n    'stacking__knn__n_neighbors': [10],\n    'stacking__final_estimator__C': [10]\n}\n\n# Menggunakan StratifiedKFold untuk handling imbalanced data\ncv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n","block_group":"67f9157ecbae4561854a5e040d06f879","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1715228249618,"execution_millis":0,"deepnote_to_be_reexecuted":false,"cell_id":"6d467c97ac8b4329bd22b2e012990212","deepnote_cell_type":"code"},"source":"# GridSearchCV setup\ngrid_search = GridSearchCV(stacking_pipeline, param_grid, cv=cv, scoring='roc_auc', verbose=1)\ngrid_search.fit(X_train, y_train)\n\n# Evaluate the model\nbest_model = grid_search.best_estimator_\ny_pred_proba = best_model.predict_proba(X_test)[:, 1]  # Get probabilities for the positive class\nprint(\"Best parameters:\", grid_search.best_params_)\nprint(\"ROC-AUC on test set:\", roc_auc_score(y_test, y_pred_proba))","block_group":"9c8bbf083d734e45b75bd18cb20f4730","execution_count":null,"outputs":[{"name":"stdout","text":"Fitting 10 folds for each of 1 candidates, totalling 10 fits\n","output_type":"stream"},{"output_type":"error","ename":"KernelInterrupted","evalue":"Execution interrupted by the Jupyter kernel.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKernelInterrupted\u001b[0m: Execution interrupted by the Jupyter kernel."]}],"outputs_reference":"dbtable:cell_outputs/b03fb5ee-209e-44a6-a5ac-e387417c23f7","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1715169550072,"execution_millis":83524,"deepnote_to_be_reexecuted":true,"cell_id":"fa7e203ca668471c9c38cf891e9af0ff","deepnote_cell_type":"code"},"source":"# Evaluasi pada validation set\ny_pred = grid_search.predict(X_test)\nprint(classification_report(y_test, y_pred))\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test, y_pred))\n# Melatih model\nrandom_search.fit(X_train, y_train)\n\n# Menampilkan hasil terbaik\nprint(\"Best parameters:\", random_search.best_params_)\nprint(\"Best score:\", random_search.best_score_)\nprint(\"ROC-AUC on test set:\", roc_auc_score(y_test, y_pred_proba))","block_group":"0052d734544d4497ab37008ab0b532d5","execution_count":null,"outputs":[{"name":"stdout","text":"Fitting 10 folds for each of 1 candidates, totalling 10 fits\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/model_selection/_search.py:306: UserWarning: The total space of parameters 1 is smaller than n_iter=50. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n  warnings.warn(\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 25170, number of negative: 93654\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005140 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 118824, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211826 -> initscore=-1.313954\n[LightGBM] [Info] Start training from score -1.313954\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 25169, number of negative: 93655\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004205 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 118824, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211817 -> initscore=-1.314005\n[LightGBM] [Info] Start training from score -1.314005\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 25169, number of negative: 93655\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004100 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1890\n[LightGBM] [Info] Number of data points in the train set: 118824, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211817 -> initscore=-1.314005\n[LightGBM] [Info] Start training from score -1.314005\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 25169, number of negative: 93655\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014063 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 118824, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211817 -> initscore=-1.314005\n[LightGBM] [Info] Start training from score -1.314005\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 25169, number of negative: 93655\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004068 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 118824, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211817 -> initscore=-1.314005\n[LightGBM] [Info] Start training from score -1.314005\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 25169, number of negative: 93655\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004157 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1890\n[LightGBM] [Info] Number of data points in the train set: 118824, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211817 -> initscore=-1.314005\n[LightGBM] [Info] Start training from score -1.314005\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 25169, number of negative: 93655\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018128 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 118824, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211817 -> initscore=-1.314005\n[LightGBM] [Info] Start training from score -1.314005\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 25170, number of negative: 93655\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004209 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1890\n[LightGBM] [Info] Number of data points in the train set: 118825, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211824 -> initscore=-1.313965\n[LightGBM] [Info] Start training from score -1.313965\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 25170, number of negative: 93655\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005829 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1890\n[LightGBM] [Info] Number of data points in the train set: 118825, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211824 -> initscore=-1.313965\n[LightGBM] [Info] Start training from score -1.313965\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 25170, number of negative: 93655\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004019 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 118825, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211824 -> initscore=-1.313965\n[LightGBM] [Info] Start training from score -1.313965\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 27966, number of negative: 104061\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006102 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1890\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211820 -> initscore=-1.313988\n[LightGBM] [Info] Start training from score -1.313988\nBest parameters: {'classifier__subsample': 1, 'classifier__n_estimators': 500, 'classifier__max_depth': 10, 'classifier__learning_rate': 0.01, 'classifier__colsample_bytree': 0.8}\nBest score: 0.8986303267960715\nROC-AUC on test set: 0.9007634208991739\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/8f04d1be-af12-4f8d-ab83-98e712f2a160","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1715169838259,"execution_millis":99140,"deepnote_to_be_reexecuted":true,"cell_id":"396e59fb15b747eb9a7d017a83b12f14","deepnote_cell_type":"code"},"source":"# Evaluation on the test set\ny_pred = best_model.predict(X_test)\nprint(classification_report(y_test, y_pred))\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test, y_pred))\n\n# Fit the model on all training data\ngrid_search.fit(X_df, y_df)","block_group":"da8f0bbe4e924e31ba932bc2b30b8c4f","execution_count":null,"outputs":[{"name":"stdout","text":"[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n              precision    recall  f1-score   support\n\n           0       0.89      0.95      0.92     26052\n           1       0.75      0.58      0.66      6955\n\n    accuracy                           0.87     33007\n   macro avg       0.82      0.76      0.79     33007\nweighted avg       0.86      0.87      0.87     33007\n\nConfusion Matrix:\n[[24737  1315]\n [ 2923  4032]]\nFitting 10 folds for each of 1 candidates, totalling 10 fits\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005966 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n[LightGBM] [Info] Start training from score -1.315306\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005086 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n[LightGBM] [Info] Start training from score -1.315306\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007098 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1890\n[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n[LightGBM] [Info] Start training from score -1.315306\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005221 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1890\n[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n[LightGBM] [Info] Start training from score -1.315346\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007032 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1890\n[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n[LightGBM] [Info] Start training from score -1.315314\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005136 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n[LightGBM] [Info] Start training from score -1.315314\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005056 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n[LightGBM] [Info] Start training from score -1.315314\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005685 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n[LightGBM] [Info] Start training from score -1.315314\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005294 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n[LightGBM] [Info] Start training from score -1.315314\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008196 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n[LightGBM] [Info] Start training from score -1.315314\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 34921, number of negative: 130113\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005599 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1890\n[LightGBM] [Info] Number of data points in the train set: 165034, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315315\n[LightGBM] [Info] Start training from score -1.315315\n","output_type":"stream"},{"output_type":"execute_result","execution_count":28,"data":{"text/plain":"GridSearchCV(cv=StratifiedKFold(n_splits=10, random_state=42, shuffle=True),\n             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n                                       ('classifier',\n                                        LGBMClassifier(random_state=42))]),\n             param_grid={'classifier__colsample_bytree': [0.8],\n                         'classifier__learning_rate': [0.01],\n                         'classifier__max_depth': [10],\n                         'classifier__n_estimators': [500],\n                         'classifier__subsample': [1]},\n             scoring='roc_auc', verbose=1)","text/html":"<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=StratifiedKFold(n_splits=10, random_state=42, shuffle=True),\n             estimator=Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n                                       (&#x27;classifier&#x27;,\n                                        LGBMClassifier(random_state=42))]),\n             param_grid={&#x27;classifier__colsample_bytree&#x27;: [0.8],\n                         &#x27;classifier__learning_rate&#x27;: [0.01],\n                         &#x27;classifier__max_depth&#x27;: [10],\n                         &#x27;classifier__n_estimators&#x27;: [500],\n                         &#x27;classifier__subsample&#x27;: [1]},\n             scoring=&#x27;roc_auc&#x27;, verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=StratifiedKFold(n_splits=10, random_state=42, shuffle=True),\n             estimator=Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n                                       (&#x27;classifier&#x27;,\n                                        LGBMClassifier(random_state=42))]),\n             param_grid={&#x27;classifier__colsample_bytree&#x27;: [0.8],\n                         &#x27;classifier__learning_rate&#x27;: [0.01],\n                         &#x27;classifier__max_depth&#x27;: [10],\n                         &#x27;classifier__n_estimators&#x27;: [500],\n                         &#x27;classifier__subsample&#x27;: [1]},\n             scoring=&#x27;roc_auc&#x27;, verbose=1)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n                (&#x27;classifier&#x27;, LGBMClassifier(random_state=42))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier(random_state=42)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"},"metadata":{}}],"outputs_reference":"s3:deepnote-cell-outputs-production/e3ef8b37-36b4-4078-9985-ff1d8b0e378d","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1715170455977,"execution_millis":4244,"deepnote_to_be_reexecuted":true,"cell_id":"0bd8f0970ac243d6b91a01c28ae44716","deepnote_cell_type":"code"},"source":"# Predict probabilities for submission data\ny_submission_proba = grid_search.predict_proba(X_submission)[:, 1]  # Probabilities of Exited\nprint(y_submission_proba)\n\n# Prepare submission dictionary\nsubmission_dict = {'id': X_submission['id'], 'Exited': y_submission_proba}\nsubmission_dict = pd.DataFrame(submission_dict)\nsubmission_dict","block_group":"ffa1a5a6dc0742f6a823ee9eafca1c28","execution_count":null,"outputs":[{"name":"stdout","text":"[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[0.01788578 0.84044794 0.02779601 ... 0.028606   0.183122   0.16679128]\n","output_type":"stream"},{"output_type":"execute_result","execution_count":31,"data":{"application/vnd.deepnote.dataframe.v3+json":{"column_count":2,"row_count":110023,"columns":[{"name":"id","dtype":"int64"},{"name":"Exited","dtype":"float64"},{"name":"_deepnote_index_column","dtype":"int64"}],"rows":[{"id":165034,"Exited":0.01788577959198581,"_deepnote_index_column":0},{"id":165035,"Exited":0.8404479364796582,"_deepnote_index_column":1},{"id":165036,"Exited":0.027796009225333658,"_deepnote_index_column":2},{"id":165037,"Exited":0.23202946984247375,"_deepnote_index_column":3},{"id":165038,"Exited":0.3833444803476499,"_deepnote_index_column":4},{"id":165039,"Exited":0.028028842307969145,"_deepnote_index_column":5},{"id":165040,"Exited":0.03712306340423593,"_deepnote_index_column":6},{"id":165041,"Exited":0.09255136480504343,"_deepnote_index_column":7},{"id":165042,"Exited":0.6066983307145828,"_deepnote_index_column":8},{"id":165043,"Exited":0.010462306994832858,"_deepnote_index_column":9}]},"text/plain":"            id    Exited\n0       165034  0.017886\n1       165035  0.840448\n2       165036  0.027796\n3       165037  0.232029\n4       165038  0.383344\n...        ...       ...\n110018  275052  0.043406\n110019  275053  0.101982\n110020  275054  0.028606\n110021  275055  0.183122\n110022  275056  0.166791\n\n[110023 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>Exited</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>165034</td>\n      <td>0.017886</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>165035</td>\n      <td>0.840448</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>165036</td>\n      <td>0.027796</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>165037</td>\n      <td>0.232029</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>165038</td>\n      <td>0.383344</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>110018</th>\n      <td>275052</td>\n      <td>0.043406</td>\n    </tr>\n    <tr>\n      <th>110019</th>\n      <td>275053</td>\n      <td>0.101982</td>\n    </tr>\n    <tr>\n      <th>110020</th>\n      <td>275054</td>\n      <td>0.028606</td>\n    </tr>\n    <tr>\n      <th>110021</th>\n      <td>275055</td>\n      <td>0.183122</td>\n    </tr>\n    <tr>\n      <th>110022</th>\n      <td>275056</td>\n      <td>0.166791</td>\n    </tr>\n  </tbody>\n</table>\n<p>110023 rows × 2 columns</p>\n</div>"},"metadata":{}}],"outputs_reference":"s3:deepnote-cell-outputs-production/94e79647-3dc4-423e-bba2-a256b6198688","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1715170460230,"execution_millis":724,"deepnote_to_be_reexecuted":true,"cell_id":"ad404b52f5894d7892f044e516aefbb6","deepnote_cell_type":"code"},"source":"submission_dict.to_csv('bankChurn_gabung.csv', index=False)","block_group":"271dcf9f53f8461ca073abccc0e239ab","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=2a79941c-6614-47fe-9427-0e9f23998893' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote_persisted_session":{"createdAt":"2024-05-08T13:00:09.339Z"},"deepnote_notebook_id":"0fdcc34fa1d74bcb9d37f7ccf5593303","deepnote_execution_queue":[]}}
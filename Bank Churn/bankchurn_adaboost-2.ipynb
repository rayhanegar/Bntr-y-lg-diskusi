{"cells":[{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"da0a9da33d0d4b8e964167d1c923c775","deepnote_cell_type":"text-cell-h1"},"source":"# Spaceship Adaboost Submission","block_group":"082f42d739ae4c33b25722d4d688de17"},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1716147051692,"execution_millis":3175,"deepnote_to_be_reexecuted":false,"cell_id":"51d247cd1acf4bd9bb50d3404a1efc1e","deepnote_cell_type":"code"},"source":"!pip install xgboost==2.0.3","block_group":"d312077bc5c24672a6ec875447391fed","execution_count":null,"outputs":[{"name":"stdout","text":"Requirement already satisfied: xgboost==2.0.3 in /root/venv/lib/python3.9/site-packages (2.0.3)\nRequirement already satisfied: scipy in /shared-libs/python3.9/py/lib/python3.9/site-packages (from xgboost==2.0.3) (1.9.3)\nRequirement already satisfied: numpy in /shared-libs/python3.9/py/lib/python3.9/site-packages (from xgboost==2.0.3) (1.23.4)\n\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/47cef82f-be20-4738-bf00-b5e7141eb774","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1716147054872,"execution_millis":3151,"deepnote_to_be_reexecuted":false,"cell_id":"10de08352ffb484792b4887f9656a3b6","deepnote_cell_type":"code"},"source":"!pip install lightgbm==4.3.0","block_group":"34f2a61ca65e488184ff3961c71caf10","execution_count":null,"outputs":[{"name":"stdout","text":"Requirement already satisfied: lightgbm==4.3.0 in /root/venv/lib/python3.9/site-packages (4.3.0)\nRequirement already satisfied: scipy in /shared-libs/python3.9/py/lib/python3.9/site-packages (from lightgbm==4.3.0) (1.9.3)\nRequirement already satisfied: numpy in /shared-libs/python3.9/py/lib/python3.9/site-packages (from lightgbm==4.3.0) (1.23.4)\n\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/317a5be9-e7dc-42c5-8ec6-7079788ed263","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1716147058030,"execution_millis":3122,"deepnote_to_be_reexecuted":false,"cell_id":"b6575b8d4eb1461895e3b4a99c72f2fa","deepnote_cell_type":"code"},"source":"!pip install lightgbm==4.3.0","block_group":"e4967277cb7b4dcc913d5d7e266f7df1","execution_count":null,"outputs":[{"name":"stdout","text":"Requirement already satisfied: lightgbm==4.3.0 in /root/venv/lib/python3.9/site-packages (4.3.0)\nRequirement already satisfied: scipy in /shared-libs/python3.9/py/lib/python3.9/site-packages (from lightgbm==4.3.0) (1.9.3)\nRequirement already satisfied: numpy in /shared-libs/python3.9/py/lib/python3.9/site-packages (from lightgbm==4.3.0) (1.23.4)\n\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/6097d468-3831-4414-b4d3-ac7cb4b98fe1","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1716147061158,"execution_millis":3162,"deepnote_to_be_reexecuted":false,"cell_id":"cb3b5cc3de2b41c38beb0c1cd7aca1dc","deepnote_cell_type":"code"},"source":"!pip install catboost==1.2.5","block_group":"b9a664b420444b7fbf25a0becde68b5e","execution_count":null,"outputs":[{"name":"stdout","text":"Requirement already satisfied: catboost==1.2.5 in /root/venv/lib/python3.9/site-packages (1.2.5)\nRequirement already satisfied: pandas>=0.24 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from catboost==1.2.5) (2.1.4)\nRequirement already satisfied: matplotlib in /shared-libs/python3.9/py/lib/python3.9/site-packages (from catboost==1.2.5) (3.6.0)\nRequirement already satisfied: plotly in /shared-libs/python3.9/py/lib/python3.9/site-packages (from catboost==1.2.5) (5.10.0)\nRequirement already satisfied: six in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from catboost==1.2.5) (1.16.0)\nRequirement already satisfied: scipy in /shared-libs/python3.9/py/lib/python3.9/site-packages (from catboost==1.2.5) (1.9.3)\nRequirement already satisfied: numpy>=1.16.0 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from catboost==1.2.5) (1.23.4)\nRequirement already satisfied: graphviz in /root/venv/lib/python3.9/site-packages (from catboost==1.2.5) (0.20.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from pandas>=0.24->catboost==1.2.5) (2.8.2)\nRequirement already satisfied: tzdata>=2022.1 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from pandas>=0.24->catboost==1.2.5) (2022.5)\nRequirement already satisfied: pytz>=2020.1 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from pandas>=0.24->catboost==1.2.5) (2022.5)\nRequirement already satisfied: pyparsing>=2.2.1 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from matplotlib->catboost==1.2.5) (3.0.9)\nRequirement already satisfied: cycler>=0.10 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from matplotlib->catboost==1.2.5) (0.11.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from matplotlib->catboost==1.2.5) (1.4.4)\nRequirement already satisfied: contourpy>=1.0.1 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from matplotlib->catboost==1.2.5) (1.0.5)\nRequirement already satisfied: pillow>=6.2.0 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from matplotlib->catboost==1.2.5) (9.2.0)\nRequirement already satisfied: fonttools>=4.22.0 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from matplotlib->catboost==1.2.5) (4.37.4)\nRequirement already satisfied: packaging>=20.0 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from matplotlib->catboost==1.2.5) (21.3)\nRequirement already satisfied: tenacity>=6.2.0 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from plotly->catboost==1.2.5) (8.1.0)\n\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/6d00e488-0328-4e31-b3f3-afd7316b919f","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1716147064326,"execution_millis":810,"deepnote_to_be_reexecuted":false,"cell_id":"b0a9aaacb4124808a4e743f66e2c770a","deepnote_cell_type":"code"},"source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.ensemble import StackingClassifier","block_group":"6d67ae98cbb84c27b03fe6f9075b0099","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"2c2d35cf47714e2593fd4069576bb324","deepnote_cell_type":"text-cell-h1"},"source":"# Bring the Data In","block_group":"0c79d79c638f4ac7a0e36c426c763934"},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1716147065146,"execution_millis":143357,"deepnote_to_be_reexecuted":false,"cell_id":"17167b1d0a8e4d52a89d131f6edcdde1","deepnote_cell_type":"code"},"source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\nfrom sklearn.metrics import make_scorer, accuracy_score\n\n# Load data\nX_df = pd.read_csv('preprocessedbankchurn_train.csv')\nX_df.drop(\"Exited\", axis=1, inplace=True)\ny_df = pd.read_csv('preprocessedbankchurn_train.csv')['Exited']\nX_submission = pd.read_csv('preprocessedbankchurn_test.csv')\n\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(X_df, y_df, test_size=0.2, random_state=42)\n\n# Base estimator\nbase_estimator = DecisionTreeClassifier(max_depth=1)\n\n# Create pipeline\npipeline = Pipeline([\n    ('scaler', StandardScaler()),  # Normalize data\n    ('classifier', AdaBoostClassifier(base_estimator=base_estimator))\n])\n\n\n# Define GridSearch parameters\nparameters = {\n    'classifier__n_estimators': [200],\n    'classifier__learning_rate': [1]\n}\n\n\n\ndef custom_accuracy(y_true, y_prob, threshold=0.5):\n    y_pred = (y_prob[:, 1] >= threshold).astype(int)\n    return accuracy_score(y_true, y_pred)\n\n# Membuat scorer kustom\naccuracy_scorer = make_scorer(custom_accuracy, needs_proba=True)\n\n# Menggunakan dalam GridSearchCV\ngrid_search = GridSearchCV(pipeline, parameters, scoring=accuracy_scorer, cv=5)\ngrid_search.fit(X_train, y_train)\n\n\n# Evaluate the model\nbest_model = grid_search.best_estimator_\ny_pred_proba = best_model.predict_proba(X_test)[:, 1]  # Get probabilities for the positive class\nprint(\"Best parameters:\", grid_search.best_params_)\nprint(\"Accuracy on test set:\", accuracy_score(y_test, y_pred_proba))\n\n# Evaluation on the test set\ny_pred = best_model.predict(X_test)\nprint(classification_report(y_test, y_pred))\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test, y_pred))\n\n# Fit the model on all training data\ngrid_search.fit(X_df, y_df)\n\n\n","block_group":"1ef835adfdc04c2c8f92f4e62c1cbe16","execution_count":null,"outputs":[{"name":"stderr","text":"/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 767, in _score\n    scores = scorer(estimator, X_test, y_test)\n  File \"/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 219, in __call__\n    return self._score(\n  File \"/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 312, in _score\n    return self._sign * self._score_func(y, y_pred, **self._kwargs)\n  File \"/tmp/ipykernel_673/1524278816.py\", line 38, in custom_accuracy\n    y_pred = (y_prob[:, 1] >= threshold).astype(int)\nIndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n\n  warnings.warn(\n/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 767, in _score\n    scores = scorer(estimator, X_test, y_test)\n  File \"/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 219, in __call__\n    return self._score(\n  File \"/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 312, in _score\n    return self._sign * self._score_func(y, y_pred, **self._kwargs)\n  File \"/tmp/ipykernel_673/1524278816.py\", line 38, in custom_accuracy\n    y_pred = (y_prob[:, 1] >= threshold).astype(int)\nIndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n\n  warnings.warn(\n/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 767, in _score\n    scores = scorer(estimator, X_test, y_test)\n  File \"/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 219, in __call__\n    return self._score(\n  File \"/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 312, in _score\n    return self._sign * self._score_func(y, y_pred, **self._kwargs)\n  File \"/tmp/ipykernel_673/1524278816.py\", line 38, in custom_accuracy\n    y_pred = (y_prob[:, 1] >= threshold).astype(int)\nIndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n\n  warnings.warn(\n/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 767, in _score\n    scores = scorer(estimator, X_test, y_test)\n  File \"/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 219, in __call__\n    return self._score(\n  File \"/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 312, in _score\n    return self._sign * self._score_func(y, y_pred, **self._kwargs)\n  File \"/tmp/ipykernel_673/1524278816.py\", line 38, in custom_accuracy\n    y_pred = (y_prob[:, 1] >= threshold).astype(int)\nIndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n\n  warnings.warn(\n/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 767, in _score\n    scores = scorer(estimator, X_test, y_test)\n  File \"/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 219, in __call__\n    return self._score(\n  File \"/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 312, in _score\n    return self._sign * self._score_func(y, y_pred, **self._kwargs)\n  File \"/tmp/ipykernel_673/1524278816.py\", line 38, in custom_accuracy\n    y_pred = (y_prob[:, 1] >= threshold).astype(int)\nIndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n\n  warnings.warn(\n/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/model_selection/_search.py:953: UserWarning: One or more of the test scores are non-finite: [nan]\n  warnings.warn(\nBest parameters: {'classifier__learning_rate': 1, 'classifier__n_estimators': 200}\n","output_type":"stream"},{"output_type":"error","ename":"ValueError","evalue":"Classification metrics can't handle a mix of binary and continuous targets","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn [6], line 53\u001b[0m\n\u001b[1;32m     51\u001b[0m y_pred_proba \u001b[38;5;241m=\u001b[39m best_model\u001b[38;5;241m.\u001b[39mpredict_proba(X_test)[:, \u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# Get probabilities for the positive class\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest parameters:\u001b[39m\u001b[38;5;124m\"\u001b[39m, grid_search\u001b[38;5;241m.\u001b[39mbest_params_)\n\u001b[0;32m---> 53\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy on test set:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43maccuracy_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred_proba\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Evaluation on the test set\u001b[39;00m\n\u001b[1;32m     56\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m best_model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n","File \u001b[0;32m/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/metrics/_classification.py:211\u001b[0m, in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;124;03m\"\"\"Accuracy classification score.\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \n\u001b[1;32m    147\u001b[0m \u001b[38;5;124;03mIn multilabel classification, this function computes subset accuracy:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;124;03m0.5\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;66;03m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[0;32m--> 211\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n","File \u001b[0;32m/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/metrics/_classification.py:93\u001b[0m, in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     90\u001b[0m     y_type \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y_type) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClassification metrics can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt handle a mix of \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m targets\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m     95\u001b[0m             type_true, type_pred\n\u001b[1;32m     96\u001b[0m         )\n\u001b[1;32m     97\u001b[0m     )\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# We can't have more than one value on y_type => The set is no more needed\u001b[39;00m\n\u001b[1;32m    100\u001b[0m y_type \u001b[38;5;241m=\u001b[39m y_type\u001b[38;5;241m.\u001b[39mpop()\n","\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of binary and continuous targets"]}],"outputs_reference":"s3:deepnote-cell-outputs-production/8ed101ad-58db-42a4-9310-00d6d0a6c74f","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1715811778930,"execution_millis":0,"deepnote_to_be_reexecuted":true,"cell_id":"2d4f17ce0c8c41729549fc7ae9de9289","deepnote_cell_type":"code"},"source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.pipeline import Pipeline\n\n# Load data\nX_df = pd.read_csv('preprocessedbankchurn_train.csv')\nX_df.drop(\"Exited\", axis=1, inplace=True)\ny_df = pd.read_csv('preprocessedbankchurn_train.csv')['Exited']\nX_submission = pd.read_csv('preprocessedbankchurn_test.csv')\n\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(X_df, y_df, test_size=0.2, random_state=42)\n\n# Define base models\nbase_models = [\n    ('rf', RandomForestClassifier(random_state=42)),\n    ('gb', GradientBoostingClassifier(random_state=42)),\n    ('xgb', XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)),\n    ('lgbm', LGBMClassifier(random_state=42)),\n    ('catboost', CatBoostClassifier(verbose=0, random_state=42)),\n    ('mlp', MLPClassifier(random_state=42))\n]\n\n# Define the meta-model\nmeta_model = CatBoostClassifier(verbose=0, random_state=42)\n\n# Create the voting classifier with soft voting\nvoting_clf = VotingClassifier(estimators=base_models, voting='soft')\n\n# Feature selection\nselector = SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=42))\nX_train_selected = selector.fit_transform(X_train, y_train)\nX_test_selected = selector.transform(X_test)\n\n# Train the voting classifier\nvoting_clf.fit(X_train_selected, y_train)\n\n# Predict and evaluate\ny_pred = voting_clf.predict(X_test_selected)\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'Accuracy of voting ensemble: {accuracy:.4f}')\n\n# Create the stacking ensemble with voting classifier as meta-model\nstacking_clf = StackingClassifier(\n    estimators=base_models,\n    final_estimator=voting_clf,\n    cv=5\n)\n\n# Define parameter grid for RandomizedSearchCV\nparam_grid = {\n    'rf__n_estimators': [100],\n    'rf__max_depth': [10],\n    'gb__n_estimators': [100],\n    'gb__learning_rate': [0.1],\n    'xgb__n_estimators': [100],\n    'xgb__learning_rate': [0.1],\n    'lgbm__n_estimators': [100],\n    'lgbm__learning_rate': [0.1],\n    'catboost__iterations': [100],\n    'catboost__learning_rate': [0.1],\n    'mlp__hidden_layer_sizes': [(50, 50, 50)],\n    'mlp__alpha': [0.01]\n}\n\n# Hyperparameter tuning with RandomizedSearchCV\nrandom_search = RandomizedSearchCV(estimator=stacking_clf, param_distributions=param_grid, n_iter=50, cv=3, n_jobs=-1, scoring='accuracy', random_state=42)\nrandom_search.fit(X_train_selected, y_train)\n\n# Predict and evaluate the best model\nbest_model = random_search.best_estimator_\ny_pred = best_model.predict(X_test_selected)\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'Best model parameters: {random_search.best_params_}')\nprint(f'Accuracy of optimized stacking ensemble: {accuracy:.4f}')\n","block_group":"e776a397d124455b84a076a8dff8a49d","execution_count":null,"outputs":[{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 27966, number of negative: 104061\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003316 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1856\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 9\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211820 -> initscore=-1.313988\n[LightGBM] [Info] Start training from score -1.313988\nAccuracy of voting ensemble: 0.8491\n/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/model_selection/_search.py:306: UserWarning: The total space of parameters 1 is smaller than n_iter=50. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n  warnings.warn(\n[LightGBM] [Info] Number of positive: 18644, number of negative: 69374\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002225 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1854\n[LightGBM] [Info] Number of data points in the train set: 88018, number of used features: 9\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211820 -> initscore=-1.313988\n[LightGBM] [Info] Start training from score -1.313988\n[LightGBM] [Info] Number of positive: 14915, number of negative: 55499\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005929 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1853\n[LightGBM] [Info] Number of data points in the train set: 70414, number of used features: 9\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211819 -> initscore=-1.313998\n[LightGBM] [Info] Start training from score -1.313998\n[LightGBM] [Info] Number of positive: 14915, number of negative: 55499\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001746 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1855\n[LightGBM] [Info] Number of data points in the train set: 70414, number of used features: 9\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211819 -> initscore=-1.313998\n[LightGBM] [Info] Start training from score -1.313998\n[LightGBM] [Info] Number of positive: 14915, number of negative: 55499\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001792 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1850\n[LightGBM] [Info] Number of data points in the train set: 70414, number of used features: 9\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211819 -> initscore=-1.313998\n[LightGBM] [Info] Start training from score -1.313998\n[LightGBM] [Info] Number of positive: 14916, number of negative: 55499\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001744 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1855\n[LightGBM] [Info] Number of data points in the train set: 70415, number of used features: 9\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211830 -> initscore=-1.313931\n[LightGBM] [Info] Start training from score -1.313931\n[LightGBM] [Info] Number of positive: 14915, number of negative: 55500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001770 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1851\n[LightGBM] [Info] Number of data points in the train set: 70415, number of used features: 9\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211816 -> initscore=-1.314016\n[LightGBM] [Info] Start training from score -1.314016\n[LightGBM] [Info] Number of positive: 18644, number of negative: 69374\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001631 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1328\n[LightGBM] [Info] Number of data points in the train set: 88018, number of used features: 6\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211820 -> initscore=-1.313988\n[LightGBM] [Info] Start training from score -1.313988\n[LightGBM] [Info] Number of positive: 18644, number of negative: 69374\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002426 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1854\n[LightGBM] [Info] Number of data points in the train set: 88018, number of used features: 9\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211820 -> initscore=-1.313988\n[LightGBM] [Info] Start training from score -1.313988\n[LightGBM] [Info] Number of positive: 14915, number of negative: 55499\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006824 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1853\n[LightGBM] [Info] Number of data points in the train set: 70414, number of used features: 9\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211819 -> initscore=-1.313998\n[LightGBM] [Info] Start training from score -1.313998\n[LightGBM] [Info] Number of positive: 14915, number of negative: 55499\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001821 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1854\n[LightGBM] [Info] Number of data points in the train set: 70414, number of used features: 9\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211819 -> initscore=-1.313998\n[LightGBM] [Info] Start training from score -1.313998\n[LightGBM] [Info] Number of positive: 14915, number of negative: 55499\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001767 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1853\n[LightGBM] [Info] Number of data points in the train set: 70414, number of used features: 9\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211819 -> initscore=-1.313998\n[LightGBM] [Info] Start training from score -1.313998\n[LightGBM] [Info] Number of positive: 14916, number of negative: 55499\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001807 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1855\n[LightGBM] [Info] Number of data points in the train set: 70415, number of used features: 9\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211830 -> initscore=-1.313931\n[LightGBM] [Info] Start training from score -1.313931\n[LightGBM] [Info] Number of positive: 14915, number of negative: 55500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001801 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1852\n[LightGBM] [Info] Number of data points in the train set: 70415, number of used features: 9\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211816 -> initscore=-1.314016\n[LightGBM] [Info] Start training from score -1.314016\n[LightGBM] [Info] Number of positive: 18644, number of negative: 69374\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001603 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1278\n[LightGBM] [Info] Number of data points in the train set: 88018, number of used features: 6\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211820 -> initscore=-1.313988\n[LightGBM] [Info] Start training from score -1.313988\n[LightGBM] [Info] Number of positive: 18644, number of negative: 69374\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002248 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1853\n[LightGBM] [Info] Number of data points in the train set: 88018, number of used features: 9\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211820 -> initscore=-1.313988\n[LightGBM] [Info] Start training from score -1.313988\n[LightGBM] [Info] Number of positive: 14915, number of negative: 55499\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001739 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1854\n[LightGBM] [Info] Number of data points in the train set: 70414, number of used features: 9\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211819 -> initscore=-1.313998\n[LightGBM] [Info] Start training from score -1.313998\n[LightGBM] [Info] Number of positive: 14915, number of negative: 55499\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013413 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1853\n[LightGBM] [Info] Number of data points in the train set: 70414, number of used features: 9\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211819 -> initscore=-1.313998\n[LightGBM] [Info] Start training from score -1.313998\n[LightGBM] [Info] Number of positive: 14915, number of negative: 55499\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008551 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1853\n[LightGBM] [Info] Number of data points in the train set: 70414, number of used features: 9\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211819 -> initscore=-1.313998\n[LightGBM] [Info] Start training from score -1.313998\n[LightGBM] [Info] Number of positive: 14916, number of negative: 55499\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001781 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1855\n[LightGBM] [Info] Number of data points in the train set: 70415, number of used features: 9\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211830 -> initscore=-1.313931\n[LightGBM] [Info] Start training from score -1.313931\n[LightGBM] [Info] Number of positive: 14915, number of negative: 55500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001879 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1853\n[LightGBM] [Info] Number of data points in the train set: 70415, number of used features: 9\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211816 -> initscore=-1.314016\n[LightGBM] [Info] Start training from score -1.314016\n[LightGBM] [Info] Number of positive: 18644, number of negative: 69374\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.049557 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1280\n[LightGBM] [Info] Number of data points in the train set: 88018, number of used features: 6\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211820 -> initscore=-1.313988\n[LightGBM] [Info] Start training from score -1.313988\n[LightGBM] [Info] Number of positive: 27966, number of negative: 104061\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003228 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1856\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 9\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211820 -> initscore=-1.313988\n[LightGBM] [Info] Start training from score -1.313988\n[LightGBM] [Info] Number of positive: 22373, number of negative: 83248\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002905 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1854\n[LightGBM] [Info] Number of data points in the train set: 105621, number of used features: 9\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211823 -> initscore=-1.313969\n[LightGBM] [Info] Start training from score -1.313969\n[LightGBM] [Info] Number of positive: 22372, number of negative: 83249\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002778 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1855\n[LightGBM] [Info] Number of data points in the train set: 105621, number of used features: 9\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211814 -> initscore=-1.314026\n[LightGBM] [Info] Start training from score -1.314026\n[LightGBM] [Info] Number of positive: 22373, number of negative: 83249\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.057431 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1856\n[LightGBM] [Info] Number of data points in the train set: 105622, number of used features: 9\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211821 -> initscore=-1.313981\n[LightGBM] [Info] Start training from score -1.313981\n[LightGBM] [Info] Number of positive: 22373, number of negative: 83249\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002668 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1854\n[LightGBM] [Info] Number of data points in the train set: 105622, number of used features: 9\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211821 -> initscore=-1.313981\n[LightGBM] [Info] Start training from score -1.313981\n[LightGBM] [Info] Number of positive: 22373, number of negative: 83249\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058652 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1854\n[LightGBM] [Info] Number of data points in the train set: 105622, number of used features: 9\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211821 -> initscore=-1.313981\n[LightGBM] [Info] Start training from score -1.313981\n","output_type":"stream"},{"output_type":"error","ename":"KernelInterrupted","evalue":"Execution interrupted by the Jupyter kernel.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKernelInterrupted\u001b[0m: Execution interrupted by the Jupyter kernel."]}],"outputs_reference":"s3:deepnote-cell-outputs-production/73617032-592b-4c09-9a52-6dbfca4fc9cc","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1715758935142,"execution_millis":2204,"deepnote_to_be_reexecuted":true,"cell_id":"3b8e0f64a6a5499ab6ccd83e8a0983da","deepnote_cell_type":"code"},"source":"# Predict probabilities for submission data\ny_submission_proba = best_model.predict_proba(X_submission)[:, 1]  # Probabilities of Exited\nprint(y_submission_proba)\n\n# Prepare submission dictionary\nsubmission_dict = {'id': X_submission['id'], 'Exited': y_submission_proba}\nsubmission_dict = pd.DataFrame(submission_dict)\nsubmission_dict","block_group":"8631fd94f9444f55b802e1906d5a21f9","execution_count":null,"outputs":[{"name":"stdout","text":"[0.19618976 0.73099815 0.19885849 ... 0.20330466 0.28166607 0.2691457 ]\n","output_type":"stream"},{"output_type":"execute_result","execution_count":12,"data":{"application/vnd.deepnote.dataframe.v3+json":{"column_count":2,"row_count":110023,"columns":[{"name":"id","dtype":"int64"},{"name":"Exited","dtype":"float64"},{"name":"_deepnote_index_column","dtype":"int64"}],"rows":[{"id":165034,"Exited":0.19618975584929835,"_deepnote_index_column":0},{"id":165035,"Exited":0.7309981487834764,"_deepnote_index_column":1},{"id":165036,"Exited":0.1988584896284548,"_deepnote_index_column":2},{"id":165037,"Exited":0.31962716738266356,"_deepnote_index_column":3},{"id":165038,"Exited":0.4092644931767189,"_deepnote_index_column":4},{"id":165039,"Exited":0.19993910343631466,"_deepnote_index_column":5},{"id":165040,"Exited":0.21207018444544817,"_deepnote_index_column":6},{"id":165041,"Exited":0.22394108034048874,"_deepnote_index_column":7},{"id":165042,"Exited":0.5788979669122918,"_deepnote_index_column":8},{"id":165043,"Exited":0.1948671121975302,"_deepnote_index_column":9}]},"text/plain":"            id    Exited\n0       165034  0.196190\n1       165035  0.730998\n2       165036  0.198858\n3       165037  0.319627\n4       165038  0.409264\n...        ...       ...\n110018  275052  0.209023\n110019  275053  0.240303\n110020  275054  0.203305\n110021  275055  0.281666\n110022  275056  0.269146\n\n[110023 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>Exited</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>165034</td>\n      <td>0.196190</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>165035</td>\n      <td>0.730998</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>165036</td>\n      <td>0.198858</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>165037</td>\n      <td>0.319627</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>165038</td>\n      <td>0.409264</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>110018</th>\n      <td>275052</td>\n      <td>0.209023</td>\n    </tr>\n    <tr>\n      <th>110019</th>\n      <td>275053</td>\n      <td>0.240303</td>\n    </tr>\n    <tr>\n      <th>110020</th>\n      <td>275054</td>\n      <td>0.203305</td>\n    </tr>\n    <tr>\n      <th>110021</th>\n      <td>275055</td>\n      <td>0.281666</td>\n    </tr>\n    <tr>\n      <th>110022</th>\n      <td>275056</td>\n      <td>0.269146</td>\n    </tr>\n  </tbody>\n</table>\n<p>110023 rows Ã— 2 columns</p>\n</div>"},"metadata":{}}],"outputs_reference":"s3:deepnote-cell-outputs-production/bef51e0d-5fa5-42b9-a83e-8c77b9d4fc72","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1715758957859,"execution_millis":567,"deepnote_to_be_reexecuted":true,"cell_id":"584e759cd76c44f0b9f18ce1778e3eac","deepnote_cell_type":"code"},"source":"submission_dict.to_csv('banksurn_testensmbel3.csv', index=False)","block_group":"9f2e6f9d492e41deb8e6c451c49745cf","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1715032504876,"execution_millis":1026,"deepnote_to_be_reexecuted":true,"cell_id":"4c6e09aa89ba49aaa3d9da946d805fd3","deepnote_cell_type":"code"},"source":"X_df = pd.read_csv('preprocessedbankchurn_train.csv')\nX_df.drop(\"Exited\", axis=1, inplace=True)\ny_df = pd.read_csv('preprocessedbankchurn_train.csv')\ny_df = y_df.loc[:, ['Exited']]\nX_submission = pd.read_csv('preprocessedbankchurn_test.csv')\n\nX_df.info()","block_group":"9acd7481bdec43f3ba43972876e9aaa0","execution_count":null,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 165034 entries, 0 to 165033\nData columns (total 17 columns):\n #   Column               Non-Null Count   Dtype  \n---  ------               --------------   -----  \n 0   id                   165034 non-null  int64  \n 1   CustomerId           165034 non-null  int64  \n 2   CreditScore          165034 non-null  int64  \n 3   Age                  165034 non-null  float64\n 4   Tenure               165034 non-null  int64  \n 5   Balance              165034 non-null  float64\n 6   NumOfProducts        165034 non-null  int64  \n 7   HasCrCard            165034 non-null  float64\n 8   IsActiveMember       165034 non-null  float64\n 9   EstimatedSalary      165034 non-null  float64\n 10  France               165034 non-null  int64  \n 11  Germany              165034 non-null  int64  \n 12  Spain                165034 non-null  int64  \n 13  Female               165034 non-null  int64  \n 14  Male                 165034 non-null  int64  \n 15  ExitedfamilySize     165034 non-null  float64\n 16  NotExitedfamilySize  165034 non-null  float64\ndtypes: float64(7), int64(10)\nmemory usage: 21.4 MB\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/13bd9ec5-07df-492b-91ea-264a42d3cde5","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1715032505908,"execution_millis":372,"deepnote_to_be_reexecuted":true,"cell_id":"7fa11cff749044b49668c387e962c0e9","deepnote_cell_type":"code"},"source":"X_submission.info()","block_group":"91575ba172d8481e916462e6b5fcda67","execution_count":null,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 110023 entries, 0 to 110022\nData columns (total 17 columns):\n #   Column               Non-Null Count   Dtype  \n---  ------               --------------   -----  \n 0   id                   110023 non-null  int64  \n 1   CustomerId           110023 non-null  int64  \n 2   CreditScore          110023 non-null  int64  \n 3   Age                  110023 non-null  float64\n 4   Tenure               110023 non-null  int64  \n 5   Balance              110023 non-null  float64\n 6   NumOfProducts        110023 non-null  int64  \n 7   HasCrCard            110023 non-null  float64\n 8   IsActiveMember       110023 non-null  float64\n 9   EstimatedSalary      110023 non-null  float64\n 10  France               110023 non-null  int64  \n 11  Germany              110023 non-null  int64  \n 12  Spain                110023 non-null  int64  \n 13  Female               110023 non-null  int64  \n 14  Male                 110023 non-null  int64  \n 15  ExitedfamilySize     110023 non-null  float64\n 16  NotExitedfamilySize  110023 non-null  float64\ndtypes: float64(7), int64(10)\nmemory usage: 14.3 MB\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/5ed5bdfc-129c-49f9-a9b4-56734a154320","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1715032505938,"execution_millis":342,"deepnote_to_be_reexecuted":true,"cell_id":"00147316ed174368ba779c25a706fd28","deepnote_cell_type":"code"},"source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X_df, y_df, test_size=0.2, random_state=42)\n# Mengubah target menjadi 1D array\ny_train = y_train['Exited'].values\ny_test = y_test['Exited'].values\n\n\n# Base estimator\nbase_estimator = DecisionTreeClassifier(max_depth=1)\n\n# Membuat pipeline\npipeline = Pipeline([\n    ('scaler', StandardScaler()),  # Normalisasi data\n    ('classifier', AdaBoostClassifier(base_estimator=base_estimator))\n])\n\nparameters = {\n    'classifier__n_estimators': [50, 100, 200],\n    'classifier__learning_rate': [0.01, 0.1, 1]\n}\n\n# Menggunakan StratifiedKFold untuk handling imbalanced data\ncv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)","block_group":"f3e98e79ef544fb98587f50a6297debd","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"337505c1b7f840ff93c0c7f911970490","deepnote_cell_type":"text-cell-h1"},"source":"# Model Training","block_group":"e01f12f006324577a8685ffcb2e53c45"},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1715032505974,"execution_millis":317667,"deepnote_to_be_reexecuted":true,"cell_id":"873d1aada75e49869076282d021f3753","deepnote_cell_type":"code"},"source":"# Membuat GridSearchCV\ngrid_search = GridSearchCV(pipeline, parameters, cv=cv, scoring='accuracy', verbose=1)\ngrid_search.fit(X_train, y_train)\n\n#Evaluasi model\nbest_model = grid_search.best_estimator_\ny_pred = best_model.predict(X_test)\nprint(\"Best parameters:\", grid_search.best_params_)\nprint(\"Accuracy on test set:\", accuracy_score(y_test, y_pred))","block_group":"ffa1a5a6dc0742f6a823ee9eafca1c28","execution_count":null,"outputs":[{"name":"stdout","text":"Fitting 10 folds for each of 9 candidates, totalling 90 fits\n","output_type":"stream"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn [5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Membuat GridSearchCV\u001b[39;00m\n\u001b[1;32m      2\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(pipeline, parameters, cv\u001b[38;5;241m=\u001b[39mcv, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#Evaluasi model\u001b[39;00m\n\u001b[1;32m      6\u001b[0m best_model \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_estimator_\n","File \u001b[0;32m/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/model_selection/_search.py:875\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    869\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    870\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    871\u001b[0m     )\n\u001b[1;32m    873\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 875\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    878\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    879\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n","File \u001b[0;32m/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/model_selection/_search.py:1379\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1377\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1378\u001b[0m     \u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1379\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/model_selection/_search.py:822\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    815\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    816\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    817\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    818\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    819\u001b[0m         )\n\u001b[1;32m    820\u001b[0m     )\n\u001b[0;32m--> 822\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    823\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    828\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    829\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    840\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    841\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    842\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    843\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    844\u001b[0m     )\n","File \u001b[0;32m/shared-libs/python3.9/py/lib/python3.9/site-packages/joblib/parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1085\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1086\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1088\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1089\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1092\u001b[0m     \u001b[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[1;32m   1093\u001b[0m     \u001b[38;5;66;03m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[1;32m   1094\u001b[0m     \u001b[38;5;66;03m# consumption.\u001b[39;00m\n","File \u001b[0;32m/shared-libs/python3.9/py/lib/python3.9/site-packages/joblib/parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 901\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n","File \u001b[0;32m/shared-libs/python3.9/py/lib/python3.9/site-packages/joblib/parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    818\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[0;32m--> 819\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    820\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    823\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n","File \u001b[0;32m/shared-libs/python3.9/py/lib/python3.9/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n","File \u001b[0;32m/shared-libs/python3.9/py/lib/python3.9/site-packages/joblib/_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[1;32m    595\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/shared-libs/python3.9/py/lib/python3.9/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n","File \u001b[0;32m/shared-libs/python3.9/py/lib/python3.9/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n","File \u001b[0;32m/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/utils/fixes.py:117\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig):\n\u001b[0;32m--> 117\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    684\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m    685\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 686\u001b[0m         \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    689\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[1;32m    690\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n","File \u001b[0;32m/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/pipeline.py:382\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    381\u001b[0m         fit_params_last_step \u001b[38;5;241m=\u001b[39m fit_params_steps[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[0;32m--> 382\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_final_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params_last_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n","File \u001b[0;32m/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:506\u001b[0m, in \u001b[0;36mAdaBoostClassifier.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    500\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    501\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAlgorithm must be \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSAMME\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSAMME.R\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    502\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malgorithm\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    503\u001b[0m     )\n\u001b[1;32m    505\u001b[0m \u001b[38;5;66;03m# Fit\u001b[39;00m\n\u001b[0;32m--> 506\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:160\u001b[0m, in \u001b[0;36mBaseWeightBoosting.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    156\u001b[0m random_state \u001b[38;5;241m=\u001b[39m check_random_state(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state)\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m iboost \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_estimators):\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;66;03m# Boosting step\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m     sample_weight, estimator_weight, estimator_error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_boost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43miboost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# Early termination\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","File \u001b[0;32m/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:568\u001b[0m, in \u001b[0;36mAdaBoostClassifier._boost\u001b[0;34m(self, iboost, X, y, sample_weight, random_state)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[38;5;124;03m\"\"\"Implement a single boost.\u001b[39;00m\n\u001b[1;32m    530\u001b[0m \n\u001b[1;32m    531\u001b[0m \u001b[38;5;124;03mPerform a single boost according to the real multi-class SAMME.R\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;124;03m    If None then boosting has terminated early.\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malgorithm \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSAMME.R\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 568\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_boost_real\u001b[49m\u001b[43m(\u001b[49m\u001b[43miboost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# elif self.algorithm == \"SAMME\":\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_boost_discrete(iboost, X, y, sample_weight, random_state)\n","File \u001b[0;32m/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:577\u001b[0m, in \u001b[0;36mAdaBoostClassifier._boost_real\u001b[0;34m(self, iboost, X, y, sample_weight, random_state)\u001b[0m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;124;03m\"\"\"Implement a single boost using the SAMME.R real algorithm.\"\"\"\u001b[39;00m\n\u001b[1;32m    575\u001b[0m estimator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m--> 577\u001b[0m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m y_predict_proba \u001b[38;5;241m=\u001b[39m estimator\u001b[38;5;241m.\u001b[39mpredict_proba(X)\n\u001b[1;32m    581\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m iboost \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n","File \u001b[0;32m/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/tree/_classes.py:969\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    940\u001b[0m     \u001b[38;5;124;03m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[1;32m    941\u001b[0m \n\u001b[1;32m    942\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    966\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[1;32m    967\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 969\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    975\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n","File \u001b[0;32m/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/tree/_classes.py:458\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    448\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[1;32m    449\u001b[0m         splitter,\n\u001b[1;32m    450\u001b[0m         min_samples_split,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    455\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[1;32m    456\u001b[0m     )\n\u001b[0;32m--> 458\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"outputs_reference":"s3:deepnote-cell-outputs-production/54f532e9-6691-4a0d-abfb-0af459108c1d","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1715027000161,"execution_millis":862,"deepnote_to_be_reexecuted":true,"cell_id":"cea63de60d0d41a4bd04a0b91db0746f","deepnote_cell_type":"code"},"source":"# Evaluasi pada validation set\ny_pred = grid_search.predict(X_test)\nprint(classification_report(y_test, y_pred))\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test, y_pred))","block_group":"77ebd873ac4449b9bc8a4851f956eb72","execution_count":null,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0       0.89      0.95      0.92     26052\n           1       0.75      0.56      0.64      6955\n\n    accuracy                           0.87     33007\n   macro avg       0.82      0.76      0.78     33007\nweighted avg       0.86      0.87      0.86     33007\n\nConfusion Matrix:\n[[24755  1297]\n [ 3028  3927]]\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/ffdcfb44-d551-4ec1-a7f7-dd00bdd97a26","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1715027001038,"execution_millis":957,"deepnote_to_be_reexecuted":true,"cell_id":"646acc332a1a49828adad013d01d5974","deepnote_cell_type":"code"},"source":"best_ada = grid_search.best_estimator_\ny_pred_light = best_ada.predict(X_test)\nprint(\"Best parameters:\", grid_search.best_params_)\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred_light))\nprint(\"Classification Report:\\n\", classification_report(y_test, y_pred_light))\nprint(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_light))","block_group":"4688de50a0874df890980b8a72f907a7","execution_count":null,"outputs":[{"name":"stdout","text":"Best parameters: {'classifier__learning_rate': 1, 'classifier__n_estimators': 200}\nAccuracy: 0.8689671887781379\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.89      0.95      0.92     26052\n           1       0.75      0.56      0.64      6955\n\n    accuracy                           0.87     33007\n   macro avg       0.82      0.76      0.78     33007\nweighted avg       0.86      0.87      0.86     33007\n\nConfusion Matrix:\n [[24755  1297]\n [ 3028  3927]]\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/74af3e9b-4118-4734-8fa1-d0a9f2de2512","content_dependencies":null},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"a0203cbdf31b4007be434418f5e4486c","deepnote_cell_type":"text-cell-h1"},"source":"# Submission Prediction","block_group":"4c856cb090fc4ca8847589c39997e4a2"},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1715027001999,"execution_millis":248,"deepnote_to_be_reexecuted":true,"cell_id":"19e18cb5a9fb4173b7c6f71d32d5aaab","deepnote_cell_type":"code"},"source":"X_submission.info()","block_group":"ba8a558937cc4653971c76f73c5a05ab","execution_count":null,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 110023 entries, 0 to 110022\nData columns (total 17 columns):\n #   Column               Non-Null Count   Dtype  \n---  ------               --------------   -----  \n 0   id                   110023 non-null  int64  \n 1   CustomerId           110023 non-null  int64  \n 2   CreditScore          110023 non-null  int64  \n 3   Age                  110023 non-null  float64\n 4   Tenure               110023 non-null  int64  \n 5   Balance              110023 non-null  float64\n 6   NumOfProducts        110023 non-null  int64  \n 7   HasCrCard            110023 non-null  float64\n 8   IsActiveMember       110023 non-null  float64\n 9   EstimatedSalary      110023 non-null  float64\n 10  France               110023 non-null  int64  \n 11  Germany              110023 non-null  int64  \n 12  Spain                110023 non-null  int64  \n 13  Female               110023 non-null  int64  \n 14  Male                 110023 non-null  int64  \n 15  ExitedfamilySize     110023 non-null  float64\n 16  NotExitedfamilySize  110023 non-null  float64\ndtypes: float64(7), int64(10)\nmemory usage: 14.3 MB\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/107ebd4e-f8eb-4e24-9ca9-bf98f9767019","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1715027002017,"execution_millis":231,"deepnote_to_be_reexecuted":true,"cell_id":"ce040da7ce17464cbee7b0d0b0af6f63","deepnote_cell_type":"code"},"source":"X_train.info()","block_group":"9e9b41c78fb6488ca7f85e327ff652d8","execution_count":null,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nIndex: 132027 entries, 149380 to 121958\nData columns (total 17 columns):\n #   Column               Non-Null Count   Dtype  \n---  ------               --------------   -----  \n 0   id                   132027 non-null  int64  \n 1   CustomerId           132027 non-null  int64  \n 2   CreditScore          132027 non-null  int64  \n 3   Age                  132027 non-null  float64\n 4   Tenure               132027 non-null  int64  \n 5   Balance              132027 non-null  float64\n 6   NumOfProducts        132027 non-null  int64  \n 7   HasCrCard            132027 non-null  float64\n 8   IsActiveMember       132027 non-null  float64\n 9   EstimatedSalary      132027 non-null  float64\n 10  France               132027 non-null  int64  \n 11  Germany              132027 non-null  int64  \n 12  Spain                132027 non-null  int64  \n 13  Female               132027 non-null  int64  \n 14  Male                 132027 non-null  int64  \n 15  ExitedfamilySize     132027 non-null  float64\n 16  NotExitedfamilySize  132027 non-null  float64\ndtypes: float64(7), int64(10)\nmemory usage: 18.1 MB\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/24447dea-257e-4157-9200-28d2e6f1d90f","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1715029630353,"execution_millis":1733102,"deepnote_to_be_reexecuted":true,"cell_id":"e65e6f1b85a14b0abfe5574de4902adc","deepnote_cell_type":"code"},"source":"# For training, we use ALL data from spaceship_train_X_v2.csv and spaceship_train_y.csv\ngrid_search.fit(X_df, y_df['Exited'].values)\n# Prediksi data submission\ny_prediction = grid_search.predict(X_submission)\nprint(y_prediction)\n","block_group":"51e9e9b1a74b47e3beec7c41ad94151c","execution_count":null,"outputs":[{"name":"stdout","text":"Fitting 10 folds for each of 9 candidates, totalling 90 fits\n[0 1 0 ... 0 0 0]\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/f8daae55-5b45-467d-b1d5-e29f94116e0e","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1715028721277,"execution_millis":178,"deepnote_to_be_reexecuted":true,"cell_id":"6ba63b74a7bc4bf986006f50585d1fdc","deepnote_cell_type":"code"},"source":"X_submission.info()","block_group":"e5126cdabfd546f1a59d03b6b2bb9e4d","execution_count":null,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 110023 entries, 0 to 110022\nData columns (total 17 columns):\n #   Column               Non-Null Count   Dtype  \n---  ------               --------------   -----  \n 0   id                   110023 non-null  int64  \n 1   CustomerId           110023 non-null  int64  \n 2   CreditScore          110023 non-null  int64  \n 3   Age                  110023 non-null  float64\n 4   Tenure               110023 non-null  int64  \n 5   Balance              110023 non-null  float64\n 6   NumOfProducts        110023 non-null  int64  \n 7   HasCrCard            110023 non-null  float64\n 8   IsActiveMember       110023 non-null  float64\n 9   EstimatedSalary      110023 non-null  float64\n 10  France               110023 non-null  int64  \n 11  Germany              110023 non-null  int64  \n 12  Spain                110023 non-null  int64  \n 13  Female               110023 non-null  int64  \n 14  Male                 110023 non-null  int64  \n 15  ExitedfamilySize     110023 non-null  float64\n 16  NotExitedfamilySize  110023 non-null  float64\ndtypes: float64(7), int64(10)\nmemory usage: 14.3 MB\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/222978d4-2a24-4fb6-8f7f-0cd67d054dce","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1715028721278,"execution_millis":177,"deepnote_to_be_reexecuted":true,"cell_id":"3c859423e7f042d59a6838ae8f876380","deepnote_cell_type":"code"},"source":"X_train.info()","block_group":"e27540986b294c2e9e761901e7387d2c","execution_count":null,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nIndex: 132027 entries, 149380 to 121958\nData columns (total 17 columns):\n #   Column               Non-Null Count   Dtype  \n---  ------               --------------   -----  \n 0   id                   132027 non-null  int64  \n 1   CustomerId           132027 non-null  int64  \n 2   CreditScore          132027 non-null  int64  \n 3   Age                  132027 non-null  float64\n 4   Tenure               132027 non-null  int64  \n 5   Balance              132027 non-null  float64\n 6   NumOfProducts        132027 non-null  int64  \n 7   HasCrCard            132027 non-null  float64\n 8   IsActiveMember       132027 non-null  float64\n 9   EstimatedSalary      132027 non-null  float64\n 10  France               132027 non-null  int64  \n 11  Germany              132027 non-null  int64  \n 12  Spain                132027 non-null  int64  \n 13  Female               132027 non-null  int64  \n 14  Male                 132027 non-null  int64  \n 15  ExitedfamilySize     132027 non-null  float64\n 16  NotExitedfamilySize  132027 non-null  float64\ndtypes: float64(7), int64(10)\nmemory usage: 18.1 MB\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/a8a1299c-986c-4e17-b9fd-88a220a4d7a0","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1715031363457,"execution_millis":232,"deepnote_table_state":{"sortBy":[],"filters":[],"pageSize":100,"pageIndex":0},"deepnote_table_loading":false,"deepnote_to_be_reexecuted":true,"cell_id":"771da9af1d7c4c088afcb1664d36f37a","deepnote_cell_type":"code"},"source":"submission_dict = {'id':X_submission['id'], 'Exited':y_prediction}\nsubmission_dict = pd.DataFrame(submission_dict)\nsubmission_dict","block_group":"dd269131f3a14a0d86bd4f70d52f78ba","execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":18,"data":{"application/vnd.deepnote.dataframe.v3+json":{"column_count":2,"row_count":110023,"columns":[{"name":"id","dtype":"int64"},{"name":"Exited","dtype":"int64"},{"name":"_deepnote_index_column","dtype":"int64"}],"rows":[{"id":165034,"Exited":0,"_deepnote_index_column":0},{"id":165035,"Exited":1,"_deepnote_index_column":1},{"id":165036,"Exited":0,"_deepnote_index_column":2},{"id":165037,"Exited":0,"_deepnote_index_column":3},{"id":165038,"Exited":0,"_deepnote_index_column":4},{"id":165039,"Exited":0,"_deepnote_index_column":5},{"id":165040,"Exited":0,"_deepnote_index_column":6},{"id":165041,"Exited":0,"_deepnote_index_column":7},{"id":165042,"Exited":1,"_deepnote_index_column":8},{"id":165043,"Exited":0,"_deepnote_index_column":9},{"id":165044,"Exited":0,"_deepnote_index_column":10},{"id":165045,"Exited":0,"_deepnote_index_column":11},{"id":165046,"Exited":0,"_deepnote_index_column":12},{"id":165047,"Exited":0,"_deepnote_index_column":13},{"id":165048,"Exited":0,"_deepnote_index_column":14},{"id":165049,"Exited":0,"_deepnote_index_column":15},{"id":165050,"Exited":0,"_deepnote_index_column":16},{"id":165051,"Exited":0,"_deepnote_index_column":17},{"id":165052,"Exited":0,"_deepnote_index_column":18},{"id":165053,"Exited":0,"_deepnote_index_column":19},{"id":165054,"Exited":0,"_deepnote_index_column":20},{"id":165055,"Exited":0,"_deepnote_index_column":21},{"id":165056,"Exited":0,"_deepnote_index_column":22},{"id":165057,"Exited":0,"_deepnote_index_column":23},{"id":165058,"Exited":0,"_deepnote_index_column":24},{"id":165059,"Exited":0,"_deepnote_index_column":25},{"id":165060,"Exited":1,"_deepnote_index_column":26},{"id":165061,"Exited":0,"_deepnote_index_column":27},{"id":165062,"Exited":0,"_deepnote_index_column":28},{"id":165063,"Exited":0,"_deepnote_index_column":29},{"id":165064,"Exited":0,"_deepnote_index_column":30},{"id":165065,"Exited":0,"_deepnote_index_column":31},{"id":165066,"Exited":0,"_deepnote_index_column":32},{"id":165067,"Exited":0,"_deepnote_index_column":33},{"id":165068,"Exited":0,"_deepnote_index_column":34},{"id":165069,"Exited":0,"_deepnote_index_column":35},{"id":165070,"Exited":1,"_deepnote_index_column":36},{"id":165071,"Exited":0,"_deepnote_index_column":37},{"id":165072,"Exited":0,"_deepnote_index_column":38},{"id":165073,"Exited":1,"_deepnote_index_column":39},{"id":165074,"Exited":0,"_deepnote_index_column":40},{"id":165075,"Exited":0,"_deepnote_index_column":41},{"id":165076,"Exited":0,"_deepnote_index_column":42},{"id":165077,"Exited":0,"_deepnote_index_column":43},{"id":165078,"Exited":0,"_deepnote_index_column":44},{"id":165079,"Exited":0,"_deepnote_index_column":45},{"id":165080,"Exited":0,"_deepnote_index_column":46},{"id":165081,"Exited":0,"_deepnote_index_column":47},{"id":165082,"Exited":0,"_deepnote_index_column":48},{"id":165083,"Exited":0,"_deepnote_index_column":49},{"id":165084,"Exited":0,"_deepnote_index_column":50},{"id":165085,"Exited":0,"_deepnote_index_column":51},{"id":165086,"Exited":0,"_deepnote_index_column":52},{"id":165087,"Exited":0,"_deepnote_index_column":53},{"id":165088,"Exited":0,"_deepnote_index_column":54},{"id":165089,"Exited":0,"_deepnote_index_column":55},{"id":165090,"Exited":0,"_deepnote_index_column":56},{"id":165091,"Exited":0,"_deepnote_index_column":57},{"id":165092,"Exited":0,"_deepnote_index_column":58},{"id":165093,"Exited":0,"_deepnote_index_column":59},{"id":165094,"Exited":0,"_deepnote_index_column":60},{"id":165095,"Exited":0,"_deepnote_index_column":61},{"id":165096,"Exited":0,"_deepnote_index_column":62},{"id":165097,"Exited":0,"_deepnote_index_column":63},{"id":165098,"Exited":0,"_deepnote_index_column":64},{"id":165099,"Exited":0,"_deepnote_index_column":65},{"id":165100,"Exited":1,"_deepnote_index_column":66},{"id":165101,"Exited":0,"_deepnote_index_column":67},{"id":165102,"Exited":0,"_deepnote_index_column":68},{"id":165103,"Exited":0,"_deepnote_index_column":69},{"id":165104,"Exited":0,"_deepnote_index_column":70},{"id":165105,"Exited":0,"_deepnote_index_column":71},{"id":165106,"Exited":0,"_deepnote_index_column":72},{"id":165107,"Exited":0,"_deepnote_index_column":73},{"id":165108,"Exited":0,"_deepnote_index_column":74},{"id":165109,"Exited":0,"_deepnote_index_column":75},{"id":165110,"Exited":0,"_deepnote_index_column":76},{"id":165111,"Exited":0,"_deepnote_index_column":77},{"id":165112,"Exited":0,"_deepnote_index_column":78},{"id":165113,"Exited":0,"_deepnote_index_column":79},{"id":165114,"Exited":0,"_deepnote_index_column":80},{"id":165115,"Exited":0,"_deepnote_index_column":81},{"id":165116,"Exited":0,"_deepnote_index_column":82},{"id":165117,"Exited":0,"_deepnote_index_column":83},{"id":165118,"Exited":0,"_deepnote_index_column":84},{"id":165119,"Exited":0,"_deepnote_index_column":85},{"id":165120,"Exited":0,"_deepnote_index_column":86},{"id":165121,"Exited":0,"_deepnote_index_column":87},{"id":165122,"Exited":0,"_deepnote_index_column":88},{"id":165123,"Exited":0,"_deepnote_index_column":89},{"id":165124,"Exited":0,"_deepnote_index_column":90},{"id":165125,"Exited":0,"_deepnote_index_column":91},{"id":165126,"Exited":0,"_deepnote_index_column":92},{"id":165127,"Exited":0,"_deepnote_index_column":93},{"id":165128,"Exited":0,"_deepnote_index_column":94},{"id":165129,"Exited":0,"_deepnote_index_column":95},{"id":165130,"Exited":1,"_deepnote_index_column":96},{"id":165131,"Exited":0,"_deepnote_index_column":97},{"id":165132,"Exited":0,"_deepnote_index_column":98},{"id":165133,"Exited":0,"_deepnote_index_column":99}]},"text/plain":"            id  Exited\n0       165034       0\n1       165035       1\n2       165036       0\n3       165037       0\n4       165038       0\n...        ...     ...\n110018  275052       0\n110019  275053       0\n110020  275054       0\n110021  275055       0\n110022  275056       0\n\n[110023 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>Exited</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>165034</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>165035</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>165036</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>165037</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>165038</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>110018</th>\n      <td>275052</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>110019</th>\n      <td>275053</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>110020</th>\n      <td>275054</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>110021</th>\n      <td>275055</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>110022</th>\n      <td>275056</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>110023 rows Ã— 2 columns</p>\n</div>"},"metadata":{}}],"outputs_reference":"s3:deepnote-cell-outputs-production/70e1902c-e5c6-41d1-885c-24f375890156","content_dependencies":null},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"16f9c46ed80341d0b3e445859e4bff0b","deepnote_cell_type":"text-cell-h2"},"source":"## Export CSV","block_group":"11179e1880944debabb23367b2d4f7f0"},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1715054203342,"execution_millis":2587,"deepnote_to_be_reexecuted":true,"cell_id":"1fc520f6e30145f1b606e457ffa1a96b","deepnote_cell_type":"code"},"source":"submission_dict.to_csv('bankChurn_adaboost3.csv', index=False)","block_group":"af3684a41e194ff4ba4e0fbc07d01dd3","execution_count":null,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"'dict' object has no attribute 'to_csv'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn [3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msubmission_dict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbankChurn_adaboost3.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n","\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'to_csv'"]}],"outputs_reference":"dbtable:cell_outputs/57e2e3ef-0283-4bf2-be4d-a1c1e8794cac","content_dependencies":null},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=2a79941c-6614-47fe-9427-0e9f23998893' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote_persisted_session":{"createdAt":"2024-05-19T20:15:11.465Z"},"deepnote_notebook_id":"b6ada6cc7be14ec194bd41aeac33cae9","deepnote_execution_queue":[]}}
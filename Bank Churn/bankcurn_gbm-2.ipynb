{"cells":[{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"dc27cc549a104bf48f386bcdedfe3a63","deepnote_cell_type":"text-cell-h1"},"source":"# Spaceship KNN Submission","block_group":"082f42d739ae4c33b25722d4d688de17"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"bc80f9ad40bf4904a000a57390462824","deepnote_cell_type":"text-cell-p"},"source":"Hello there whoever handles this aku lupa, ini cuma coba-coba aja si pake KNeighborsClassifier buat test submission. Feel free to modify ya, lov u makasi semangat. -Egar","block_group":"7354f831abba43d4bdedda716dd7347c"},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1715234772977,"execution_millis":11687,"deepnote_to_be_reexecuted":false,"cell_id":"625264603a564362a3e6131c93f7f28d","deepnote_cell_type":"code"},"source":"!pip install xgboost==2.0.3","block_group":"4d8f13274afa46beb38ebda5d05c28b3","execution_count":null,"outputs":[{"name":"stdout","text":"Requirement already satisfied: xgboost==2.0.3 in /root/venv/lib/python3.9/site-packages (2.0.3)\nRequirement already satisfied: numpy in /shared-libs/python3.9/py/lib/python3.9/site-packages (from xgboost==2.0.3) (1.23.4)\nRequirement already satisfied: scipy in /shared-libs/python3.9/py/lib/python3.9/site-packages (from xgboost==2.0.3) (1.9.3)\n\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/319d8474-23bc-4cd3-9b2e-0e69b7bbcb9e","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1715234784679,"execution_millis":11217,"deepnote_to_be_reexecuted":false,"cell_id":"1f8c692c43f944d7a35af88210605688","deepnote_cell_type":"code"},"source":"!pip install lightgbm==4.3.0","block_group":"8c8476ba731f4952bd83427824ce647d","execution_count":null,"outputs":[{"name":"stdout","text":"Requirement already satisfied: lightgbm==4.3.0 in /root/venv/lib/python3.9/site-packages (4.3.0)\nRequirement already satisfied: scipy in /shared-libs/python3.9/py/lib/python3.9/site-packages (from lightgbm==4.3.0) (1.9.3)\nRequirement already satisfied: numpy in /shared-libs/python3.9/py/lib/python3.9/site-packages (from lightgbm==4.3.0) (1.23.4)\n\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/f970942b-9478-40b0-8931-607f92274fd6","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1715234795911,"execution_millis":62,"deepnote_to_be_reexecuted":false,"cell_id":"86499c8b397b45ccb3c7b38d1663e111","deepnote_cell_type":"code"},"source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.decomposition import PCA\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.ensemble import GradientBoostingClassifier, BaggingClassifier\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline","block_group":"6d67ae98cbb84c27b03fe6f9075b0099","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"e5061d67442b40c6ba067f8b5d0dbbb2","deepnote_cell_type":"text-cell-h1"},"source":"# Bring the Data In","block_group":"0c79d79c638f4ac7a0e36c426c763934"},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1715234795999,"execution_millis":2478,"deepnote_to_be_reexecuted":false,"cell_id":"0f2f648b08bc464caee3d8f7ab9c199e","deepnote_cell_type":"code"},"source":"# Load data\nX_df = pd.read_csv('preprocessedbankchurn_train.csv')\nX_df.drop(\"Exited\", axis=1, inplace=True)\ny_df = pd.read_csv('preprocessedbankchurn_train.csv')['Exited']\nX_submission = pd.read_csv('preprocessedbankchurn_test.csv')\n\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(X_df, y_df, test_size=0.2, random_state=42)\n","block_group":"9acd7481bdec43f3ba43972876e9aaa0","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"1d2c1898f8da49e9bc4983be8dddcada","deepnote_cell_type":"text-cell-h1"},"source":"# Model Training","block_group":"e01f12f006324577a8685ffcb2e53c45"},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1715234798479,"execution_millis":26498,"deepnote_to_be_reexecuted":false,"cell_id":"11e4cef4db984d3e895be71ee611244f","deepnote_cell_type":"code"},"source":"base_estimator = DecisionTreeClassifier(random_state=42)\n\n# Mendefinisikan BaggingClassifier\nbagging_model = BaggingClassifier(\n    base_estimator=base_estimator,\n    n_estimators=10,  # Jumlah model yang akan dibuat\n    random_state=42,\n    bootstrap=True,  # Sampel dengan penggantian\n    max_samples=0.8,  # Proporsi sampel untuk dibuat oleh setiap estimator\n    max_features=0.8  # Proporsi fitur untuk dibuat oleh setiap estimator\n)\n\nbagging_model.fit(X_train, y_train)\n\n# Mengevaluasi model\ny_pred = bagging_model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Test accuracy: {accuracy:.4f}\")","block_group":"67f9157ecbae4561854a5e040d06f879","execution_count":null,"outputs":[{"name":"stdout","text":"Test accuracy: 0.8537\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/8ebc385b-32c9-4ad8-b83c-5cd79278d097","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1715229139367,"execution_millis":316285,"deepnote_to_be_reexecuted":false,"cell_id":"f15ed3b2e42f47708f92895214192ea8","deepnote_cell_type":"code"},"source":"# GridSearchCV setup\ngrid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='roc_auc', verbose=1)\ngrid_search.fit(X_train, y_train)\n\n# Evaluate the model\nbest_model = grid_search.best_estimator_\ny_pred_proba = best_model.predict_proba(X_test)[:, 1]  # Get probabilities for the positive class\nprint(\"Best parameters:\", grid_search.best_params_)\nprint(\"ROC-AUC on test set:\", roc_auc_score(y_test, y_pred_proba))\n\n# Evaluasi pada test set\ntest_accuracy = grid_search.score(X_test, y_test)\nprint(\"Test set accuracy: {:.2f}\".format(test_accuracy))","block_group":"9c8bbf083d734e45b75bd18cb20f4730","execution_count":null,"outputs":[{"name":"stdout","text":"Fitting 5 folds for each of 1 candidates, totalling 5 fits\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 22373, number of negative: 83248\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008240 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1888\n[LightGBM] [Info] Number of data points in the train set: 105621, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211823 -> initscore=-1.313969\n[LightGBM] [Info] Start training from score -1.313969\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 17898, number of negative: 66598\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007735 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1888\n[LightGBM] [Info] Number of data points in the train set: 84496, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211821 -> initscore=-1.313986\n[LightGBM] [Info] Start training from score -1.313986\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 17899, number of negative: 66598\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006572 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1888\n[LightGBM] [Info] Number of data points in the train set: 84497, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211830 -> initscore=-1.313930\n[LightGBM] [Info] Start training from score -1.313930\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 17899, number of negative: 66598\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.055999 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 84497, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211830 -> initscore=-1.313930\n[LightGBM] [Info] Start training from score -1.313930\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 17898, number of negative: 66599\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008544 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1888\n[LightGBM] [Info] Number of data points in the train set: 84497, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211818 -> initscore=-1.314001\n[LightGBM] [Info] Start training from score -1.314001\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 17898, number of negative: 66599\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006672 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1887\n[LightGBM] [Info] Number of data points in the train set: 84497, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211818 -> initscore=-1.314001\n[LightGBM] [Info] Start training from score -1.314001\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 22372, number of negative: 83249\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009670 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 105621, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211814 -> initscore=-1.314026\n[LightGBM] [Info] Start training from score -1.314026\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 17897, number of negative: 66599\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009909 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1887\n[LightGBM] [Info] Number of data points in the train set: 84496, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211809 -> initscore=-1.314056\n[LightGBM] [Info] Start training from score -1.314056\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 17898, number of negative: 66599\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.120928 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 84497, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211818 -> initscore=-1.314001\n[LightGBM] [Info] Start training from score -1.314001\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 17898, number of negative: 66599\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008373 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 84497, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211818 -> initscore=-1.314001\n[LightGBM] [Info] Start training from score -1.314001\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 17898, number of negative: 66599\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.079898 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 84497, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211818 -> initscore=-1.314001\n[LightGBM] [Info] Start training from score -1.314001\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 17897, number of negative: 66600\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008885 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1888\n[LightGBM] [Info] Number of data points in the train set: 84497, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211806 -> initscore=-1.314071\n[LightGBM] [Info] Start training from score -1.314071\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 22373, number of negative: 83249\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.087017 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 105622, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211821 -> initscore=-1.313981\n[LightGBM] [Info] Start training from score -1.313981\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 17898, number of negative: 66599\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.055707 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1888\n[LightGBM] [Info] Number of data points in the train set: 84497, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211818 -> initscore=-1.314001\n[LightGBM] [Info] Start training from score -1.314001\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 17898, number of negative: 66599\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006731 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 84497, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211818 -> initscore=-1.314001\n[LightGBM] [Info] Start training from score -1.314001\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 17899, number of negative: 66599\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007806 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1888\n[LightGBM] [Info] Number of data points in the train set: 84498, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211827 -> initscore=-1.313945\n[LightGBM] [Info] Start training from score -1.313945\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 17899, number of negative: 66599\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006502 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1887\n[LightGBM] [Info] Number of data points in the train set: 84498, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211827 -> initscore=-1.313945\n[LightGBM] [Info] Start training from score -1.313945\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 17898, number of negative: 66600\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060620 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1888\n[LightGBM] [Info] Number of data points in the train set: 84498, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211816 -> initscore=-1.314016\n[LightGBM] [Info] Start training from score -1.314016\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 22373, number of negative: 83249\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036982 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1888\n[LightGBM] [Info] Number of data points in the train set: 105622, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211821 -> initscore=-1.313981\n[LightGBM] [Info] Start training from score -1.313981\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 17898, number of negative: 66599\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006490 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1888\n[LightGBM] [Info] Number of data points in the train set: 84497, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211818 -> initscore=-1.314001\n[LightGBM] [Info] Start training from score -1.314001\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 17898, number of negative: 66599\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007936 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1888\n[LightGBM] [Info] Number of data points in the train set: 84497, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211818 -> initscore=-1.314001\n[LightGBM] [Info] Start training from score -1.314001\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 17899, number of negative: 66599\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006512 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1888\n[LightGBM] [Info] Number of data points in the train set: 84498, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211827 -> initscore=-1.313945\n[LightGBM] [Info] Start training from score -1.313945\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 17899, number of negative: 66599\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007982 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1887\n[LightGBM] [Info] Number of data points in the train set: 84498, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211827 -> initscore=-1.313945\n[LightGBM] [Info] Start training from score -1.313945\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 17898, number of negative: 66600\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.054548 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1888\n[LightGBM] [Info] Number of data points in the train set: 84498, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211816 -> initscore=-1.314016\n[LightGBM] [Info] Start training from score -1.314016\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 22373, number of negative: 83249\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008439 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 105622, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211821 -> initscore=-1.313981\n[LightGBM] [Info] Start training from score -1.313981\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 17898, number of negative: 66599\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007759 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1888\n[LightGBM] [Info] Number of data points in the train set: 84497, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211818 -> initscore=-1.314001\n[LightGBM] [Info] Start training from score -1.314001\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 17898, number of negative: 66599\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006789 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 84497, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211818 -> initscore=-1.314001\n[LightGBM] [Info] Start training from score -1.314001\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 17899, number of negative: 66599\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006557 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 84498, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211827 -> initscore=-1.313945\n[LightGBM] [Info] Start training from score -1.313945\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 17899, number of negative: 66599\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007295 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1888\n[LightGBM] [Info] Number of data points in the train set: 84498, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211827 -> initscore=-1.313945\n[LightGBM] [Info] Start training from score -1.313945\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 17898, number of negative: 66600\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.085430 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1888\n[LightGBM] [Info] Number of data points in the train set: 84498, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211816 -> initscore=-1.314016\n[LightGBM] [Info] Start training from score -1.314016\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 27966, number of negative: 104061\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010709 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1890\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211820 -> initscore=-1.313988\n[LightGBM] [Info] Start training from score -1.313988\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 22373, number of negative: 83248\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009882 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1888\n[LightGBM] [Info] Number of data points in the train set: 105621, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211823 -> initscore=-1.313969\n[LightGBM] [Info] Start training from score -1.313969\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 22372, number of negative: 83249\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009984 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 105621, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211814 -> initscore=-1.314026\n[LightGBM] [Info] Start training from score -1.314026\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 22373, number of negative: 83249\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009727 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 105622, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211821 -> initscore=-1.313981\n[LightGBM] [Info] Start training from score -1.313981\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 22373, number of negative: 83249\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.057577 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1888\n[LightGBM] [Info] Number of data points in the train set: 105622, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211821 -> initscore=-1.313981\n[LightGBM] [Info] Start training from score -1.313981\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 22373, number of negative: 83249\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009591 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 105622, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211821 -> initscore=-1.313981\n[LightGBM] [Info] Start training from score -1.313981\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\nBest parameters: {'stacking__final_estimator__C': 10, 'stacking__lightgbm__lightgbm__learning_rate': 0.01, 'stacking__lightgbm__lightgbm__max_depth': 5, 'stacking__xgboost__xgboost__learning_rate': 0.01, 'stacking__xgboost__xgboost__max_depth': 5}\nROC-AUC on test set: 0.8896803390398874\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\nTest set accuracy: 0.89\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/166210c6-e07d-44f8-b82d-ae5617abdf25","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1715229455746,"execution_millis":373435,"deepnote_to_be_reexecuted":false,"cell_id":"f10cd8aa62c64a1c808b8e0986d5dfd3","deepnote_cell_type":"code"},"source":"# Evaluation on the test set\ny_pred = best_model.predict(X_test)\nprint(classification_report(y_test, y_pred))\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test, y_pred))\n\n# Fit the model on all training data\ngrid_search.fit(X_df, y_df)","block_group":"da8f0bbe4e924e31ba932bc2b30b8c4f","execution_count":null,"outputs":[{"name":"stdout","text":"[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n              precision    recall  f1-score   support\n\n           0       0.89      0.95      0.92     26052\n           1       0.75      0.54      0.63      6955\n\n    accuracy                           0.86     33007\n   macro avg       0.82      0.74      0.77     33007\nweighted avg       0.86      0.86      0.86     33007\n\nConfusion Matrix:\n[[24803  1249]\n [ 3217  3738]]\nFitting 5 folds for each of 1 candidates, totalling 5 fits\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010367 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1890\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 22349, number of negative: 83272\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009554 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 105621, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211596 -> initscore=-1.315331\n[LightGBM] [Info] Start training from score -1.315331\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 22349, number of negative: 83272\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008567 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 105621, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211596 -> initscore=-1.315331\n[LightGBM] [Info] Start training from score -1.315331\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 22350, number of negative: 83272\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008579 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 105622, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211604 -> initscore=-1.315286\n[LightGBM] [Info] Start training from score -1.315286\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 22350, number of negative: 83272\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010371 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 105622, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211604 -> initscore=-1.315286\n[LightGBM] [Info] Start training from score -1.315286\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 22350, number of negative: 83272\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009007 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 105622, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211604 -> initscore=-1.315286\n[LightGBM] [Info] Start training from score -1.315286\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010441 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 22349, number of negative: 83272\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008445 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 105621, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211596 -> initscore=-1.315331\n[LightGBM] [Info] Start training from score -1.315331\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 22349, number of negative: 83272\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008635 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 105621, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211596 -> initscore=-1.315331\n[LightGBM] [Info] Start training from score -1.315331\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 22350, number of negative: 83272\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008447 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1888\n[LightGBM] [Info] Number of data points in the train set: 105622, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211604 -> initscore=-1.315286\n[LightGBM] [Info] Start training from score -1.315286\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 22350, number of negative: 83272\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.056956 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1888\n[LightGBM] [Info] Number of data points in the train set: 105622, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211604 -> initscore=-1.315286\n[LightGBM] [Info] Start training from score -1.315286\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 22350, number of negative: 83272\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010047 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1888\n[LightGBM] [Info] Number of data points in the train set: 105622, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211604 -> initscore=-1.315286\n[LightGBM] [Info] Start training from score -1.315286\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010456 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 22349, number of negative: 83272\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008364 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 105621, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211596 -> initscore=-1.315331\n[LightGBM] [Info] Start training from score -1.315331\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 22349, number of negative: 83272\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010011 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 105621, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211596 -> initscore=-1.315331\n[LightGBM] [Info] Start training from score -1.315331\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 22350, number of negative: 83272\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.090974 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1888\n[LightGBM] [Info] Number of data points in the train set: 105622, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211604 -> initscore=-1.315286\n[LightGBM] [Info] Start training from score -1.315286\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 22350, number of negative: 83272\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060469 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1888\n[LightGBM] [Info] Number of data points in the train set: 105622, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211604 -> initscore=-1.315286\n[LightGBM] [Info] Start training from score -1.315286\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 22350, number of negative: 83272\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.086962 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1888\n[LightGBM] [Info] Number of data points in the train set: 105622, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211604 -> initscore=-1.315286\n[LightGBM] [Info] Start training from score -1.315286\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 27936, number of negative: 104091\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010487 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211593 -> initscore=-1.315349\n[LightGBM] [Info] Start training from score -1.315349\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 22349, number of negative: 83272\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009530 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 105621, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211596 -> initscore=-1.315331\n[LightGBM] [Info] Start training from score -1.315331\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 22348, number of negative: 83273\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010336 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1888\n[LightGBM] [Info] Number of data points in the train set: 105621, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211587 -> initscore=-1.315388\n[LightGBM] [Info] Start training from score -1.315388\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 22349, number of negative: 83273\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009605 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 105622, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315343\n[LightGBM] [Info] Start training from score -1.315343\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 22349, number of negative: 83273\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.083140 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1888\n[LightGBM] [Info] Number of data points in the train set: 105622, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315343\n[LightGBM] [Info] Start training from score -1.315343\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 22349, number of negative: 83273\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010393 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1887\n[LightGBM] [Info] Number of data points in the train set: 105622, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315343\n[LightGBM] [Info] Start training from score -1.315343\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104091\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015148 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 132028, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n[LightGBM] [Info] Start training from score -1.315314\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 22350, number of negative: 83272\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012828 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 105622, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211604 -> initscore=-1.315286\n[LightGBM] [Info] Start training from score -1.315286\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 22349, number of negative: 83273\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010056 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1888\n[LightGBM] [Info] Number of data points in the train set: 105622, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315343\n[LightGBM] [Info] Start training from score -1.315343\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 22349, number of negative: 83273\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008375 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 105622, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315343\n[LightGBM] [Info] Start training from score -1.315343\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 22350, number of negative: 83273\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.079083 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1888\n[LightGBM] [Info] Number of data points in the train set: 105623, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211602 -> initscore=-1.315298\n[LightGBM] [Info] Start training from score -1.315298\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 22350, number of negative: 83273\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009985 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1888\n[LightGBM] [Info] Number of data points in the train set: 105623, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211602 -> initscore=-1.315298\n[LightGBM] [Info] Start training from score -1.315298\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 34921, number of negative: 130113\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.169483 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1890\n[LightGBM] [Info] Number of data points in the train set: 165034, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315315\n[LightGBM] [Info] Start training from score -1.315315\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.090419 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1890\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011294 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012156 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n[LightGBM] [Info] Start training from score -1.315304\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 27936, number of negative: 104091\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011577 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211593 -> initscore=-1.315349\n[LightGBM] [Info] Start training from score -1.315349\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 27937, number of negative: 104091\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061391 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1889\n[LightGBM] [Info] Number of data points in the train set: 132028, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n[LightGBM] [Info] Start training from score -1.315314\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","output_type":"stream"},{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"GridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('stacking',\n                                        StackingClassifier(estimators=[('xgboost',\n                                                                        Pipeline(steps=[('scaler',\n                                                                                         StandardScaler()),\n                                                                                        ('xgboost',\n                                                                                         XGBClassifier(base_score=None,\n                                                                                                       booster=None,\n                                                                                                       callbacks=None,\n                                                                                                       colsample_bylevel=None,\n                                                                                                       colsample_bynode=None,\n                                                                                                       colsample_bytree=None,\n                                                                                                       device=None,\n                                                                                                       early_stopping_rounds=None,\n                                                                                                       enable_categorical=False,\n                                                                                                       eval_met...\n                                                                                         StandardScaler()),\n                                                                                        ('lightgbm',\n                                                                                         LGBMClassifier())]))],\n                                                           final_estimator=LogisticRegression()))]),\n             param_grid={'stacking__final_estimator__C': [10],\n                         'stacking__lightgbm__lightgbm__learning_rate': [0.01],\n                         'stacking__lightgbm__lightgbm__max_depth': [5],\n                         'stacking__xgboost__xgboost__learning_rate': [0.01],\n                         'stacking__xgboost__xgboost__max_depth': [5]},\n             scoring='roc_auc', verbose=1)","text/html":"<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n             estimator=Pipeline(steps=[(&#x27;stacking&#x27;,\n                                        StackingClassifier(estimators=[(&#x27;xgboost&#x27;,\n                                                                        Pipeline(steps=[(&#x27;scaler&#x27;,\n                                                                                         StandardScaler()),\n                                                                                        (&#x27;xgboost&#x27;,\n                                                                                         XGBClassifier(base_score=None,\n                                                                                                       booster=None,\n                                                                                                       callbacks=None,\n                                                                                                       colsample_bylevel=None,\n                                                                                                       colsample_bynode=None,\n                                                                                                       colsample_bytree=None,\n                                                                                                       device=None,\n                                                                                                       early_stopping_rounds=None,\n                                                                                                       enable_categorical=False,\n                                                                                                       eval_met...\n                                                                                         StandardScaler()),\n                                                                                        (&#x27;lightgbm&#x27;,\n                                                                                         LGBMClassifier())]))],\n                                                           final_estimator=LogisticRegression()))]),\n             param_grid={&#x27;stacking__final_estimator__C&#x27;: [10],\n                         &#x27;stacking__lightgbm__lightgbm__learning_rate&#x27;: [0.01],\n                         &#x27;stacking__lightgbm__lightgbm__max_depth&#x27;: [5],\n                         &#x27;stacking__xgboost__xgboost__learning_rate&#x27;: [0.01],\n                         &#x27;stacking__xgboost__xgboost__max_depth&#x27;: [5]},\n             scoring=&#x27;roc_auc&#x27;, verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5,\n             estimator=Pipeline(steps=[(&#x27;stacking&#x27;,\n                                        StackingClassifier(estimators=[(&#x27;xgboost&#x27;,\n                                                                        Pipeline(steps=[(&#x27;scaler&#x27;,\n                                                                                         StandardScaler()),\n                                                                                        (&#x27;xgboost&#x27;,\n                                                                                         XGBClassifier(base_score=None,\n                                                                                                       booster=None,\n                                                                                                       callbacks=None,\n                                                                                                       colsample_bylevel=None,\n                                                                                                       colsample_bynode=None,\n                                                                                                       colsample_bytree=None,\n                                                                                                       device=None,\n                                                                                                       early_stopping_rounds=None,\n                                                                                                       enable_categorical=False,\n                                                                                                       eval_met...\n                                                                                         StandardScaler()),\n                                                                                        (&#x27;lightgbm&#x27;,\n                                                                                         LGBMClassifier())]))],\n                                                           final_estimator=LogisticRegression()))]),\n             param_grid={&#x27;stacking__final_estimator__C&#x27;: [10],\n                         &#x27;stacking__lightgbm__lightgbm__learning_rate&#x27;: [0.01],\n                         &#x27;stacking__lightgbm__lightgbm__max_depth&#x27;: [5],\n                         &#x27;stacking__xgboost__xgboost__learning_rate&#x27;: [0.01],\n                         &#x27;stacking__xgboost__xgboost__max_depth&#x27;: [5]},\n             scoring=&#x27;roc_auc&#x27;, verbose=1)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;stacking&#x27;,\n                 StackingClassifier(estimators=[(&#x27;xgboost&#x27;,\n                                                 Pipeline(steps=[(&#x27;scaler&#x27;,\n                                                                  StandardScaler()),\n                                                                 (&#x27;xgboost&#x27;,\n                                                                  XGBClassifier(base_score=None,\n                                                                                booster=None,\n                                                                                callbacks=None,\n                                                                                colsample_bylevel=None,\n                                                                                colsample_bynode=None,\n                                                                                colsample_bytree=None,\n                                                                                device=None,\n                                                                                early_stopping_rounds=None,\n                                                                                enable_categorical=False,\n                                                                                eval_metric=&#x27;logloss&#x27;,\n                                                                                feature_types=...\n                                                                                max_cat_to_onehot=None,\n                                                                                max_delta_step=None,\n                                                                                max_depth=None,\n                                                                                max_leaves=None,\n                                                                                min_child_weight=None,\n                                                                                missing=nan,\n                                                                                monotone_constraints=None,\n                                                                                multi_strategy=None,\n                                                                                n_estimators=None,\n                                                                                n_jobs=None,\n                                                                                num_parallel_tree=None,\n                                                                                random_state=None, ...))])),\n                                                (&#x27;lightgbm&#x27;,\n                                                 Pipeline(steps=[(&#x27;scaler&#x27;,\n                                                                  StandardScaler()),\n                                                                 (&#x27;lightgbm&#x27;,\n                                                                  LGBMClassifier())]))],\n                                    final_estimator=LogisticRegression()))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">stacking: StackingClassifier</label><div class=\"sk-toggleable__content\"><pre>StackingClassifier(estimators=[(&#x27;xgboost&#x27;,\n                                Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n                                                (&#x27;xgboost&#x27;,\n                                                 XGBClassifier(base_score=None,\n                                                               booster=None,\n                                                               callbacks=None,\n                                                               colsample_bylevel=None,\n                                                               colsample_bynode=None,\n                                                               colsample_bytree=None,\n                                                               device=None,\n                                                               early_stopping_rounds=None,\n                                                               enable_categorical=False,\n                                                               eval_metric=&#x27;logloss&#x27;,\n                                                               feature_types=None,\n                                                               gamma=None,\n                                                               grow_policy=...\n                                                               max_cat_to_onehot=None,\n                                                               max_delta_step=None,\n                                                               max_depth=None,\n                                                               max_leaves=None,\n                                                               min_child_weight=None,\n                                                               missing=nan,\n                                                               monotone_constraints=None,\n                                                               multi_strategy=None,\n                                                               n_estimators=None,\n                                                               n_jobs=None,\n                                                               num_parallel_tree=None,\n                                                               random_state=None, ...))])),\n                               (&#x27;lightgbm&#x27;,\n                                Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n                                                (&#x27;lightgbm&#x27;,\n                                                 LGBMClassifier())]))],\n                   final_estimator=LogisticRegression())</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>xgboost</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=&#x27;logloss&#x27;,\n              feature_types=None, gamma=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=None, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n              max_leaves=None, min_child_weight=None, missing=nan,\n              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n              n_jobs=None, num_parallel_tree=None, random_state=None, ...)</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>lightgbm</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier()</pre></div></div></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>final_estimator</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div>"},"metadata":{}}],"outputs_reference":"s3:deepnote-cell-outputs-production/8bf1608d-a592-4a2e-81d2-33dfbcd22aaf","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1715229829243,"execution_millis":2023,"deepnote_to_be_reexecuted":false,"cell_id":"0d297f6fd6f4447db0dfe0f817c12c59","deepnote_cell_type":"code"},"source":"# Predict probabilities for submission data\ny_submission_proba = grid_search.predict_proba(X_submission)[:, 1]  # Probabilities of Exited\nprint(y_submission_proba)\n\n# Prepare submission dictionary\nsubmission_dict = {'id': X_submission['id'], 'Exited': y_submission_proba}\nsubmission_dict = pd.DataFrame(submission_dict)\nsubmission_dict","block_group":"ffa1a5a6dc0742f6a823ee9eafca1c28","execution_count":null,"outputs":[{"name":"stdout","text":"[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[0.04470724 0.89299227 0.04470724 ... 0.04470724 0.11143137 0.15025619]\n","output_type":"stream"},{"output_type":"execute_result","execution_count":11,"data":{"application/vnd.deepnote.dataframe.v3+json":{"column_count":2,"row_count":110023,"columns":[{"name":"id","dtype":"int64"},{"name":"Exited","dtype":"float64"},{"name":"_deepnote_index_column","dtype":"int64"}],"rows":[{"id":165034,"Exited":0.044707238654133015,"_deepnote_index_column":0},{"id":165035,"Exited":0.8929922693949688,"_deepnote_index_column":1},{"id":165036,"Exited":0.044707238654133015,"_deepnote_index_column":2},{"id":165037,"Exited":0.18696581935942175,"_deepnote_index_column":3},{"id":165038,"Exited":0.27273002829934423,"_deepnote_index_column":4},{"id":165039,"Exited":0.044707238654133015,"_deepnote_index_column":5},{"id":165040,"Exited":0.05933274670608735,"_deepnote_index_column":6},{"id":165041,"Exited":0.05933274670608735,"_deepnote_index_column":7},{"id":165042,"Exited":0.8339305623424205,"_deepnote_index_column":8},{"id":165043,"Exited":0.044707238654133015,"_deepnote_index_column":9}]},"text/plain":"            id    Exited\n0       165034  0.044707\n1       165035  0.892992\n2       165036  0.044707\n3       165037  0.186966\n4       165038  0.272730\n...        ...       ...\n110018  275052  0.061882\n110019  275053  0.081961\n110020  275054  0.044707\n110021  275055  0.111431\n110022  275056  0.150256\n\n[110023 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>Exited</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>165034</td>\n      <td>0.044707</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>165035</td>\n      <td>0.892992</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>165036</td>\n      <td>0.044707</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>165037</td>\n      <td>0.186966</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>165038</td>\n      <td>0.272730</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>110018</th>\n      <td>275052</td>\n      <td>0.061882</td>\n    </tr>\n    <tr>\n      <th>110019</th>\n      <td>275053</td>\n      <td>0.081961</td>\n    </tr>\n    <tr>\n      <th>110020</th>\n      <td>275054</td>\n      <td>0.044707</td>\n    </tr>\n    <tr>\n      <th>110021</th>\n      <td>275055</td>\n      <td>0.111431</td>\n    </tr>\n    <tr>\n      <th>110022</th>\n      <td>275056</td>\n      <td>0.150256</td>\n    </tr>\n  </tbody>\n</table>\n<p>110023 rows × 2 columns</p>\n</div>"},"metadata":{}}],"outputs_reference":"s3:deepnote-cell-outputs-production/38612218-07fe-4a35-b480-31aa603ce2fd","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1715229831335,"execution_millis":1419,"deepnote_to_be_reexecuted":false,"cell_id":"d97c62130a89483bb1357426338acdaf","deepnote_cell_type":"code"},"source":"submission_dict.to_csv('bankChurn_ensmenle.csv', index=False)","block_group":"271dcf9f53f8461ca073abccc0e239ab","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=2a79941c-6614-47fe-9427-0e9f23998893' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iVVRGLTgiPz4KPHN2ZyB3aWR0aD0iODBweCIgaGVpZ2h0PSI4MHB4IiB2aWV3Qm94PSIwIDAgODAgODAiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayI+CiAgICA8IS0tIEdlbmVyYXRvcjogU2tldGNoIDU0LjEgKDc2NDkwKSAtIGh0dHBzOi8vc2tldGNoYXBwLmNvbSAtLT4KICAgIDx0aXRsZT5Hcm91cCAzPC90aXRsZT4KICAgIDxkZXNjPkNyZWF0ZWQgd2l0aCBTa2V0Y2guPC9kZXNjPgogICAgPGcgaWQ9IkxhbmRpbmciIHN0cm9rZT0ibm9uZSIgc3Ryb2tlLXdpZHRoPSIxIiBmaWxsPSJub25lIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiPgogICAgICAgIDxnIGlkPSJBcnRib2FyZCIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoLTEyMzUuMDAwMDAwLCAtNzkuMDAwMDAwKSI+CiAgICAgICAgICAgIDxnIGlkPSJHcm91cC0zIiB0cmFuc2Zvcm09InRyYW5zbGF0ZSgxMjM1LjAwMDAwMCwgNzkuMDAwMDAwKSI+CiAgICAgICAgICAgICAgICA8cG9seWdvbiBpZD0iUGF0aC0yMCIgZmlsbD0iIzAyNjVCNCIgcG9pbnRzPSIyLjM3NjIzNzYyIDgwIDM4LjA0NzY2NjcgODAgNTcuODIxNzgyMiA3My44MDU3NTkyIDU3LjgyMTc4MjIgMzIuNzU5MjczOSAzOS4xNDAyMjc4IDMxLjY4MzE2ODMiPjwvcG9seWdvbj4KICAgICAgICAgICAgICAgIDxwYXRoIGQ9Ik0zNS4wMDc3MTgsODAgQzQyLjkwNjIwMDcsNzYuNDU0OTM1OCA0Ny41NjQ5MTY3LDcxLjU0MjI2NzEgNDguOTgzODY2LDY1LjI2MTk5MzkgQzUxLjExMjI4OTksNTUuODQxNTg0MiA0MS42NzcxNzk1LDQ5LjIxMjIyODQgMjUuNjIzOTg0Niw0OS4yMTIyMjg0IEMyNS40ODQ5Mjg5LDQ5LjEyNjg0NDggMjkuODI2MTI5Niw0My4yODM4MjQ4IDM4LjY0NzU4NjksMzEuNjgzMTY4MyBMNzIuODcxMjg3MSwzMi41NTQ0MjUgTDY1LjI4MDk3Myw2Ny42NzYzNDIxIEw1MS4xMTIyODk5LDc3LjM3NjE0NCBMMzUuMDA3NzE4LDgwIFoiIGlkPSJQYXRoLTIyIiBmaWxsPSIjMDAyODY4Ij48L3BhdGg+CiAgICAgICAgICAgICAgICA8cGF0aCBkPSJNMCwzNy43MzA0NDA1IEwyNy4xMTQ1MzcsMC4yNTcxMTE0MzYgQzYyLjM3MTUxMjMsLTEuOTkwNzE3MDEgODAsMTAuNTAwMzkyNyA4MCwzNy43MzA0NDA1IEM4MCw2NC45NjA0ODgyIDY0Ljc3NjUwMzgsNzkuMDUwMzQxNCAzNC4zMjk1MTEzLDgwIEM0Ny4wNTUzNDg5LDc3LjU2NzA4MDggNTMuNDE4MjY3Nyw3MC4zMTM2MTAzIDUzLjQxODI2NzcsNTguMjM5NTg4NSBDNTMuNDE4MjY3Nyw0MC4xMjg1NTU3IDM2LjMwMzk1NDQsMzcuNzMwNDQwNSAyNS4yMjc0MTcsMzcuNzMwNDQwNSBDMTcuODQzMDU4NiwzNy43MzA0NDA1IDkuNDMzOTE5NjYsMzcuNzMwNDQwNSAwLDM3LjczMDQ0MDUgWiIgaWQ9IlBhdGgtMTkiIGZpbGw9IiMzNzkzRUYiPjwvcGF0aD4KICAgICAgICAgICAgPC9nPgogICAgICAgIDwvZz4KICAgIDwvZz4KPC9zdmc+' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote_persisted_session":{"createdAt":"2024-05-09T06:27:50.554Z"},"deepnote_notebook_id":"93faf6e94f74418194174879b52c5b26","deepnote_execution_queue":[]}}